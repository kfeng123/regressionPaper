\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,amsthm}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}
\begin{document}

\begin{frontmatter}

\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
%\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Main}

Suppose $(X_1^T,Y_1)$,\ldots,$(X_n^T, Y_n)$ are i.i.d.\ from $N_{p+1}(\mu,\Sigma)$,
where $X_i\in \mathbb{R}^p$ and $Y_i\in \mathbb{R}$. 
Denote $X=(X_1,\ldots,X_n)$, $Y={(Y_1,\ldots,Y_n)}^T$.

Write $Y=\beta_0 \textbf{1}_n+X^T \beta+\epsilon$, where $\textbf{1}_n$ is $n$ dimensional vector with all elements equal to $1$. $\epsilon$ has distribution $N(0,\sigma^2 I_n)$.

The problem is to test hypotheses $H: \beta=0$.

%The test statistic is

%\[
%    T=\frac{
%        (\textbf{1}_n^T(X^T X)^{-1}Q_n Y)^2
%    }{
%        \hat{\sigma}^2
%        \textbf{1}_n^T(X^T X)^{-1}Q_n (X^T X)^{-1}\textbf{1}_n
%    }.
%\]
%where $Q_n=I_n-\frac{1}{n}\textbf{1}_n\textbf{1}_n^T$ and
%\[
%    \hat{\sigma}^2=\frac{1}{n-2} Y^T Q_n\Big[
%        I_n-\frac{(X^T X)^{-1}\textbf{1}_n\textbf{1}_n^T(X^T X)^{-1}}{
%        \textbf{1}_n^T(X^T X)^{-1}Q_n(X^T X)^{-1}\textbf{1}_n
%        }
%%        \Big]Q_n Y
%\] 


Let $Q_n=WW^T$ be the rank decomposition of $Q_n$, where $W_n$ is a $n\times n-1$ matrix with $W^T W=I_{n-1}$. The new test statistic is

\[
    T=\frac{y^T Q_n y}{
        y^T W{(W^T X^T X W)}^{-1}W^T y
    } 
    \]
or equivalently
\[
    \frac{y^T Q_n y}{
        y^T Q_n{(X^T X)}^{-1}Q_n y-{(y^T Q_n{(X^T X)}^{-1}\textbf{1}_n)}^2/(\textbf{1}_n{(X^T X)}^{-1}\textbf{1}_n)
    } 
    \]


Let $\tilde{y}=W^T y$, $\tilde{X}=XW$, $\tilde{\epsilon}=W^T \epsilon$. Then
\[
    \tilde{y}=\tilde{X}^T \beta + \tilde{\epsilon}
    \]
and
\[
    T=\frac{\tilde{y}^T \tilde{y}}{
        \tilde{y}^T{(\tilde{X}^T \tilde{X})}^{-1}\tilde{y}
    }
    \]
Next we derive another form of of $T$. We follow the similar technique of Hotelling's $T^2$.


Let $R$ be an $(n-1)\times (n-1)$ orthogonal matrix satisfies 
\[
    R\tilde{y}=
    \begin{pmatrix}
        \|\tilde{y}\|\\
        0\\
        \ldots\\
        0
    \end{pmatrix}.
    \]
%Note that $R$ is independent of $\tilde{X}$ under null hypotheses since it only depends on $\tilde{y}$.
We can write
    \begin{equation}\label{Tformula}
        \begin{aligned}
            T&=\frac{\|\tilde{y}\|^2
        }{\tilde{y}R^T{(R\tilde{X}^T\tilde{X}R^T)}^{-1}R\tilde{y}}\\
            %&={(R\tilde{X}^T \tilde{X} R^T)}_{11\cdot 2}.
        \end{aligned}
    \end{equation}
Denote by $B=R\tilde{X}^T \tilde{X} R^T$, then
\[
    T=\frac{1}{{(B^{-1})}_{11}}.
    \]

Let
\[
    B=\begin{pmatrix} 
        b_{11} & b_{(1)}^T\\
        b_{(1)} & B_{22}
    \end{pmatrix},
    \]

and apply the matrix inverse formula, we have ${(B^{-1})}_{11}=1/(b_{11}-b_{(1)}^T B_{22}^{-1}b_{(1)})$. Hence

\[
   T= b_{11}-b_{(1)}^T B_{22}^{-1}b_{(1)}.
    \]

\section{Asymptotic distribution}
Note that conditioning on $\tilde{y}$, $R$ is a constant orthogonal matrix.
And $\tilde{y}$ is independent of $\tilde{X}$ under null hypotheses.
So  $B|\tilde{y}$ has the same distribution with $\tilde{X}^T \tilde{X}$ under null hypotheses.
Hence $B$ is independent of $\tilde{y}$ and can be written as
    \begin{equation}\label{Xdis}
    B=\sum_{i=1}^p \lambda_i z_i z_i^T
    \end{equation}
where $z_i$'s are i.i.d.\ $n-1$ dimensional random vectors  distributed as $N(0,I_{n-1})$, $\lambda_1\geq \lambda_2\ldots \geq \lambda_p>0$ are eigenvalues of $\Sigma_X$. 
Denote by $\Lambda=\textrm{diag} (\lambda_1,\ldots,\lambda_p)$, $Z=(Z_1,\ldots,Z_p)$. Let $Z_{(1)}$ and $Z_{(2)}$ be the first $1$ row and last $n-2$ rows of $Z$, that is
\[
    Z=\begin{pmatrix} 
        Z_{(1)}\\
        Z_{(2)}
    \end{pmatrix}.
    \]
Then
\begin{equation}
    \begin{aligned}
        B&=Z\Lambda Z^T\\
        &=\begin{pmatrix}
            Z_{(1)}\Lambda Z_{(1)}^T & Z_{(1)}\Lambda Z_{(2)}^T\\
            Z_{(2)}\Lambda Z_{(1)}^T & Z_{(2)}\Lambda Z_{(2)}^T\\
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Hence

\begin{equation}
    \begin{aligned}
        T&=Z_{(1)}\Lambda Z_{(1)}^T-Z_{(1)}\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda Z_{(1)}^T\\
        &=Z_{(1)}\big(\Lambda -\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda \big)Z_{(1)}^T.\\
    \end{aligned}
\end{equation}
But
    \begin{equation}
        \begin{aligned}
    \textrm{rank}(\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda)
            &=\textrm{rank}(\Lambda^{\frac{1}{2}} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{\frac{1}{2}})\\
            &=\textrm{rank}(I_{n-2})=n-2,\\
        \end{aligned}
    \end{equation}
and
    \begin{equation}
        \begin{aligned}
    \textrm{rank}(\Lambda-\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda)
            &=\textrm{rank}(I_p-\Lambda^{\frac{1}{2}} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{\frac{1}{2}})\\
            &=p-n+2.\\
        \end{aligned}
    \end{equation}
    Hence
    \[
        T\sim\sum_{i=1}^{p-n+2} \lambda_{i}(\Lambda -\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda) \chi^2_1
        \]
By Weyl's inequality, we have for $1\leq i\leq p-n+2$
\begin{equation}
    \lambda_i(\Lambda -\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda)
    \leq \lambda_i(\Lambda),
\end{equation}
and
\begin{equation}
    \begin{aligned}
        &\lambda_i(\Lambda -\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda)\\
        \geq& \lambda_{i+n-2}(\Lambda)+\lambda_{p-n+2}(-\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda)\\
        =& \lambda_{i+n-2}.
    \end{aligned}
\end{equation}
Hence
\[
    \sum_{i=n-1}^p \lambda_i \chi^2_1\leq T\leq\sum_{i=1}^{p-n+2}\lambda_i \chi^2_1
    \]

Note that under condition \({\textrm{tr}\Sigma^4}/{{(\textrm{tr}\Sigma^2)}^2} \to 0\), we have by Liapounoff central limit theorem that
\[
    \frac{\sum_{i=1}^p \lambda_i \chi^2_1-\textrm{tr}\Sigma_X}{
        \sqrt{\textrm{tr}(\Sigma_X^2)}
    }\xrightarrow{\mathcal{L}}N(0,1).
    \]
And 
    \begin{equation}\label{xiaoO}
    \frac{T-\textrm{tr}\Sigma_X}{\sqrt{\textrm{tr}(\Sigma_X^2)}}-
    \frac{\sum_{i=1}^p \lambda_i \chi^2_1-\textrm{tr}\Sigma_X}{\sqrt{\textrm{tr}(\Sigma_X^2)}}
    =\frac{T-\sum_{i=1}^p \lambda_i \chi^2_1}{\sqrt{\textrm{tr}(\Sigma_X^2)}},
    \end{equation}
    To prove $\eqref{xiaoO}\xrightarrow{P}0$, we only need to prove
\[
    \textrm{E}\Big(\frac{\sum_{i=1}^{n-2} \lambda_i \chi^2_1}{
        \sqrt{\textrm{tr}(\Sigma_X^2)}}\Big)\to 0,
    \]
that is
    \begin{equation}\label{tiaojian}
    \textrm{E}\Big(\frac{\sum_{i=1}^{n-2} \lambda_i}{\sqrt{\textrm{tr}(\Sigma_X^2)}}\Big)\to 0.
    \end{equation}
If $\lambda_i$'s are bounded below and above, then~\eqref{tiaojian} is equivalent to

    \begin{equation}\label{npOrder}
        n/\sqrt{p}\to 0,
    \end{equation}
    or $p/n^2 \to \infty$. We thus obtain the following theorem.


\begin{theorem}
    Suppose
    \[
    \textrm{E}\Big(\frac{\sum_{i=1}^{n-2} \lambda_i}{\sqrt{\textrm{tr}(\Sigma_X^2)}}\Big)\to 0,
        \] 
    and
    \[
        \frac{\textrm{tr}\Sigma^4}{{(\textrm{tr}\Sigma^2)}^2} \to 0.
        \]
    Then under null hypotheses, we have
    \[
    \frac{T-\textrm{tr}\Sigma_X}{
        \sqrt{\textrm{tr}(\Sigma_X^2)}
    }\xrightarrow{\mathcal{L}}N(0,1).
        \]
\end{theorem}
\section{Full Asymptotic Results}

$\Sigma_X=P\Lambda P^T$

Non-spike: there's no principal component ($r=0$). That is, $\lambda_1=\cdots = \lambda_p$.

Spike: there's $r$ principal components. That is, $\lambda_1\geq \lambda_2\geq \cdots \lambda_r\geq \lambda_{r+1}=\cdots =\lambda_p$. Denote by $P_1$ the first $r$ column of $P$ and $P_2$ the last $p-r$ column of $P$.

\begin{equation}
    \begin{aligned}
        Y&=\beta_0 \textbf{1}_n+X^T\beta+\epsilon\\
        &=\beta_0 \textbf{1}_n+X^T P_1P_1^T\beta+X^T P_2 P_2^T\beta+\epsilon\\
    \end{aligned}
\end{equation}

In either case, let $\lambda$ be $\lambda=\lambda_{r+1}=\cdots=\lambda_{p}$.

PCR try to do regression between $Y$ and (estimated) $X^T P_1$. If $P_1$ is observed, then the problem is reduced to testing an ordinary regression model. However, it's not the case.

Simply estimating $P_1$ and invoke classical testing procedure may not be a good idea since the estimation may not be consistent in high dimension. In fact, there may be even no principal component!

In this paper, testing PCR means testing:

$H_0$: There's no principal component or there's $r$ principal components but $P_1^T \beta =0$.

$H_1$; There's $r$ principal components and $P_1^T \beta \neq 0$.

Next we consider:

1. There's no PC.\@

2. There's $r$ principal components but $P_1\beta=0$.

\subsection{circumstance 1}


\begin{equation}
    T=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }
\end{equation}
Independent of data, generate a random $p$ dimensional orthonormal matrix $O$ with Haar invariant distribution. And 

\begin{equation}
    T=\frac{{(O\beta)}^T O\tilde{X}{(O\tilde{X})}^T O\beta+
        2{(O\beta)}^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}{(O\tilde{X})}^T \beta+
        2{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}
    }
\end{equation}

Note that conditioning on $O$, $O\tilde{X}$ is a random matrix with each entry independently distributed as $N(0,\lambda)$. Hence $O$ is independent of $O\tilde{X}$. Observe also that $O\beta/\|\beta\|$ is uniformly distributed on the unit ball.  We can without loss of generality assuming that $\beta/\|\beta\|$ is uniformly distributed on the unit ball. Independent of data, generate $R>0$ with $R^2$ distributed as $\chi^2_{p}$. Then $\xi=R\beta/\|\beta\|$ distributed as $N_p(0,I_p)$.
Note that conditioning on $\tilde{X}$, $\eta={(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{X}^T \xi$ is distributed as $N_{n-1}(0,I_{n-1})$. Hence $\eta$ is independent of $\tilde{X}$.

Then

\begin{equation}
    \begin{aligned}
        T&=\frac{{(\|\beta\|/R)}^2\xi^T \tilde{X}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=
        \frac{{(\|\beta\|/R)}^2\eta^T \tilde{X}^T\tilde{X} \eta+
        2(\|\beta\|/R)\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\eta^T\eta+
        2(\|\beta\|/R)\eta^T{(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}

We consider

\begin{equation}
    \begin{aligned}
    \frac{\frac{\|\beta\|^2}{R^2} \eta^T\eta
        }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}}&=
        \frac{(n-1)\|\beta\|^2}{p}
        \frac{\frac{p}{R^2} \frac{\eta^T\eta}{n-1}
        }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}}
    \end{aligned}
\end{equation}

Note that
\begin{equation}
    \begin{aligned}
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}&\sim
        \frac{\tilde{\epsilon}^T\tilde{\epsilon}}{{(\tilde{X}^T\tilde{X})}_{11\cdot 2}}\\
        &\sim\frac{\sigma^2}{\lambda}\frac{\chi^2_{n-1}}{\chi^2_{p-n+2}}
    \end{aligned}
\end{equation}
Hence
\begin{equation}
    \begin{aligned}
    \frac{\frac{\|\beta\|^2}{R^2} \eta^T\eta
        }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}}&=
        \frac{(n-1)(p-n+2)}{p(n-1)}\frac{\lambda}{\sigma^2}\|\beta\|^2(1+o_P(1))\\
        &=\frac{\lambda}{\sigma^2}\|\beta\|^2(1+o_P(1))
    \end{aligned}
\end{equation}

On the other hand,

\begin{equation}
    \begin{aligned}
        \frac{\frac{\|\beta\|^2}{R^2} \eta^T\tilde{X}^T\tilde{X}\eta
        }{\tilde{\epsilon}^T\tilde{\epsilon}}&=
        \frac{\|\beta\|^2}{\sigma^2 p(n-1)}
        \frac{\frac{p}{R^2} \eta^T\tilde{X}^T\tilde{X}\eta
        }{\frac{\tilde{\epsilon}^T\tilde{\epsilon}}{\sigma^2(n-1)}}
    \end{aligned}
\end{equation}
Note that
\begin{equation}
    \begin{aligned}
        \eta^T\tilde{X}^T\tilde{X}\eta&\sim
        \eta^T\eta {(\tilde{X}^T\tilde{X})}_{11}\\
        &\sim \lambda\chi^2_{n-1} \chi^2_{p}
    \end{aligned}
\end{equation}
Hence

\begin{equation}
    \begin{aligned}
        \frac{\frac{\|\beta\|^2}{R^2} \eta^T\tilde{X}^T\tilde{X}\eta
        }{\tilde{\epsilon}^T\tilde{\epsilon}}&=
        \frac{\|\beta\|^2}{\sigma^2 p(n-1)}
        \lambda (n-1)p
        (1+o_P(1))\\
        &=\frac{\lambda}{\sigma^2}\|\beta\|^2(1+o_P(1))
    \end{aligned}
\end{equation}
We can deduce that:
If $\|\beta\|^2\to 0$, then

\begin{equation}
    \begin{aligned}
        T&=
        \frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }(1+o_P(1))\\
    \end{aligned}
\end{equation}
If $\|\beta\|^2\to \infty$, then

\begin{equation}
    \begin{aligned}
        T&=
        \frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}(1+o_P(1))\\
    \end{aligned}
\end{equation}

Note that
\begin{equation}
    \begin{aligned}
        \frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }&\sim {(\tilde{X}^T\tilde{X})}_{11\cdot 2}\sim \lambda \chi^2_{p-n+2}
    \end{aligned}
\end{equation}
Hence by CLT,
\begin{equation}
    \begin{aligned}
        \frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        }\xrightarrow{\mathcal{L}}N(0,1)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        }&=
        \frac{\sqrt{p}}{\sqrt{p-n+2}}
        \frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-{\lambda(p-n+2)}}{\lambda\sqrt{2p}
        }\\
        &=
        \frac{\sqrt{p}}{\sqrt{p-n+2}}
        \Big(\frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-{\lambda p}}{\lambda\sqrt{2p}
        }+\frac{(n-2)}{\sqrt{2p}}\Big)\\
    \end{aligned}
\end{equation}
By slutsky Theorem, 
\begin{equation}
    \frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-{\lambda p}}{\lambda\sqrt{2p}
        }\xrightarrow{\mathcal{L}}N(0,1)
\end{equation}

On the other hand

\begin{equation}
    \begin{aligned}
        \frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}&\sim {(\tilde{X}^T\tilde{X})}_{11}\sim \lambda \chi^2_{p}
\\
    \end{aligned}
\end{equation}

Hence by CLT,
\begin{equation}
    \begin{aligned}
        \frac{\frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}
    -\lambda p}{\lambda\sqrt{2p}
        }\xrightarrow{\mathcal{L}}N(0,1)
    \end{aligned}
\end{equation}


$A_1\asymp n\|\beta\|^2$. $A_3\asymp n$. And $B_1\asymp \frac{n}{p}\|\beta\|^2$. $B_3\asymp n/p$.

\begin{equation}
    \begin{aligned}
        A_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{np})\\
        &=O_P(\sqrt{n}\|\beta\|)
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        B_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}^{-1}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{n/p})\\
        &=O_P(\frac{\sqrt{n}}{p}\|\beta\|)
    \end{aligned}
\end{equation}

If $\|\beta\|^2\to 0$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
    \Big|T-\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_3}{B_3}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_1+A_2)B_3-(B_1+B_2)A_3}{(B_1+B_2+B_3)B_3}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(n\|\beta\|^2)+O_P(\sqrt{n}\|\beta\|))O_P(\frac{n}{p})-(O_P(\frac{n}{p}\|\beta\|^2)+O_P(\frac{\sqrt{n}}{p}\|\beta\|))O_P(n)}{n^2/p^2}
        \Big|\\
        &=O_P(\sqrt{p}\|\beta\|^2)+O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|)
    \end{aligned}
\end{equation}
Hence if $\sqrt{p}\|\beta\|^2\to 0$ and $p/n^2\to \infty$, CLT holds.

If $\|\beta\|^2\to \infty$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{\frac{A_1}{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(\sqrt{n}\|\beta\|)+O_P(n))O_P(\frac{n}{p}\|\beta\|^2)-(O_P(\frac{\sqrt{n}}{p}\|\beta\|)+O_P(\frac{n}{p}))O_P(n\|\beta\|^2)}{\frac{n^2}{p^2}\|\beta\|^4}
        \Big|\\
        &=O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|^{-1})+O_P(\sqrt{p}\|\beta\|^{-2})
    \end{aligned}
\end{equation}
Hence if $\frac{n}{p}\|\beta\|^2\to \infty$ and $p/n^2\to \infty$, CLT holds.
\subsection{circumstance 2}




\begin{equation}
    T=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }
\end{equation}

\section{Simulation Results}

% latex table generated in R 3.3.1 by xtable 1.8-2 package
% Fri Nov  4 21:37:08 2016
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{rrrrr}
%          \hline
%          $n$ & $p$ & $|\beta|^2$ & Chen & New \\ 
%            \hline
%            40 & 310 & 0.00 & 0.05 & 0.00 \\ 
%              40 & 310 & 0.04 & 0.12 & 0.04 \\ 
%                80 & 550 & 0.00 & 0.05 & 0.00 \\ 
%                  80 & 550 & 0.04 & 0.13 & 0.00 \\ 
%                             \hline
%    \end{tabular}
%    \caption{Sparse case, $T=20$}
%\end{table}

\section*{References}
\bibliography{mybibfile}

\end{document}
