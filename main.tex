\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,amsthm}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}
\begin{document}

\begin{frontmatter}

\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
%\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Introduction}
Suppose $X_1,\ldots,X_n$ are i.i.d.\ from $p$-dimensional normal distribution $N_{p}(\mu_X,\Sigma_X)$. 
Denote $X=(X_1,\ldots,X_n)$.
In this paper, it is assumed that $n<p$, that is, high dimension setting is considered.
Consider a linear regression model 
\begin{equation}\label{regModel}
    y=\beta_0 \textbf{1}_n+X^T \beta+\epsilon,
\end{equation}
     where $\textbf{1}_n$ is $n$ dimensional vector with all elements equal to $1$ and $\epsilon$ has distribution $N(0,\sigma^2 I_n)$.


%Denote by $\Sigma_X$ the covariance matrix of $X_i$ ($i=1,\ldots,n$).
Let $\Sigma_X=P\Lambda P^T$ be the spectral decomposition of $\Sigma_X$, where $\Lambda=\textrm{diag}(\lambda_1,\ldots,\lambda_p)$ with $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_p$ and $P$ is an orthogonal matrix.
In PCA context, it is assumed that $\Sigma_X$ is spiked, that is $\lambda_1\geq \lambda_2\geq \cdots \geq\lambda_r> \lambda_{r+1}=\cdots =\lambda_p$ for some $r>0$
(See~\cite{Cai2012Sparse}). Denote by $P_1$ the first $r$ column of $P$ and $P_2$ the last $p-r$ column of $P$. The aim of PCA is to estimate $P_1$.
In this paper, we allow $\Sigma_X$ to be either spiked or non-spiked. Non-spike means that there's no principal component ($r=0$). That is, $\lambda_1=\cdots = \lambda_p$. Spike means that there's $r$ principal components for $r>0$. 
%That is, $\lambda_1\geq \lambda_2\geq \cdots \lambda_r\geq \lambda_{r+1}=\cdots =\lambda_p$. Denote by $P_1$ the first $r$ column of $P$ and $P_2$ the last $p-r$ column of $P$.
In either case, let $\lambda=\lambda_{r+1}=\cdots=\lambda_{p}$.

If $\Sigma_X$ is indeed spiked,
\begin{equation}
    \begin{aligned}
        y&=\beta_0 \textbf{1}_n+X^T P_1P_1^T\beta+X^T P_2 P_2^T\beta+\epsilon,
    \end{aligned}
\end{equation}
where $X^T P_1$ and $X^T P_2$ are independent.
PCR try to do regression between $y$ and $X^T P_1$. Since $P_1$ is not observed, it is substituted by an estimator $\tilde{P}_1$. 
Traditionally, PCR is a technique for analyzing multiple regression data that suffers from multicollinearity. Recently, PCR is a practical method to deal with high dimensional regression. If $p<n$, the full multicollinearity phenomenon shows up even if predictors are independent. 
It calls for a test procedure to justify the appropriateness of PCR.\ 
To be precise, we consider testing the hypotheses
\begin{equation}
    H:\Sigma\,\,\textrm{is non-spiked or}\,\,\Sigma\,\,\textrm{is spiked and}\,\, P_1^T \beta =0 
\end{equation}
versus
\begin{equation}
    K: \Sigma\,\,\textrm{is spiked and}\,\, P_1^T \beta \neq 0.
\end{equation}

If $P_1$ is observed, then the problem is reduced to testing an ordinary regression model. However, it's not the case. In fact, the classical $F$-test statistic for the regression between $y$  and  $X^T \tilde{P}_1$ may not be a good choice for at least three reasons:
\begin{enumerate}
    \item
From equation
\begin{equation}
    \begin{aligned}
        y&=\beta_0 \textbf{1}_n+X^T \tilde{P}_1\tilde{P}_1^T\beta+X^T (I_p -\tilde{P}_1 \tilde{P}_1^T)\beta+\epsilon,
    \end{aligned}
\end{equation}
we can see that the $F$-test suffers from Endogeneity.
\item
The estimator of $P_1$ may not be consistent in high dimension. Moreover, $\Sigma_X$ may not be spiked and, as a result, there's no principal component.
\item
Even if there's additional information or data to estimate $P_1$, we will never know weather we estimate $P_1$ well enough such that the $F$-test is valid.% Moreover, the number of principal components $r$ should be chosen. If sparse PCA estimator is adopted, more parameters should be specified by user, which may be inappropriate for an testing procedure.
\end{enumerate}



\cite{Zhao2016A} proposed a generalized likelihood ratio test (GLRT) for testing high dimensional mean values.
Roughly speaking, GLRT projects data to lower dimension by a direction $a$ such that likelihood ratio is maximized. GLRT is likelihood based, it can be regarded as a generalization of classical LRT in high dimension setting.

In this paper we apply the GLRT method to the problem of testing the significance of PCR.\
\section{New Test}
It can be seen that ${(X_1^T,y_1)}^T$,\ldots,${(X_n^T, y_n)}^T$ are i.i.d.\ from $N_{p+1}(\mu,\Sigma)$,
where
$\mu={(\mu_X^T,\beta_0)}^T$ and
\begin{equation}
    \Sigma=\begin{pmatrix}
        \Sigma_X &  \Sigma_X \beta \\
        \beta^T \Sigma_X & \beta^T \Sigma_X\beta+\sigma^2
    \end{pmatrix}.
\end{equation}
%$X_i\in \mathbb{R}^p$ and $Y_i\in \mathbb{R}$. 
%Denote $X=(X_1,\ldots,X_n)$, $Y={(Y_1,\ldots,Y_n)}^T$.
% We assume $n<p$. 
Denote $\Theta: (\mu,\Sigma)$.
%Consider the hypothesis $H: \textrm{Cov}(X,y)=0$.
Define the hypothesis $H_a$ by
\begin{equation}\label{Ha}
    H_a: \textrm{Cov}(a^T X_i, y_i)=0,
\end{equation}
where $a\in \mathbb{R}^p$ and $a^T a=1$.
%Then we have $H=\cap_a H_a$.
Let
\begin{equation}
    S=\frac{1}{n}\sum_{i=1}^n \begin{pmatrix}
        (X_i-\bar{X}){(X_i-\bar{X})}^T&(X_i-\bar{X}){(y_i-\bar{y})}^T\\
        (y_i-\bar{y}){(X_i-\bar{X})}^T&(y_i-\bar{y}){(y_i-\bar{y})}^T\\
        \end{pmatrix}=\begin{pmatrix}S_{XX} & S_{Xy} \\ S_{yX}& S_{yy}\end{pmatrix},
\end{equation}
and
\begin{equation}
    S_a=\begin{pmatrix}
        a^T& 0\\
        0& 1
    \end{pmatrix}
    S\begin{pmatrix}
        a& 0\\
        0& 1
    \end{pmatrix},
    \Sigma_a=\begin{pmatrix}
        a^T& 0\\
        0& 1
    \end{pmatrix}
    \Sigma\begin{pmatrix}
        a& 0\\
        0& 1
    \end{pmatrix}.
\end{equation}
     The likelihood function of $(a^T X_i,y_i)$, $i=1,\ldots,n$,  is 
\begin{equation}
L_a(\theta;X,Y)={(2\pi)}^{-n}|\Sigma_a|^{-n/2}\exp{(-\frac{1}{2}\textrm{tr}\Sigma_a^{-1}S_a)}.
\end{equation}
Then the maximum likelihood is
    \begin{equation}\label{like1}
        L(a)=\sup_{\theta\in \Theta}L_a(\theta ;X,Y)={(2\pi)}^{-n} |S_a|^{-n/2}e^{-n}.
\end{equation}
    If $|S_a|=0$, then~\ref{like1} is interpreted as $+\infty$.
    Similarly, the maximum likelihood under $H_a$ is
\begin{equation}
    L(a)=\sup_{\theta\in H}L_a(\theta ;X,Y)={(2\pi)}^{-n} |a^T S_{XX}aS_{yy}|^{-n/2}e^{-n}.
\end{equation}

    In~\cite{Zhao2016A}, GLRT is defined as
    \begin{equation} \label{GLRTdef}
        \min_{L(a)=+\infty} L_H(a)\quad s.t.\quad a^T a=1.
    \end{equation}
    The idea of GLRT is to find $a$ such that $L(a)=+\infty$ and $L_{H}(a)<+\infty$ as small as possible such that the discrepancy between the likelihood values $L(a)$ and $L_H (a)$ is maximized. We call the direction $a^*$ obtained  by~\eqref{GLRTdef} the GLRT direction.
    
    From the expression of $L(a)$ and $L_H(a)$,  $a^*$ is equal to% such that $a^T S_{XX} a$ is maximized subject to $|S_a|=0$.
    \begin{equation} \label{GLRTvar}
        a^*=\textrm{argmax}_{a^T a=1} a^T S_{XX} a \quad s.t.\quad |S_a|=0.
    \end{equation}
    Such a direction $a^*$ can be expected to make $|\Sigma_a|$ small and $a^T \Sigma_{XX} a$ large.
    That is, the variance of $a^T X_i$ is large and $a^T X_i$ and $y_i$ are highly correlated.
    If $X_i$ has certain principal components which are correlated to $y_i$, the direction $a^*$ is expected to be close to corresponding principal directions.


    Next we solve the optimization problem~\eqref{GLRTvar}. Let $Q_n=I_n-\frac{1}{n}\textbf{1}_n\textbf{1}_n^T$. Denote by $Q_n=WW^T$ the rank decomposition of $Q_n$, where $W_n$ is an $n\times (n-1)$ matrix with $W^T W=I_{n-1}$. 
    Then $|S_a|=0$ is equivalent to $a^T X Q X^T a y^T Q y={(a^T X Q y)}^2$ and is equivalent to $W^T X^T a=W^T y k$ for some $k\in \mathbb{R}$. It follows that
    \begin{equation}
        a=XW{(W^T X^T X W)}^{-1} W^T y k + (I-XW{(W^T X^T X W)}^{-1}W^T X^T)a.
    \end{equation}
Since $a^T a=1$,
    \begin{equation}\label{gougu}
        k^2 y^T W{(W^T X^T X W)}^{-1} W^T y + a^T(I-XW{(W^T X^T X W)}^{-1}W^T X^T)a=1.
    \end{equation}
Note that 
    \begin{equation}
        L_H (a)\propto {(a^T X Q X^T a y^T Q y)}^{-n/2}={\big(k^2{(y^T Q_n y)}^2\big)}^{-n/2}.
    \end{equation}
    To make $L_H(a)$ minimized, we should maximize $k^2$. So the second term of~\ref{gougu} should be $0$. That is

    \begin{equation}
        a=XW{(W^T X^T X W)}^{-1} W^T y k 
    \end{equation}
    Hence 
    \begin{equation}
        k^2=\frac{1}{y^T W{(W^T X^T X W)}^{-1} W^T y},
    \end{equation}
    and 
    \begin{equation}
        L_H (a)\propto {(a^T X Q X^T a y^T Q y)}^{-n/2}={\Big(\frac{{(y^T Q_n y)}^2}{y^T W{(W^T X^T X W)}^{-1} W^T y}\Big)}^{-n/2}.
    \end{equation}
After homogenization, we define
\[
    T=\frac{y^T Q_n y}{
        y^T W{(W^T X^T X W)}^{-1}W^T y
    }.
    \]
If $T$ is large, we reject $H$.

%\section{Main}

%As we have pointed out, test statistic $T$ is expected to be large when $y_i$ is correlated to some principal components of $X_i$, and be small otherwise. Hence $T$ is suitable for testing the significance of PCR.\@




%The test statistic is

%\[
%    T=\frac{
%        (\textbf{1}_n^T(X^T X)^{-1}Q_n Y)^2
%    }{
%        \hat{\sigma}^2
%        \textbf{1}_n^T(X^T X)^{-1}Q_n (X^T X)^{-1}\textbf{1}_n
%    }.
%\]
%where $Q_n=I_n-\frac{1}{n}\textbf{1}_n\textbf{1}_n^T$ and
%\[
%    \hat{\sigma}^2=\frac{1}{n-2} Y^T Q_n\Big[
%        I_n-\frac{(X^T X)^{-1}\textbf{1}_n\textbf{1}_n^T(X^T X)^{-1}}{
%        \textbf{1}_n^T(X^T X)^{-1}Q_n(X^T X)^{-1}\textbf{1}_n
%        }
%%        \Big]Q_n Y
%\] 


%or equivalently
%\[
    %\frac{y^T Q_n y}{
        %y^T Q_n{(X^T X)}^{-1}Q_n y-{(y^T Q_n{(X^T X)}^{-1}\textbf{1}_n)}^2/(\textbf{1}_n{(X^T X)}^{-1}\textbf{1}_n)
    %} 
    %\]



\section{Main Results}


Let $\tilde{y}=W^T y$, $\tilde{X}=XW$, $\tilde{\epsilon}=W^T \epsilon$. Then the columns of $\tilde{X}$ are i.i.d.\  distributed as $N(0,\Sigma_X)$, $\tilde{\epsilon}\sim N(0,\sigma^2 I_{n-1})$ and $\tilde{y}=\tilde{X}^T \beta + \tilde{\epsilon}$.
The test statistic can be written as
\[
    T=\frac{\tilde{y}^T \tilde{y}}{
        \tilde{y}^T{(\tilde{X}^T \tilde{X})}^{-1}\tilde{y}
    }.
    \]
We make the following assumption.
\begin{assumption}\label{assumptiona}
    Assume model~\eqref{regModel} holds with the columns of $X$ are i.i.d.\  distributed as $N(\mu_X,\Sigma_X)$, $\epsilon\sim N(0,\sigma^2 I_n)$ and $\sigma^2$ is fixed as $n,p\to \infty$.
\end{assumption}
\begin{assumption}\label{assumptionb}
    Assume the eigenvalues of $\Sigma_X$ satisfy $\lambda_1\geq \lambda_2\geq \cdots \geq\lambda_r> \lambda_{r+1}=\cdots =\lambda_p=\lambda$, where $r\geq 0$ and $\lambda>0$ are fixed as $n,p\to \infty$, $\lambda_1\asymp \lambda_r$ and $p^{1/2}/\lambda_r \to 0$. We say there's no principal component if $r=0$, that is $\lambda_1=\cdots=\lambda_p$.
\end{assumption}

The null hypotheses $H$ is the union of two disjoint hypothesis $H=\cup_{i=1}^2 H_i$, where $H_1$: There's no principal component; and $H_2$:  There's $r$ principal components with $r>0$ and $P_1^T\beta=0$. Under $H_1$ we have the following theorem

\begin{theorem}\label{theorema}
    Suppose Assumption~\ref{assumptiona} and~\ref{assumptionb} hold.
    Assume $p/n \to  \infty$ and $H_1$ is true. Then
            \begin{equation}
                T/(\lambda p) \xrightarrow{P} 1.
            \end{equation}
    If $\|\beta\|=o(\frac{\sqrt{n}}{\sqrt{p}})$ or $\|\beta\|^{-1}=o(\frac{\sqrt{n}}{\sqrt{p}})$, then for $\alpha\in (0,1)$ we have
\begin{equation}
    \begin{aligned}
    \Pr\Big(\frac{T-\lambda p}{\lambda\sqrt{2p}}\geq \Phi^{-1}(1-\alpha)\Big)\leq \alpha.
    \end{aligned}
\end{equation}
\end{theorem}

Under $H_2$ we have a similar theorem with one more condition $p=o(n^2)$.
\begin{theorem}\label{theoremb}
    Suppose Assumption~\ref{assumptiona} and~\ref{assumptionb} hold.
    Assume $p/n \to  \infty$, $p/n^2 \to 0$ and $H_2$ is true. Then
            \begin{equation}
                T/(\lambda p) \xrightarrow{P} 1.
            \end{equation}
    If $\|P_2^T\beta\|=o(\frac{\sqrt{n}}{\sqrt{p}})$ or $\|P_2^T\beta\|^{-1}=o(\frac{\sqrt{n}}{\sqrt{p}})$, then for $\alpha\in (0,1)$ we have
\begin{equation}
    \begin{aligned}
    \Pr\Big(\frac{T-\lambda p}{\lambda\sqrt{2p}}\geq \Phi^{-1}(1-\alpha)\Big)\leq \alpha.
    \end{aligned}
\end{equation}
\end{theorem}


Under $K$, to simplify the proof, we assume $\beta$ is generated from a normal prior distribution before data are generated. We have the following theorem:
\begin{theorem}\label{theoremc}
   Suppose Assumption~\ref{assumptiona} and~\ref{assumptionb} hold.
    Assume $p/n \to  \infty$ and $K$ is true. Assume $\beta$ has prior distribution $N(0,\sigma_\beta^2 I_p)$. Then

\end{theorem}
\section{Appendix}

For random variable $\xi$ and $\eta$, we write $\xi\sim \eta$ when $\xi$ and $\eta$ have the same distribution. For two sequences of positive random variables $\xi_n$ and $\eta_n$, we write $\xi_n\asymp \eta_n$ if $\Pr(c\eta_n\leq\xi_n\leq C\eta_n)\to 1$ for some positive $c$ and $C$.

\begin{equation}\label{Tdecom}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}.
    \end{aligned}
\end{equation}
\subsection{Lemma}

\begin{lemma}\label{matrixInv}
    Suppose $A$ is an $n\times n$ full rank symmetric matrix. And let 
    \begin{equation}
        A=\begin{pmatrix}
            A_{11} & A_{12}\\
            A_{21} & A_{22}
        \end{pmatrix},
    \end{equation}
    where $A_{11}$ is a real number, $A_{12}$ is a $1\times (n-1)$ matrix, $A_{21}$ is a $(n-1)\times 1$ matrix and $A_{22}$ is a $(n-1)\times (n-1)$ matrix. Denote $A_{11\cdot 2}=A_{11}-A_{12}A_{22}^{-1}A_{21}$. Then we have
    \begin{equation}
        {(A^{-1})}_{11}=A_{11\cdot 2}^{-1}
    \end{equation}
\end{lemma}

\begin{lemma}\label{baiyin}
    Suppose $B=\frac{1}{q}V V^T$ where V is an $p\times q$ random matrix composed of i.i.d.\  random variables with zero mean, unit variance and finite fourth moment.
    As $q\to \infty$ and $p/q \to c\in [0,+\infty)$, the largest and smallest nonzero eigenvalues of $B$ converge almost surely to ${(1+\sqrt{c})}^2$ and ${(1-\sqrt{c})}^2$, respectively.
\end{lemma}
Lemma~\ref{baiyin} is known as the Bai-Yin's law~\cite{Bai1993Limit}.

\begin{lemma}\label{wangPCA}
    Let $Z_1,\ldots,Z_{n+1}$ i.i.d.\ distributed as $N(0,I_p)$.
    $\Lambda=\textrm{diag}(\lambda_1,\ldots,\lambda_p)$, where $\lambda_1\geq \cdots \lambda_r$ and $\lambda_{r+1}=\cdots =\lambda_p=\lambda$.
    $\limsup_{n\to \infty} \lambda_1/\lambda_r<\infty$, $\lambda_1/\sqrt{p}\to \infty$.
    Suppose $p=o(n^2)$.
    Denote $Z=(Z_1,\ldots,Z_n)$. Let $\hat{V}$ be the first $r$ eigenvectors of $\Lambda^{1/2}Z Z^T\Lambda^{1/2}$, $V=(e_1,\ldots,e_r)$. Then
    \begin{equation}
        Z_{n+1}^T \Lambda^{1/2}(VV^T -\hat{V}\hat{V}^T)\Lambda^{1/2}Z_{n+1}=o(\sqrt{p})
    \end{equation}
\end{lemma}
Lemma~\ref{wangPCA} is from Wang Rui's paper.

\begin{lemma}\label{uniformLemma}
    Suppose $F_n(\cdot)$ and $F(\cdot)$ are distribution functions and $F_n\xrightarrow{L}F$, then
    \begin{equation}
        \sup_x |F_n(x)-F(x)|\to 0.
    \end{equation}
    See Exercise 3.2.9 of~\cite{durrett2010probability}.
\end{lemma}
\begin{lemma}\label{svdLemma}
    Suppose $Z$ is an $p\times n$ ($p\geq n$) random matrix with all elements i.i.d.\ distributed as $N(0,1)$. Denote by $Z=U\Lambda V^T$ the singular value decomposition (SVD) of $Z$, where $U$ is a $p\times n$ orthogonal matrix, $\Lambda$ is an $n\times n$ diagonal matrix and $V$ is an $n\times n$ orthogonal matrix. Then $U$, $\Lambda$ and $V$ are independent. (See, e.g.,~\cite{Eaton1983Multivariate})
\end{lemma}

\begin{lemma}\label{wangInverse}
    Let $A$ be an $n\times n$ symmetric positive semi-definite matrix with rank $r$. Denote by $A=P\Lambda P^T$ the spectral decomposition of $A$, where $P$ is an $n\times r$ orthogonal matrix and $\Lambda=\textrm{diag}(\lambda_1,\ldots,\lambda_r)$ is an $r\times r$ diagonal matrix with $\lambda_1\geq \lambda_2\geq\cdots \lambda_r >0$. Then we have

\begin{equation}
    {(A+I_{n})}^{-1}\geq I_n- P P^T
\end{equation}
\end{lemma}
\begin{proof}
    Let $\tilde{P}$ be an $n\times n$ orthogonal matrix such that $P$ is the first $r$ columns of $\tilde{P}$. And let $\tilde{\Lambda}=\textrm{diag}(\lambda_1,\ldots,\lambda_r,0,\ldots,0)$ be an $n\times n$ matrix. Then $P\Lambda P^T=\tilde{P}\tilde{\Lambda} \tilde{P}^T$, and
    \begin{equation}
        \begin{aligned}
            {(A+I_{n})}^{-1}&=\tilde{P} {(\tilde{\Lambda} + I_n)}^{-1} \tilde{P}^T\\
            &=\tilde{P}\textrm{diag}({(\lambda_1+1)}^{-1},\ldots,{(\lambda_r+1)}^{-1},1,\ldots,1)  \tilde{P}^T\\
            &\geq\tilde{P}\textrm{diag}(0,\ldots,0,1,\ldots,1)  \tilde{P}^T\\
            &=I_n-PP^T
        \end{aligned}
    \end{equation}
\end{proof}

\subsection{circumstance 1}
\begin{assumption}
    $r=0$.
\end{assumption}
\begin{assumption}
    $n^2/p\to 0$.
\end{assumption}

\subsubsection{First randomization of $\beta$}
Independent of data, generate a random $p$ dimensional orthonormal matrix $O$ with Haar invariant distribution. And 

\begin{equation}
    T=\frac{{(O\beta)}^T O\tilde{X}{(O\tilde{X})}^T O\beta+
        2{(O\beta)}^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}{(O\tilde{X})}^T \beta+
        2{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}
    }
\end{equation}

Note that conditioning on $O$, $O\tilde{X}$ is a random matrix with each entry independently distributed as $N(0,\lambda)$. Hence $O$ is independent of $O\tilde{X}$. Observe also that $O\beta/\|\beta\|$ is uniformly distributed on the unit ball.  We can without loss of generality and assume that $\beta/\|\beta\|$ is uniformly distributed on the surface unit ball in~\eqref{Tdecom}.

\subsubsection{Second randomization of $\beta$}
Independent of data, generate $R>0$ with $R^2$ distributed as $\chi^2_{p}$. Then $\xi=R\beta/\|\beta\|$ distributed as $N_p(0,I_p)$.
Note that conditioning on $\tilde{X}$, $\eta={(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{X}^T \xi$ is distributed as $N_{n-1}(0,I_{n-1})$. Hence $\eta$ is independent of $\tilde{X}$.

Then

\begin{equation}
    \begin{aligned}
        T&=\frac{{(\|\beta\|/R)}^2\xi^T \tilde{X}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=
        \frac{{(\|\beta\|/R)}^2\eta^T \tilde{X}^T\tilde{X} \eta+
        2(\|\beta\|/R)\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\eta^T\eta+
        2(\|\beta\|/R)\eta^T{(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}


\subsubsection{Step 3: CLT}
Similar to the derivation of the distribution of Hotelling's $T^2$ statistic.

Now we deal with 
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}=\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }
    \end{aligned}
\end{equation}
Let $O$ be an $(n-1)\times (n-1)$ orthogonal matrix satisfies 
\[
    O\tilde{\epsilon}=
    \begin{pmatrix}
        \|\tilde{\epsilon}\|\\
        0\\
        \ldots\\
        0
    \end{pmatrix}.
    \]
Then
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}=\frac{{(O\tilde{\epsilon})}^T O\tilde{\epsilon}
        }{{(O\tilde{\epsilon})}^T{\big({(\tilde{X}O^T)}^T\tilde{X}O^T\big)}^{-1} O\tilde{\epsilon}
        }.
    \end{aligned}
\end{equation}
It can be seen that $\tilde{X}O^T$ has the same distribution as $\tilde{X}$ and is independent of $O$. We have

\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}\sim 
        \frac{1}{{({(\tilde{X}^T\tilde{X})}^{-1})}_{11}}.
    \end{aligned}
\end{equation}
%where ${({(\tilde{X}^T\tilde{X})}^{-1})}_{11}$ is the first element of ${(\tilde{X}^T\tilde{X})}^{-1}$.
Apply Lemma~\ref{matrixInv}, we have 
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}\sim 
        {(\tilde{X}^T\tilde{X})}_{11\cdot 2}.
    \end{aligned}
\end{equation}
Since $\tilde{X}^T \tilde{X}\sim \textrm{Wishart}_{n-1}(\lambda I_{n-1},p)$, ${(\tilde{X}^T\tilde{X})}_{11\cdot 2}\sim \lambda \chi^2_{p-n+2}$. Hence $A_3/B_3\asymp p$ and
\begin{equation}
    \begin{aligned}
        \frac{A_3/B_3-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        }\xrightarrow{\mathcal{L}}N(0,1),
    \end{aligned}
\end{equation}
by CLT.\
%But
%\begin{equation}
    %\begin{aligned}
        %\frac{A_3/B_3-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        %}&=
        %\frac{\sqrt{p}}{\sqrt{p-n+2}}
        %\frac{A_3/B_3-{\lambda(p-n+2)}}{\lambda\sqrt{2p}
        %}\\
        %&=
        %\frac{\sqrt{p}}{\sqrt{p-n+2}}
        %\Big(\frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        %}+\frac{(n-2)}{\sqrt{2p}}\Big).
    %\end{aligned}
%\end{equation}
%By Slutsky Theorem, if $n^2/p\to 0$, we have
%\begin{equation}
    %\frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        %}\xrightarrow{\mathcal{L}}N(0,1)
%\end{equation}

Similar technique can deal with $A_1/B_1$:
\begin{equation}
    \begin{aligned}
        \frac{A_1}{B_1}=\frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}&\sim {(\tilde{X}^T\tilde{X})}_{11}\sim \lambda \chi^2_{p}
\\
    \end{aligned}.
\end{equation}
Hence $A_1/B_1\asymp p$ and 
\begin{equation}
    \begin{aligned}
        \frac{A_1/B_1
    -\lambda p}{\lambda\sqrt{2p}
        }\xrightarrow{\mathcal{L}}N(0,1),
    \end{aligned}
\end{equation}
by CLT.\@
\subsubsection{step 4}
It's obvious that $A_3\asymp n$ and $B_1\asymp \frac{n}{p}\|\beta\|^2$. We already have $A_1/B_1\asymp p$ and $A_3/B_3\asymp p$. It follows that $A_1\asymp n\|\beta\|^2$ and $B_3\asymp n/p$. And

\begin{equation}
    \begin{aligned}
        A_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{np})\\
        &=O_P(\sqrt{n}\|\beta\|),
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        B_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}^{-1}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{n/p})\\
        &=O_P(\frac{\sqrt{n}}{p}\|\beta\|).
    \end{aligned}
\end{equation}

Note that 
\begin{equation}
A_2=O_P(\frac{1}{\sqrt{n}})n\|\beta\|=O_P(\frac{1}{\sqrt{n}})\sqrt{A_1}\sqrt{A_3}
        \leq O_P(\frac{1}{\sqrt{n}})(A_1+A_3)
\end{equation}
Similarly we have $B_2=O_P(\frac{1}{\sqrt{n}})(B_1+B_3)$. It follows that
\begin{equation}\label{TAB}
    T=\frac{A_1+A_3}{B_1+B_3}(1+O_P(\frac{1}{\sqrt{n}})).
\end{equation}

For every $\epsilon>0$, we have
\begin{equation}\label{myA1A3B1B3}
    \begin{aligned}
        &\Pr\Big(\frac{A_1+A_3}{B_1+B_3}\geq (\lambda p)(1+\epsilon)\Big)\\
        =&\Pr({A_1+A_3}\geq ({B_1+B_3})(\lambda p)(1+\epsilon))\\
        \leq&\Pr(A_1\geq B_1(\lambda p)(1+\epsilon))+
        \Pr(A_3\geq B_3(\lambda p)(1+\epsilon)).
    \end{aligned}
\end{equation}
But
\begin{equation}
    \frac{A_1}{\lambda p B_1}\xrightarrow{P}1\quad \textrm{and}\quad 
    \frac{A_3}{\lambda p B_3}\xrightarrow{P}1.
\end{equation}
It follows that~\eqref{myA1A3B1B3} tends to $0$. Similarly,
\begin{equation}
        \Pr\Big(\frac{A_1+A_3}{B_1+B_3}\leq (\lambda p)(1-\epsilon)\Big)\to 0.
\end{equation}
We have proved
\begin{equation}\label{largeNumberAB}
    \frac{A_1+A_3}{\lambda p(B_1+B_3)}\xrightarrow{P} 1.
\end{equation}
Together with~\eqref{TAB}, it follows that $T\xrightarrow{P} 1$.

%We can deduce that:
%If $\|\beta\|^2\to 0$, then
%\begin{equation}
    %\begin{aligned}
        %T&=
        %\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    %}{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        %}(1+o_P(1)),
    %\end{aligned}
%\end{equation}
%and $T/(\lambda p)\xrightarrow{P} 1$.
%If $\|\beta\|^2\to \infty$, then
%\begin{equation}
    %\begin{aligned}
        %T&=
        %\frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        %}{\eta^T\eta}(1+o_P(1)),
    %\end{aligned}
%\end{equation}
%and $T/(\lambda p)\xrightarrow{P} 1$.


\subsubsection{Step 5}

By Cauchy inequility,
    $\tilde{\epsilon}^T {(\tilde{X}^T\tilde{X})}^{-1} \tilde{\epsilon}
    \tilde{\epsilon}^T \tilde{X}^T\tilde{X} \tilde{\epsilon}\geq {(\tilde{\epsilon}^T \tilde{\epsilon})}^2$.
    Denote $B_4=\tilde{\epsilon}^T \tilde{X}^T\tilde{X} \tilde{\epsilon}$. Using similar technique as before, we have $B_4\asymp np $ and $(B_4/A_3-\lambda p)/(\lambda \sqrt{2p})\xrightarrow{\mathcal{L}}N(0,1)$.
    Together with~\eqref{TAB} and~\eqref{largeNumberAB}, we have
    \begin{equation}\label{ChangT}
        \begin{aligned}
            T&=\frac{A_1+A_3}{B_1+B_3}\frac{1+\frac{A_2}{A_1+A_3}}{1+\frac{B_2}{B_1+B_3}}\\
            &=\frac{A_1+A_3}{B_1+B_3}{(1+\frac{A_2}{A_1+A_3})}{(1-\frac{B_2}{B_1+B_3}(1+o_P(1)))}\\
            &=\frac{A_1+A_3}{B_1+B_3}{(1+(\frac{|A_2|}{A_1+A_3}+\frac{|B_2|}{B_1+B_3})(1+o_P(1)))}\\
            %&=\frac{A_1+A_3}{B_1+B_3}+\lambda p(\frac{|A_2|}{A_1+A_3}+\frac{|B_2|}{B_1+B_3})(1+o_P(1))\\
            &=\frac{A_1+A_3}{B_1+B_3}+O_P(p)(\frac{|A_2|}{A_1+A_3}+\frac{|B_2|}{B_1+B_3})\\
            &=\frac{A_1+A_3}{B_1+B_3}+O_P(p)(\frac{\sqrt{n}\|\beta\|}{n\|\beta\|^2+n}+\frac{\frac{\sqrt{n}}{p}\|\beta\|}{\frac{n}{p}\|\beta\|^2+\frac{n}{p}})\\
            &=\frac{A_1+A_3}{B_1+B_3}+O_P(\frac{p\sqrt{n}\|\beta\|}{n\|\beta\|^2+n})\\
            &\leq\frac{A_1+A_3}{B_1+A_3^2/B_4}+O_P(\frac{p\sqrt{n}\|\beta\|}{n\|\beta\|^2+n}).\\
        \end{aligned}
    \end{equation}
We deal with the two terms seperately.


\begin{equation}
    \begin{aligned}
        &\frac{\frac{A_1+A_3}{B_1+A_3^2/B_4}-{\lambda p}}{\lambda\sqrt{2p}}=&c\frac{{A_1}/{B_1}-\lambda p}{\lambda \sqrt{2p}}
        +(1-c)\frac{{B_4}/{A_3}-\lambda p}{\lambda \sqrt{2p}},
    \end{aligned}
\end{equation}
where 
\begin{equation}
    c=\frac{B_1}{B_1+A_3^2/B_4}\asymp \frac{\frac{n}{p}\|\beta\|^2}{\frac{n}{p}\|\beta\|^2+\frac{n}{p}}
    =\frac{\|\beta\|^2}{\|\beta\|^2+1}.
\end{equation}
Hence by Slutsky's theorem, we have
\begin{equation}
    \begin{aligned}
        &\frac{\frac{A_1+A_3}{B_1+A_3^2/B_4}-{\lambda p}}{\lambda\sqrt{2p}}\xrightarrow{\mathcal{L}} N(0,1),
    \end{aligned}
\end{equation}
if $\|\beta\|\to 0$ or $\|\beta\|\to \infty$.

To control the second term of~\eqref{ChangT}, we further require
\begin{equation}
\frac{\sqrt{np}\|\beta\|}{n\|\beta\|^2+n}\to 0.
\end{equation}
Equivalently, if $\|\beta\|\to 0$, we require $\|\beta\|=o(\frac{\sqrt{n}}{\sqrt{p}})$; if $\|\beta\|\to \infty$, we require $\|\beta\|^{-1}=o(\frac{\sqrt{n}}{\sqrt{p}})$.

If these conditions are satiesfied, we have 
\begin{equation}
    \Pr\Big(\frac{T-\lambda p}{\lambda\sqrt{2p}}\geq \Phi^{-1}(1-\alpha)\Big)\leq
    \alpha
\end{equation}




%If $\|\beta\|^2\to 0$, we have
%\begin{equation}
    %\begin{aligned}
        %&\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        %}-
    %\frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        %}\Big|\\
        %&=
        %\frac{1}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_3}{B_3}
        %\Big|\\
        %&=
        %\frac{1}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{(A_1+A_2)B_3-(B_1+B_2)A_3}{(B_1+B_2+B_3)B_3}
        %\Big|\\
        %&=
        %\frac{O_{P}(1)}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{(O_P(n\|\beta\|^2)+O_P(\sqrt{n}\|\beta\|))O_P(\frac{n}{p})-(O_P(\frac{n}{p}\|\beta\|^2)+O_P(\frac{\sqrt{n}}{p}\|\beta\|))O_P(n)}{n^2/p^2}
        %\Big|\\
        %&=O_P(\sqrt{p}\|\beta\|^2)+O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|)
    %\end{aligned}
%\end{equation}
%Hence if $\sqrt{p}\|\beta\|^2\to 0$ and $\frac{p}{n}\|\beta\|^2\to 0$, CLT holds.
%
%On the other hand. If $\|\beta\|^2\to \infty$, we have
%\begin{equation}
    %\begin{aligned}
        %&\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        %}-
    %\frac{{A_1}/{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        %}\Big|\\
        %&=
        %\frac{1}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        %\Big|\\
        %&=
        %\frac{1}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        %\Big|\\
        %&=
        %\frac{O_{P}(1)}{\lambda\sqrt{2p}}
        %\Big|
        %\frac{(O_P(\sqrt{n}\|\beta\|)+O_P(n))O_P(\frac{n}{p}\|\beta\|^2)-(O_P(\frac{\sqrt{n}}{p}\|\beta\|)+O_P(\frac{n}{p}))O_P(n\|\beta\|^2)}{\frac{n^2}{p^2}\|\beta\|^4}
        %\Big|\\
        %&=O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|^{-1})+O_P(\sqrt{p}\|\beta\|^{-2})
    %\end{aligned}
%\end{equation}
%Hence if $\frac{n}{p}\|\beta\|^2\to \infty$ and $\frac{1}{\sqrt{p}}\|\beta\|^2\to \infty$, CLT holds.
\subsection{circumstance 2}
\begin{assumption}
$P_1^T \beta=0$.
\end{assumption}


\begin{equation}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{\beta^T P_2 P_2^T \tilde{X}\tilde{X}^T P_2 P_2^T\beta+
        2\beta^T P_2 P_2^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T P_2 P_2^T\tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T\beta+
        2\beta^T P_2 P_2^T\tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}
\subsubsection{Step 1}
 Like before, we have $A_3/B_3\sim {(\tilde{X}^T\tilde{X})}_{11\cdot 2}$. Denote by $\Lambda=\textrm{diag} (\lambda_1,\ldots,\lambda_p)$. Let $Z=(Z_1,\ldots,Z_p)$ be a $n-1\times p$ matrix with all elements independently distributed as $N(0,1)$. Let $Z_{(1)}$ and $Z_{(2)}$ be the first $1$ row and last $n-2$ rows of $Z$, that is
\[
    Z=\begin{pmatrix} 
        Z_{(1)}\\
        Z_{(2)}
    \end{pmatrix}.
    \]
Then
\begin{equation}
    \begin{aligned}
        \tilde{X}^T\tilde{X}&\sim Z\Lambda Z^T\\
        &=\begin{pmatrix}
            Z_{(1)}\Lambda Z_{(1)}^T & Z_{(1)}\Lambda Z_{(2)}^T\\
            Z_{(2)}\Lambda Z_{(1)}^T & Z_{(2)}\Lambda Z_{(2)}^T\\
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Hence

\begin{equation}
    \begin{aligned}
        A_3/B_3&\sim Z_{(1)}\Lambda Z_{(1)}^T-Z_{(1)}\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda Z_{(1)}^T\\
        &=Z_{(1)}\Lambda^{1/2}\big(I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2} \big)\Lambda^{1/2}Z_{(1)}^T\\
        &\leq Z_{(1)}\Lambda^{1/2}\big(I_p -\hat{V}\hat{V}^T\big)\Lambda^{1/2}Z_{(1)}^T,
    \end{aligned}
\end{equation}
where $\hat{V}$ is the first $r$ eigenvectors of $\Lambda^{1/2}Z_{(2)}^T Z_{(2)}\Lambda^{1/2}$. From PCA theory (see~\cite{Cai2012Sparse}), $\hat{V}\hat{V}^T$ is a good estimator of population principal space $VV^T$ even in high dimensional setting. Here 
 $V=(e_1,\ldots, e_r)$, where $e_i$ is the vector with all elements equal to $0$ but the $i$th equal to $1$. Note that we have required $p=o(n^2)$. Then by lemma~\ref{wangPCA},
\begin{equation}
    Z_{(1)}\Lambda^{1/2}\big(VV^T -\hat{V}\hat{V}^T\big)\Lambda^{1/2}Z_{(1)}^T=o(\sqrt{p}).
\end{equation}

Note that
\begin{equation}
    Z_{(1)}\Lambda^{1/2}\big (I-VV^T) \Lambda^{1/2}Z_{(1)}^T\sim\lambda \chi^2_{p-r}
\end{equation}

Hence $A_3/B_3\leq \lambda\chi^2_{p-r}+o(\sqrt{p})$.

On the other hand, the non-zero eigenvalues of $\Lambda^{1/2}\big(I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2} \big)\Lambda^{1/2}$ is no less than that of $\lambda(I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2})$.
Hence $A_3/B_3\geq \lambda \chi^2_{p-n+2}$.

It follows that $A_3/B_3\asymp p$ if $p/n\to \infty$.


%\textbf{Another proof:}

%\begin{equation}
    %\begin{aligned}
        %{B_3}\leq \tilde{\epsilon}^T{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\asymp \frac{n}{p-n+2}
    %\end{aligned}
%\end{equation}
%
%To get the lower bound, let $P_2^T \tilde{X}=U_2 D_2 V_2^T$ be the SVD of $P_2^T \tilde{X}$, where $U_2$ is a $(p-r)\times (n-1)$ orthonormal matrix, $D_2$ is a $(n-1)\times(n-1)$ diagonal matrix and $V_2$ is a $(n-1)\times (n-1)$ orthonormal matrix. Then
%
%\begin{equation}
    %\begin{aligned}
        %{B_3}&= \tilde{\epsilon}^T{(\tilde{X}^T P_1 P_1^T\tilde{X}+\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T{(\tilde{X}^T P_1 P_1^T\tilde{X}+V_2 D_2^2 V_2^T)}^{-1}\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T V_2 D_2^{-1}{(D_2^{-1} V_2^T\tilde{X}^T P_1 P_1^T\tilde{X}V_2 D_2^{-1}+I_{n-1})}^{-1} D_2^{-1} V_2^T\tilde{\epsilon}\\
        %&\geq \tilde{\epsilon}^T V_2 D_2^{-1}(I_{n-1}-U^* U^{*T}) D_2^{-1} V_2^T\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{\epsilon}- \tilde{\epsilon}^T V_2 D_2^{-1}U^* U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}\\
%\end{aligned}
%\end{equation}
%for some $(n-1)\times r$ random matrix $U^*$. But
%\begin{equation}
    %\begin{aligned}
        %\tilde{\epsilon}^T V_2 D_2^{-1}U^* U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}&=\textrm{tr}(U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}\tilde{\epsilon}^T V_2 D_2^{-1}U^*)\\
        %&\leq \lambda_{\max}(V_2^T \tilde{\epsilon}\tilde{\epsilon}^T V_2) \textrm{tr}(U^{*T} D_2^{-2}U^*)\\
        %&\leq \|\tilde{\epsilon}\|^2 r {(\lambda_{\min}(D_2))}^{-2}\\
        %&\asymp \frac{n}{p}
    %\end{aligned}
%\end{equation}
%
%Hence 
%\begin{equation}
    %{B_3}= \tilde{\epsilon}^T{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}(1+O_P(1/p))
%\end{equation}
%Hence
%\begin{equation}
    %\frac{A_3}{B_3}= \lambda \chi^2_{p-r-n+2}(1+O_P(1/p))
%\end{equation}
%Hence
%\begin{equation}
    %\frac{{A_3}/{B_3}-\lambda(p-r-n+2)}{\lambda \sqrt{2(p-r-n+2)}}\xrightarrow{\mathcal{L}}N(0,1).
%\end{equation}
%There's no restriction between $n$ and $p$!!! Is there anything wrong? If no, it can be generalized to Xu and Zhao's test!!!


\subsubsection{Step 2: $B_1$ and $B_2$}

Note that $P_2^T \tilde{X}$ is an $(p-r)\times (n-1)$ matrix with all elements independently distributed as $N(0,\lambda)$. Similar to non-spiked circumstance, we have $A_1\asymp n\|P_2^T\beta\|^2$, $A_2=O_P(\sqrt{n}\|P_2^T \beta\|)$, $A_3\asymp n$ and $B_3\asymp n/p$.

Next we deal with $B_1$.
Let $P_2^T \tilde{X}=U_2 D_2 V_2^T$ be the SVD of $P_2^T \tilde{X}$, where $U_2$ is a $(p-r)\times (n-1)$ orthonormal matrix, $D_2$ is a $(n-1)\times(n-1)$ diagonal matrix and $V_2$ is a $(n-1)\times (n-1)$ orthonormal matrix. Without loss of generality, we can assume $P_2^T \beta/\|P_2^T\beta\|$ is uniformly distributed on the surface of unit ball.

$B_1$ has the following upper bound:
\begin{equation}\label{upperbound}
    \begin{aligned}
        B_1&\leq \beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta\\
        &=\beta^T P_2 U_2 U_2^T P_2^T \beta\\
        %&\asymp \frac{n-1}{p-r}\|P_2^T\beta\|^2.
    \end{aligned}
\end{equation}
Independent of $P_2^T\beta/\|P_2^T \beta\|$ and $U_2$, we generate $R\sim \chi^2_{p-r}$. Then we have
\begin{equation}
    \sqrt{R}\frac{P_2^T \beta}{\|P_2^T \beta\|} \sim N_{p-r}(0,I_{p-r}).
\end{equation}
Hence
\begin{equation}
    \begin{aligned}
        &\beta^T P_2 U_2 U_2^T P_2^T \beta\\
        &=\frac{\sqrt{R}\beta^T P_2}{\|P_2 \beta^T\|} U_2 U_2^T \frac{\sqrt{R}P_2^T \beta}{\|P_2^T \beta\|}\frac{1}{R}\|P_2^T \beta\|^2\\
        &\asymp \frac{n-1}{p-r}\|P_2^T\beta\|^2.
    \end{aligned}
\end{equation}

To get the lower bound, note that
\begin{equation}
    \begin{aligned}
        B_1&= \beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_1 P_1^T \tilde{X}+\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta\\
        &= \beta^T P_2 U_2 D_2 V^T_2 {(\tilde{X}^T P_1 P_1^T \tilde{X}+V_2 D_2^2 V_2^T)}^{-1}V_2 D_2 U_2^T P_2^T \beta\\
        &= \beta^T P_2 U_2  {(D_2^{-1} V_2^T \tilde{X}^T P_1 P_1^T \tilde{X} V_2 D_2^{-1} +I_{n-1})}^{-1} U_2^T P_2^T \beta.
    \end{aligned}
\end{equation}
Here $U_2$ is independent of $(V_2,D_2,P_1^T\tilde{X})$. By lemma~\ref{wangInverse}
\begin{equation}
    \begin{aligned}
        {(D_2^{-1}V_2^T\tilde{X}^T P_1 P_1^T \tilde{X}V_2 D_2^{-1} +I_{n-1})}^{-1}\geq I_{n-1}- U^* U^{*T}
    \end{aligned}
\end{equation}
where $U^*$ is the first $r$ eigenvectors of $D_2^{-1} V_2^T \tilde{X}^T P_1 P_1^T \tilde{X}V_2 D_2^{-1}$ and is independent of $U_2$. Since $U_2$ has Haar distribution, we have
\begin{equation}\label{lowerbound}
    \begin{aligned}
        B_1&\geq \beta^T P_2 U_2 (I_{n-1}-U^* U^{*T})U_2^T P_2^T \beta\\
        &= \beta^T P_2 U_2 U_2^T P_2^T \beta-  \beta^T P_2 U_2 U^* U^{*T} U_2^T P_2^T \beta.\\
        %&\asymp \frac{n-1-r}{p-r}\|P_2^T \beta\|^2
    \end{aligned}
\end{equation}
The difference of upper bound and lower bound is
\begin{equation}
    \begin{aligned}
 \beta^T P_2 U_2 U^* U^{*T}U_2^T P_2^T \beta \asymp \frac{r}{p-r}\|P_2^T \beta\|^2.
    \end{aligned}
\end{equation}
Hence 
\begin{equation}
    \begin{aligned}
        B_1&=\beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta+O_p(\frac{r}{p-r}\|P_2^T \beta\|^2)\\
        &=\beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta(1+O_P(1/n)).\\
    \end{aligned}
\end{equation}
So that $B_1\asymp \frac{n}{p}\|P_2^T \beta\|^2$.

For $B_2$ we have
\begin{equation}
    \begin{aligned}
    B_2&=O_P(1)\sqrt{\beta^T P_2 P_2^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-2}\tilde{X}^T P_2 P_2^T \beta}\\
    &\leq \lambda_{\min}{(\tilde{X}^T\tilde{X})}^{-1/2}O_P(1)\sqrt{\beta^T P_2 P_2^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta}.
    \end{aligned}
\end{equation}
But $\lambda_{\min}{(\tilde{X}^T\tilde{X})}\geq \lambda_{\min}{(\tilde{X}^T P_2 P_2^T \tilde{X})}\asymp p-r$ by Lemma~\ref{baiyin}.
Hence $B_2=O_P(\frac{\sqrt{n}}{p}\|P_2^T \beta\|)$.

Hence the similar law of large number and CLT holds.
\subsubsection{Step 3}

Use similar technique as before, we have
\begin{equation}
    \frac{A_1}{B_1}\sim \frac{\chi^2_p}{1+O_P(1/n)}= \lambda\chi^2_p (1+O_P(1/n)).
\end{equation}
It follows by large number that
\begin{equation}\label{thm2h1}
    \frac{A_1/B_1}{\lambda p}\xrightarrow{P} 1.
\end{equation}
And if  $p=o(n^2)$, we have
\begin{equation}\label{thm2c1}
    \begin{aligned}
        \frac{A_1/B_1-\lambda p}{\lambda\sqrt{2p}}\sim \frac{\chi^2_p (1+O_P(1/n))-p}{\sqrt{2p}}\xrightarrow{\mathcal{L}} N(0,1).
    \end{aligned}
\end{equation}
Recall that if $p=o(n^2)$, we have $A_3/B_3\geq \lambda \chi^2_{p-n+2}$ and
$A_3/B_3\leq \lambda \chi^2_{p-r}+o(\sqrt{p})\leq \lambda \chi^2_{p}+o(\sqrt{p})$. Then
\begin{equation}\label{thm2h2}
    \frac{A_3/B_3}{\lambda p}\xrightarrow{P} 1,
\end{equation}
and
\begin{equation}\label{thm2c2}
    \begin{aligned}
        \frac{A_3/B_3-\lambda p}{\lambda\sqrt{2p}}\leq \frac{\chi^2_p +o(\sqrt{p})-p}{\sqrt{2p}}\xrightarrow{\mathcal{L}} N(0,1).
    \end{aligned}
\end{equation}
From~\eqref{thm2h1} and~\eqref{thm2h2} we can deduce $T/(\lambda p)\xrightarrow{P}1$ by similar argument as before.

Similar to~\eqref{ChangT}, we have
\begin{equation}
            T\leq\frac{A_1+A_3}{B_1+A_3}+O_P(\frac{p\sqrt{n}\|P_2^T\beta\|}{n\|P_2^T\beta\|^2+n}).\\
\end{equation}
For the first term, we have
\begin{equation}
    \begin{aligned}
        &\frac{\frac{A_1+A_3}{B_1+B_3}-\lambda p}{\lambda \sqrt{2p}}
        =c\frac{\frac{A_1}{B_1}-\lambda p}{\lambda \sqrt{2p}}+
        (1-c)\frac{\frac{A_3}{B_3}-\lambda p}{\lambda \sqrt{2p}},
    \end{aligned}
\end{equation}
where 
\begin{equation}
    c=\frac{B_1}{B_1+B_3}\asymp \frac{\|P_2^T \beta\|^2}{\|P_2^T \beta\|^2+1}.
\end{equation}
Then theorem follows by the same argument as before.


\subsection{Consistency of Test}

Since $\beta\sim N(0,\sigma^2_{\beta}I_p)$ and ${(\tilde{X}^T \tilde{X})}^{-1/2}\tilde{X}^T$ is a projection matrix, we have $\gamma={(\tilde{X}^T \tilde{X})}^{-1/2}\tilde{X}^T\beta\sim N(0,\sigma^2_{\beta}I_{n-1})$. Then


\begin{equation}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{\gamma^T\tilde{X}^T\tilde{X}\gamma+2\gamma^T{(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}+\tilde{\epsilon}^T\tilde{\epsilon}}{\gamma^T\gamma+2\gamma^T{(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}+\tilde{\epsilon}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}}\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}.
    \end{aligned}
\end{equation}

It can be seen that
$A_1\sim \|\gamma\|^2\sum_{i=1}^{p}\lambda_i \chi^2_1\asymp \|\gamma\|^2 (p+\lambda_1)\asymp \sigma^2_\beta n(p+\lambda_1)$, $A_2=O_P(\sqrt{A_1})$ and $A_3\asymp n$. As for the denominator of $T$, $B_1\asymp \sigma^2_\beta n$, $B_3 \leq \tilde{\epsilon}{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\asymp n/p$ and $B_2=O_P(\sqrt{B_3}\sigma_\beta)$.


Since $A_1/B_1\sim \sum_{i=1}^p \lambda_i \chi^2_1$, we have
\begin{equation}
    \begin{aligned}
    \mathbb{P}\Big(\frac{A_1/B_1-(p-r)\lambda}{\lambda\sqrt{2(p-r)}}\geq \Phi^{-1}(1-\alpha)\Big)
        &\sim\mathbb{P}\Big(N(0,1)\geq \Phi^{-1}(1-\alpha)-\frac{\sum_{i=1}^r \lambda_i \chi^2_i}{\lambda\sqrt{2(p-r)}}\Big)\\
        &=\textrm{E}[\Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\sum_{i=1}^r \lambda_i \chi^2_i}{\lambda\sqrt{2(p-r)}}\Big)]
    \end{aligned}
\end{equation}

And note that if $p \sigma_{\beta}^2\to \infty$,
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{{A_1}/{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(\sigma_{\beta}\sqrt{n(p+\lambda_1)})+O_P(n))O_P(\sigma^2_\beta n)-(O_P(\sigma_{\beta}\frac{\sqrt{n}}{\sqrt{p}})+O_P(\frac{n}{p}))O_P(\sigma^2_{\beta} n(p+\lambda_1))}{\sigma^4_{\beta}n^2}
        \Big|\\
        &=O_P(\frac{p+\lambda_1}{\sigma_{\beta}\sqrt{n}p})+O_P(\frac{p+\lambda_1}{\sigma^2_{\beta}p^{3/2}})
    \end{aligned}
\end{equation}

Hence if 
\begin{equation}
    \frac{np^2+p^{5/2}+\lambda_1 p^{3/2}}{{(p+\lambda_1)}^2}\sigma^2_{\beta}\to \infty
\end{equation}
Then Power function holds.

\section{Simulation Results}

% latex table generated in R 3.3.1 by xtable 1.8-2 package
% Fri Nov  4 21:37:08 2016
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{rrrrr}
%          \hline
%          $n$ & $p$ & $|\beta|^2$ & Chen & New \\ 
%            \hline
%            40 & 310 & 0.00 & 0.05 & 0.00 \\ 
%              40 & 310 & 0.04 & 0.12 & 0.04 \\ 
%                80 & 550 & 0.00 & 0.05 & 0.00 \\ 
%                  80 & 550 & 0.04 & 0.13 & 0.00 \\ 
%                             \hline
%    \end{tabular}
%    \caption{Sparse case, $T=20$}
%\end{table}

\section*{References}
\bibliography{mybibfile}

\end{document}
