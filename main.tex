\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,amsthm}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}
\begin{document}

\begin{frontmatter}

\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
%\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
%\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Introduction}
\cite{Zhao2016A} proposed a generalized likelihood ratio test (GLRT) for testing high dimensional mean values. This paper derived GLRT for the problem of testing the coefficients of linear regression.
\section{GLRT}
Suppose $(X_1^T,Y_1)$,\ldots,$(X_n^T, Y_n)$ are i.i.d.\ from $N_{p+1}(\mu,\Sigma)$,
where $X_i\in \mathbb{R}^p$ and $Y_i\in \mathbb{R}$. 
Denote $X=(X_1,\ldots,X_n)$, $Y={(Y_1,\ldots,Y_n)}^T$.
 We assume $n<p$. Denote $\Theta: (\mu,\Sigma)$
Consider the hypothesis $H: \textrm{Cov}(X,y)=0$. Define the hypothesis $H_a$ as following:
\begin{equation}
    H_a: \textrm{Cov}(a^T X_i, y_i)=0
\end{equation}
where $a\in \mathbb{R}^p$ and $a^T a=1$.
Then we have $H=\cap_a H_a$.

Denote by
\begin{equation}
    S=\frac{1}{n}\sum_{i=1}^n \begin{pmatrix}
        (X_i-\bar{X}){(X_i-\bar{X})}^T&(X_i-\bar{X}){(y_i-\bar{y})}^T\\
        (y_i-\bar{y}){(X_i-\bar{X})}^T&(y_i-\bar{y}){(y_i-\bar{y})}^T\\
        \end{pmatrix}=\begin{pmatrix}S_{XX} & S_{Xy} \\ S_{yX}& S_{yy}\end{pmatrix},
\end{equation}
and
\begin{equation}
    S_a=\begin{pmatrix}
        a^T& 0\\
        0& 1
    \end{pmatrix}
    S\begin{pmatrix}
        a& 0\\
        0& 1
    \end{pmatrix},
    \Sigma_a=\begin{pmatrix}
        a^T& 0\\
        0& 1
    \end{pmatrix}
    \Sigma\begin{pmatrix}
        a& 0\\
        0& 1
    \end{pmatrix}.
\end{equation}


Then the likelihood function is 
\begin{equation}
L_a(\theta;X,Y)={(2\pi)}^{-n}|\Sigma_a|^{-n/2}\exp{(-\frac{1}{2}\textrm{tr}\Sigma_a^{-1}S_a)}.
\end{equation}
Then the maximum likelihood is
    \begin{equation}\label{like1}
        L(a)=\sup_{\theta\in \Theta}L_a(\theta ;X,Y)={(2\pi)}^{-n} |S_a|^{-n/2}e^{-n}.
\end{equation}
    If $|S_a|=0$, then~\ref{like1} is interpreted as $+\infty$.
    Similarly, the maximum likelihood under $H_a$ is
\begin{equation}
    L(a)=\sup_{\theta\in H}L_1(\theta ;X,Y)={(2\pi)}^{-n} |a^T S_{XX}aS_{yy}|^{-n/2}e^{-n}.
\end{equation}

    The goal of GLRT is to find $a$ such that $L(a)=+\infty$ and $L_{H}(a)<+\infty$ as small as possible to achieve the maximum discrepency of likelihood between null and alternative hypotheses. Equivalently, we find $a$ such that $a^T S_{XX} a$ is maximized subject to $|S_a|=0$.

    Such an $a$ can be expected to make $|\Sigma_a|$ small and $a^T \Sigma_{XX} a$ large. That is to make the variance of $a^T X_i$ large and $a^T X_i$ and $y_i$ highly correlated. If $X_i$ has certain principal components which are correlated to $y_i$, the direction $a$ is expected to be close to such principal directions.


    Let $Q_n=I_n-\frac{1}{n}\textbf{1}_n\textbf{1}_n^T$. Denote by $Q_n=WW^T$ be the rank decomposition of $Q_n$, where $W_n$ is a $n\times n-1$ matrix with $W^T W=I_{n-1}$. 
    Then $|S_a|=0$ is equivalent to $a^T X Q X^T a y^T Q y={(a^T X Q y)}^2$ and is equivelent to $W^T X^T a=W^T y k$ for some $k\in \mathbb{R}$. It can be seen that
    \begin{equation}
        a=XW{(W^T X^T X W)}^{-1} W^T y k + (I-XW{(W^T X^T X W)}^{-1}W^T X^T)a
    \end{equation}
Note that 
    \begin{equation}
        L_H (a)\propto {(a^T X Q X^T a y^T Q y)}^{-n/2}={\big(k^2{(y^T Q_n y)}^2\big)}^{-n/2}.
    \end{equation}
Since $a^T a=1$,
    \begin{equation}\label{gougu}
        k^2 y^T W{(W^T X^T X W)}^{-1} W^T y + a^T(I-XW{(W^T X^T X W)}^{-1}W^T X^T)a=1.
    \end{equation}
    To make $L_H(a)$ minimized, we should maximize $k^2$. So the second term of~\ref{gougu} should be $0$. That is

    \begin{equation}
        a=XW{(W^T X^T X W)}^{-1} W^T y k 
    \end{equation}
    Hence 
    \begin{equation}
        k^2=\frac{1}{y^T W{(W^T X^T X W)}^{-1} W^T y},
    \end{equation}
    and 
    \begin{equation}
        L_H (a)\propto {(a^T X Q X^T a y^T Q y)}^{-n/2}={\Big(\frac{{(y^T Q_n y)}^2}{y^T W{(W^T X^T X W)}^{-1} W^T y}\Big)}^{-n/2}.
    \end{equation}
After homogenization, we define
\[
    T=\frac{y^T Q_n y}{
        y^T W{(W^T X^T X W)}^{-1}W^T y
    } 
    \]


    When $L_H(a)$ is small, we reject $H$.
\section{Main}

As we have pointed out, test statistic $T$ is expected to be large when $y_i$ is correlated to some principal components of $X_i$, and be small otherwise. Hence $T$ is suitable for testing the significance of PCR.\@


Write $Y=\beta_0 \textbf{1}_n+X^T \beta+\epsilon$, where $\textbf{1}_n$ is $n$ dimensional vector with all elements equal to $1$. $\epsilon$ has distribution $N(0,\sigma^2 I_n)$.

The problem is to test hypotheses $H: \beta=0$.

%The test statistic is

%\[
%    T=\frac{
%        (\textbf{1}_n^T(X^T X)^{-1}Q_n Y)^2
%    }{
%        \hat{\sigma}^2
%        \textbf{1}_n^T(X^T X)^{-1}Q_n (X^T X)^{-1}\textbf{1}_n
%    }.
%\]
%where $Q_n=I_n-\frac{1}{n}\textbf{1}_n\textbf{1}_n^T$ and
%\[
%    \hat{\sigma}^2=\frac{1}{n-2} Y^T Q_n\Big[
%        I_n-\frac{(X^T X)^{-1}\textbf{1}_n\textbf{1}_n^T(X^T X)^{-1}}{
%        \textbf{1}_n^T(X^T X)^{-1}Q_n(X^T X)^{-1}\textbf{1}_n
%        }
%%        \Big]Q_n Y
%\] 


\[
    T=\frac{y^T Q_n y}{
        y^T W{(W^T X^T X W)}^{-1}W^T y
    } 
    \]
or equivalently
\[
    \frac{y^T Q_n y}{
        y^T Q_n{(X^T X)}^{-1}Q_n y-{(y^T Q_n{(X^T X)}^{-1}\textbf{1}_n)}^2/(\textbf{1}_n{(X^T X)}^{-1}\textbf{1}_n)
    } 
    \]


Let $\tilde{y}=W^T y$, $\tilde{X}=XW$, $\tilde{\epsilon}=W^T \epsilon$. Then
\[
    \tilde{y}=\tilde{X}^T \beta + \tilde{\epsilon}
    \]
and
\[
    T=\frac{\tilde{y}^T \tilde{y}}{
        \tilde{y}^T{(\tilde{X}^T \tilde{X})}^{-1}\tilde{y}
    }
    \]

\section{Asymptotic Results}

Denote by $\Sigma_X$ the covariance matrix of $X_i$ ($i=1,\ldots,n$). Let $\Sigma_X=P\Lambda P^T$ be the spectral decomposition of $\Sigma_X$, where $\Lambda=\textrm{diag}(\lambda_1,\ldots,\lambda_p)$ with $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_p$ and $P$ is a orthogonal matrix.

$\Sigma_X$ may be spike or non-spike.

Non-spike: there's no principal component ($r=0$). That is, $\lambda_1=\cdots = \lambda_p$.

Spike: there's $r$ principal components. That is, $\lambda_1\geq \lambda_2\geq \cdots \lambda_r\geq \lambda_{r+1}=\cdots =\lambda_p$. Denote by $P_1$ the first $r$ column of $P$ and $P_2$ the last $p-r$ column of $P$.

\begin{equation}
    \begin{aligned}
        Y&=\beta_0 \textbf{1}_n+X^T\beta+\epsilon\\
        &=\beta_0 \textbf{1}_n+X^T P_1P_1^T\beta+X^T P_2 P_2^T\beta+\epsilon\\
    \end{aligned}
\end{equation}

In either case, let $\lambda$ be $\lambda=\lambda_{r+1}=\cdots=\lambda_{p}$.

PCR try to do regression between $Y$ and (estimated) $X^T P_1$. If $P_1$ is observed, then the problem is reduced to testing an ordinary regression model. However, it's not the case.

Simply estimating $P_1$ and invoke classical testing procedure may not be a good idea since the estimation may not be consistent in high dimension. In fact, there may be even no principal component!

In this paper, testing PCR means testing:

$H_0$: There's no principal component or there's $r$ principal components but $P_1^T \beta =0$.

$H_1$; There's $r$ principal components and $P_1^T \beta \neq 0$.

Next we consider:

1. There's no PC.\@

2. There's $r$ principal components but $P_1\beta=0$.

\begin{assumption}\label{normal}
    $X$ and $\epsilon$ are normal distribution.
\end{assumption}


\begin{equation}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}


\subsection{Lemma}

\begin{lemma}\label{baiyin}
    Suppose $B=\frac{1}{q}V V^T$ where V is an $p\times q$ random matrix composed of i.i.d.\  random variables with zero mean, unit variance and finite fourth moment. As $q\to \infty$ and $p/q \to c\in [0,+\infty)$, the largest and smallest nonzero eigenvalues of $B$ converge almost surely to ${(1+\sqrt{c})}^2$ and ${(1-\sqrt{c})}^2$, respectively.
\end{lemma}
Lemma~\ref{baiyin} is known as the Bai-Yin's law~\cite{Bai1993Limit}.

\begin{lemma}\label{wangPCA}
    Let $Z_1,\ldots,Z_{n+1}$ i.i.d.\ distributed as $N(0,I_p)$.
    $\Lambda=\textrm{diag}(\lambda_1,\ldots,\lambda_p)$, where $\lambda_1\geq \cdots \lambda_r$ and $\lambda_{r+1}=\cdots =\lambda_p=\lambda$.
    $\limsup_{n\to \infty} \lambda_1/\lambda_r<\infty$, $\lambda_1/\sqrt{p}\to \infty$.
    Suppose $p=o(n^2)$.
    Denote $Z=(Z_1,\ldots,Z_n)$. Let $\hat{V}$ be the first $r$ eigenvectors of $\Lambda^{1/2}Z Z^T\Lambda^{1/2}$, $V=(e_1,\ldots,e_r)$. Then
    \begin{equation}
        Z_{n+1}^T \Lambda^{1/2}(VV^T -\hat{V}\hat{V}^T)\Lambda^{1/2}Z_{n+1}=o(\sqrt{p})
    \end{equation}
\end{lemma}
Lemma~\ref{wangPCA} is from Wang Rui's paper.

Wang Rui's PCA lemma

If limit distribution is continuous, then the convergence is uniform.

The SVD of standard normal matrix.

Wang Rui's matrix inverse subspace lemma.

\subsection{circumstance 1}
\begin{assumption}
    $r=0$.
\end{assumption}
\begin{assumption}
    $n^2/p\to 0$.
\end{assumption}

\subsubsection{Step 1}
Independent of data, generate a random $p$ dimensional orthonormal matrix $O$ with Haar invariant distribution. And 

\begin{equation}
    T=\frac{{(O\beta)}^T O\tilde{X}{(O\tilde{X})}^T O\beta+
        2{(O\beta)}^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}{(O\tilde{X})}^T \beta+
        2{(O\beta)}^T O\tilde{X}{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{({(O\tilde{X})}^T O\tilde{X})}^{-1}\tilde{\epsilon}
    }
\end{equation}

Note that conditioning on $O$, $O\tilde{X}$ is a random matrix with each entry independently distributed as $N(0,\lambda)$. Hence $O$ is independent of $O\tilde{X}$. Observe also that $O\beta/\|\beta\|$ is uniformly distributed on the unit ball.  We can without loss of generality assuming that $\beta/\|\beta\|$ is uniformly distributed on the unit ball.

\subsubsection{Step 2}
Independent of data, generate $R>0$ with $R^2$ distributed as $\chi^2_{p}$. Then $\xi=R\beta/\|\beta\|$ distributed as $N_p(0,I_p)$.
Note that conditioning on $\tilde{X}$, $\eta={(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{X}^T \xi$ is distributed as $N_{n-1}(0,I_{n-1})$. Hence $\eta$ is independent of $\tilde{X}$.

Then

\begin{equation}
    \begin{aligned}
        T&=\frac{{(\|\beta\|/R)}^2\xi^T \tilde{X}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \xi+
        2(\|\beta\|/R)\xi^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=
        \frac{{(\|\beta\|/R)}^2\eta^T \tilde{X}^T\tilde{X} \eta+
        2(\|\beta\|/R)\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{{(\|\beta\|/R)}^2\eta^T\eta+
        2(\|\beta\|/R)\eta^T{(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}


\subsubsection{Step 3: CLT}
Similar to the derivation of the distribution of Hotelling's $T^2$ statistic.

Now we deal with 
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}=\frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }
    \end{aligned}
\end{equation}
Let $O$ be an $(n-1)\times (n-1)$ orthogonal matrix satisfies 
\[
    O\tilde{\epsilon}=
    \begin{pmatrix}
        \|\tilde{\epsilon}\|\\
        0\\
        \ldots\\
        0
    \end{pmatrix}.
    \]
Then
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}=\frac{{(O\tilde{\epsilon})}^T O\tilde{\epsilon}
        }{{(O\tilde{\epsilon})}^T{\big({(\tilde{X}O^T)}^T\tilde{X}O^T\big)}^{-1} O\tilde{\epsilon}
        }
    \end{aligned}
\end{equation}
Note that $\tilde{X}O^T$ has the same distribution as $\tilde{X}$ and is independent of $O$. We have

\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}\sim 
        \frac{1}{{({(\tilde{X}^T\tilde{X})}^{-1})}_{11}}.
    \end{aligned}
\end{equation}
where ${({(\tilde{X}^T\tilde{X})}^{-1})}_{11}$ is the first element of ${(\tilde{X}^T\tilde{X})}^{-1}$. Apply the matrix inverse formula, we have 
\begin{equation}
    \begin{aligned}
        \frac{A_3}{B_3}\sim 
        {(\tilde{X}^T\tilde{X})}_{11\cdot 2}.
    \end{aligned}
\end{equation}
Since $\tilde{X}^T \tilde{X}\sim \textrm{Wishart}_{n-1}(\lambda I_{n-1},p)$, ${(\tilde{X}^T\tilde{X})}_{11\cdot 2}\sim \lambda \chi^2_{p-n+2}$.
Hence by CLT,
\begin{equation}
    \begin{aligned}
        \frac{A_3/B_3-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        }\xrightarrow{\mathcal{L}}N(0,1).
    \end{aligned}
\end{equation}
But
\begin{equation}
    \begin{aligned}
        \frac{A_3/B_3-\lambda (p-n+2)}{\lambda\sqrt{2(p-n+2)}
        }&=
        \frac{\sqrt{p}}{\sqrt{p-n+2}}
        \frac{A_3/B_3-{\lambda(p-n+2)}}{\lambda\sqrt{2p}
        }\\
        &=
        \frac{\sqrt{p}}{\sqrt{p-n+2}}
        \Big(\frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        }+\frac{(n-2)}{\sqrt{2p}}\Big).
    \end{aligned}
\end{equation}
By Slutsky Theorem, if $n^2/p\to 0$, we have
\begin{equation}
    \frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        }\xrightarrow{\mathcal{L}}N(0,1)
\end{equation}
Similar technique can deal with $A_1/B_1$.

\begin{equation}
    \begin{aligned}
        \frac{A_1}{B_1}=\frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}&\sim {(\tilde{X}^T\tilde{X})}_{11}\sim \lambda \chi^2_{p}
\\
    \end{aligned}
\end{equation}

Hence by CLT,
\begin{equation}
    \begin{aligned}
        \frac{A_1/B_1
    -\lambda p}{\lambda\sqrt{2p}
        }\xrightarrow{\mathcal{L}}N(0,1).
    \end{aligned}
\end{equation}

\subsubsection{step 4}
It's obvious that $A_3\asymp n$ and $B_1\asymp \frac{n}{p}\|\beta\|^2$. We already have $A_1/B_1\asymp p$ and $A_3/B_3\asymp p$. It follows that $A_1\asymp n\|\beta\|^2$ and $B_3\asymp n/p$. And

\begin{equation}
    \begin{aligned}
        A_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{np})\\
        &=O_P(\sqrt{n}\|\beta\|),
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        B_2&=O_P(\|\beta\|/{\sqrt{p}})\eta^T {(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}\\
        &=O_P(\|\beta\|/\sqrt{p})\sqrt{\eta^T{(\tilde{X}^T\tilde{X})}^{-1}\eta}\\
        &=O_P(\|\beta\|/\sqrt{p})O_P(\sqrt{n/p})\\
        &=O_P(\frac{\sqrt{n}}{p}\|\beta\|).
    \end{aligned}
\end{equation}



We can deduce that:
If $\|\beta\|^2\to 0$, then

\begin{equation}
    \begin{aligned}
        T&=
        \frac{\tilde{\epsilon}^T\tilde{\epsilon}
    }{\tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
        }(1+o_P(1)).
    \end{aligned}
\end{equation}
Hence $T/(\lambda p)\to 1$.
If $\|\beta\|^2\to \infty$, then

\begin{equation}
    \begin{aligned}
        T&=
        \frac{\eta^T \tilde{X}^T\tilde{X} \eta    
        }{\eta^T\eta}(1+o_P(1)).
    \end{aligned}
\end{equation}
Hence $T/(\lambda p)\to 1$.


\subsubsection{Step 5}




If $\|\beta\|^2\to 0$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_3}{B_3}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_1+A_2)B_3-(B_1+B_2)A_3}{(B_1+B_2+B_3)B_3}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(n\|\beta\|^2)+O_P(\sqrt{n}\|\beta\|))O_P(\frac{n}{p})-(O_P(\frac{n}{p}\|\beta\|^2)+O_P(\frac{\sqrt{n}}{p}\|\beta\|))O_P(n)}{n^2/p^2}
        \Big|\\
        &=O_P(\sqrt{p}\|\beta\|^2)+O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|)
    \end{aligned}
\end{equation}
Hence if $\sqrt{p}\|\beta\|^2\to 0$ and $\frac{p}{n}\|\beta\|^2\to 0$, CLT holds.

On the other hand. If $\|\beta\|^2\to \infty$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{{A_1}/{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(\sqrt{n}\|\beta\|)+O_P(n))O_P(\frac{n}{p}\|\beta\|^2)-(O_P(\frac{\sqrt{n}}{p}\|\beta\|)+O_P(\frac{n}{p}))O_P(n\|\beta\|^2)}{\frac{n^2}{p^2}\|\beta\|^4}
        \Big|\\
        &=O_P(\frac{\sqrt{p}}{\sqrt{n}}\|\beta\|^{-1})+O_P(\sqrt{p}\|\beta\|^{-2})
    \end{aligned}
\end{equation}
Hence if $\frac{n}{p}\|\beta\|^2\to \infty$ and $\frac{1}{\sqrt{p}}\|\beta\|^2\to \infty$, CLT holds.
\subsection{circumstance 2}
\begin{assumption}
$P_1^T \beta=0$.
\end{assumption}


\begin{equation}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{\beta^T P_2 P_2^T \tilde{X}\tilde{X}^T P_2 P_2^T\beta+
        2\beta^T P_2 P_2^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T P_2 P_2^T\tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T\beta+
        2\beta^T P_2 P_2^T\tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}
\subsubsection{Step 1}
 Like before, we have $A_3/B_3\sim {(\tilde{X}^T\tilde{X})}_{11\cdot 2}$. Denote by $\Lambda=\textrm{diag} (\lambda_1,\ldots,\lambda_p)$. Let $Z=(Z_1,\ldots,Z_p)$ be a $n-1\times p$ matrix with all elements independently distributed as $N(0,1)$. Let $Z_{(1)}$ and $Z_{(2)}$ be the first $1$ row and last $n-2$ rows of $Z$, that is
\[
    Z=\begin{pmatrix} 
        Z_{(1)}\\
        Z_{(2)}
    \end{pmatrix}.
    \]
Then
\begin{equation}
    \begin{aligned}
        \tilde{X}^T\tilde{X}&\sim Z\Lambda Z^T\\
        &=\begin{pmatrix}
            Z_{(1)}\Lambda Z_{(1)}^T & Z_{(1)}\Lambda Z_{(2)}^T\\
            Z_{(2)}\Lambda Z_{(1)}^T & Z_{(2)}\Lambda Z_{(2)}^T\\
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Hence

\begin{equation}
    \begin{aligned}
        T&\sim Z_{(1)}\Lambda Z_{(1)}^T-Z_{(1)}\Lambda Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda Z_{(1)}^T\\
        &=Z_{(1)}\Lambda^{1/2}\big(I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2} \big)\Lambda^{1/2}Z_{(1)}^T\\
        &\leq Z_{(1)}\Lambda^{1/2}\big(I_p -\hat{V}\hat{V}^T\big)\Lambda^{1/2}Z_{(1)}^T.\\
    \end{aligned}
\end{equation}

We require $p=o(n^2)$. The principal space is $V=(e_1,\ldots, e_r)$. Then 
\begin{equation}
    Z_{(1)}\Lambda^{1/2}\big(VV^T -\hat{V}\hat{V}^T\big)\Lambda^{1/2}Z_{(1)}^T=o(\sqrt{p})
\end{equation}

Note that
\begin{equation}
    Z_{(1)}\Lambda^{1/2}\big (I-VV^T) \Lambda^{1/2}Z_{(1)}^T\sim\lambda \chi^2_{p-r}
\end{equation}

Hence $T\leq \lambda\chi^2_{p-r}+o(\sqrt{p})$.

On the other hand, the eigenvalues of $\Lambda^{1/2}\big(I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2} \big)\Lambda^{1/2}$ is no less than $I_p -\Lambda^{1/2} Z_{(2)}^T{(Z_{(2)}\Lambda Z_{(2)}^T)}^{-1}Z_{(2)}\Lambda^{1/2}$. Hence $T\geq \lambda \chi^2_{p-n+2}$.

Hence $A_3/B_3\asymp p$ if $p/n\to \infty$.


%\textbf{Another proof:}

%\begin{equation}
    %\begin{aligned}
        %{B_3}\leq \tilde{\epsilon}^T{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\asymp \frac{n}{p-n+2}
    %\end{aligned}
%\end{equation}
%
%To get the lower bound, let $P_2^T \tilde{X}=U_2 D_2 V_2^T$ be the SVD of $P_2^T \tilde{X}$, where $U_2$ is a $(p-r)\times (n-1)$ orthonormal matrix, $D_2$ is a $(n-1)\times(n-1)$ diagonal matrix and $V_2$ is a $(n-1)\times (n-1)$ orthonormal matrix. Then
%
%\begin{equation}
    %\begin{aligned}
        %{B_3}&= \tilde{\epsilon}^T{(\tilde{X}^T P_1 P_1^T\tilde{X}+\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T{(\tilde{X}^T P_1 P_1^T\tilde{X}+V_2 D_2^2 V_2^T)}^{-1}\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T V_2 D_2^{-1}{(D_2^{-1} V_2^T\tilde{X}^T P_1 P_1^T\tilde{X}V_2 D_2^{-1}+I_{n-1})}^{-1} D_2^{-1} V_2^T\tilde{\epsilon}\\
        %&\geq \tilde{\epsilon}^T V_2 D_2^{-1}(I_{n-1}-U^* U^{*T}) D_2^{-1} V_2^T\tilde{\epsilon}\\
        %&= \tilde{\epsilon}^T {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{\epsilon}- \tilde{\epsilon}^T V_2 D_2^{-1}U^* U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}\\
%\end{aligned}
%\end{equation}
%for some $(n-1)\times r$ random matrix $U^*$. But
%\begin{equation}
    %\begin{aligned}
        %\tilde{\epsilon}^T V_2 D_2^{-1}U^* U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}&=\textrm{tr}(U^{*T} D_2^{-1} V_2^T\tilde{\epsilon}\tilde{\epsilon}^T V_2 D_2^{-1}U^*)\\
        %&\leq \lambda_{\max}(V_2^T \tilde{\epsilon}\tilde{\epsilon}^T V_2) \textrm{tr}(U^{*T} D_2^{-2}U^*)\\
        %&\leq \|\tilde{\epsilon}\|^2 r {(\lambda_{\min}(D_2))}^{-2}\\
        %&\asymp \frac{n}{p}
    %\end{aligned}
%\end{equation}
%
%Hence 
%\begin{equation}
    %{B_3}= \tilde{\epsilon}^T{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}(1+O_P(1/p))
%\end{equation}
%Hence
%\begin{equation}
    %\frac{A_3}{B_3}= \lambda \chi^2_{p-r-n+2}(1+O_P(1/p))
%\end{equation}
%Hence
%\begin{equation}
    %\frac{{A_3}/{B_3}-\lambda(p-r-n+2)}{\lambda \sqrt{2(p-r-n+2)}}\xrightarrow{\mathcal{L}}N(0,1).
%\end{equation}
%There's no restriction between $n$ and $p$!!! Is there anything wrong? If no, it can be generalized to Xu and Zhao's test!!!


\subsubsection{Step 2}

Note that $P_2^T \tilde{X}$ is an $(p-r)\times (n-1)$ matrix with all elements independently distributed as $N(0,\lambda)$.

    $A_1\asymp n\|P_2^T\beta\|^2$, $A_2=O_P(\sqrt{n}\|P_2^T \beta\|)$, $A_3\asymp n$.

    $B_3\asymp n/p$.

    As for $B_1$, 
\begin{equation}
    \begin{aligned}
        B_1&\leq \beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta\\
        &\asymp \frac{n-1}{p-r}\|P_2^T\beta\|^2
    \end{aligned}
\end{equation}
To get the lower bound, let $P_2^T \tilde{X}=U_2 D_2 V_2^T$ be the SVD of $P_2^T \tilde{X}$, where $U_2$ is a $(p-r)\times (n-1)$ orthonormal matrix, $D_2$ is a $(n-1)\times(n-1)$ diagonal matrix and $V_2$ is a $(n-1)\times (n-1)$ orthonormal matrix. Then

\begin{equation}
    \begin{aligned}
        B_1&= \beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_1 P_1^T \tilde{X}+\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta\\
        &= \beta^T P_2 U_2 D_2 V^T_2 {(\tilde{X}^T P_1 P_1^T \tilde{X}+V_2 D_2^2 V_2^T)}^{-1}V_2 D_2 U_2^T P_2^T \beta\\
        &= \beta^T P_2 U_2  {(D_2^{-1} V_2^T \tilde{X}^T P_1 P_1^T \tilde{X} V_2 D_2^{-1} +I_{n-1})}^{-1} U_2^T P_2^T \beta\\
    \end{aligned}
\end{equation}

Note that $U_2$ is independent of $(V_2,D_2,P_1^T\tilde{X})$, and

\begin{equation}
    \begin{aligned}
        {(D_2^{-1}V_2^T\tilde{X}^T P_1 P_1^T \tilde{X}V_2 D_2^{-1} +I_{n-1})}^{-1}\geq I_{n-1}- U^* U^{*T}
    \end{aligned}
\end{equation}
where $U^*$ is the first $r$ eigenvectors of $D_2^{-1} V_2^T \tilde{X}^T P_1 P_1^T \tilde{X}V_2 D_2^{-1}$ and is independent of $U_2$. Note also that $U_2$ is of Haar distribution. Hence
\begin{equation}
    \begin{aligned}
        B_1&\geq \beta^T P_2 U_2 (I_{n-1}-U^* U^{*T})U_2^T P_2^T \beta\\
        &\asymp \frac{n-1-r}{p-r}\|P_2^T \beta\|^2
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        \textrm{Upper-Lower}&\leq \beta^T P_2 U_2 U^* U^{*T}U_2^T P_2^T \beta\\
        &\asymp \frac{r}{p-r}\|P_2^T \beta\|^2
    \end{aligned}
\end{equation}
Hence 
\begin{equation}
    \begin{aligned}
        B_1&=\beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta+O_p(\frac{r}{p-r}\|P_2^T \beta\|^2)\\
        &=\beta^T P_2 P_2^T \tilde{X} {(\tilde{X}^T P_2 P_2^T \tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta(1+O_P(1/n))\\
    \end{aligned}
\end{equation}



\begin{equation}
    \begin{aligned}
    B_2&=O_P(1)\sqrt{\beta^T P_2 P_2^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-2}\tilde{X}^T P_2 P_2^T \beta}\\
    &\leq \lambda_{\min}{(\tilde{X}^T\tilde{X})}^{-1/2}O_P(1)\sqrt{\beta^T P_2 P_2^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T P_2 P_2^T \beta}\\
    \end{aligned}
\end{equation}

\begin{equation}
    \lambda_{\min}{(\tilde{X}^T\tilde{X})}\geq \lambda_{\min}{(\tilde{X}^T P_2 P_2^T \tilde{X})}\asymp p-r
\end{equation}
Hence $B_2=O_P(\frac{\sqrt{n}}{p}\|P_2^T \beta\|)$.

Hence the similar law of large number and CLT holds.
\subsubsection{Step 3}
\begin{equation}
    \frac{A_1}{B_1}\sim \frac{\chi^2_p}{1+O_P(1/n)}= \lambda\chi^2_p (1+O_P(1/n))
\end{equation}
Hence if $\|P_2^T\beta\|\to \infty$ or $\|P_2^T \beta\|\to 0$,
\begin{equation}
    \frac{T}{\lambda p}\xrightarrow{P} 1.
\end{equation}

We have
\begin{equation}
    \begin{aligned}
        \frac{A_1/B_1-\lambda p}{\lambda\sqrt{2p}}\sim \frac{\chi^2_p (1+O_P(1/n))-p}{\sqrt{2p}}\xrightarrow{\mathcal{L}} N(0,1),
    \end{aligned}
\end{equation}
if $p=o(n^2)$.

Because $A_3/B_3\leq \lambda \chi^2_{p-r}+o(\sqrt{p})\leq \lambda \chi^2_{p}+o(\sqrt{p})$ (if $p=o(n^2)$). We have
\begin{equation}
    \begin{aligned}
        \frac{A_3/B_3-\lambda p}{\lambda\sqrt{2p}}\leq \frac{\chi^2_p +o(\sqrt{p})-p}{\sqrt{2p}}\xrightarrow{\mathcal{L}} N(0,1).
    \end{aligned}
\end{equation}

If $\|P_2^T\beta\|^2\to 0$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{A_3/B_3-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_3}{B_3}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_1+A_2)B_3-(B_1+B_2)A_3}{(B_1+B_2+B_3)B_3}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(n\|P_2^T\beta\|^2)+O_P(\sqrt{n}\|P_2^T\beta\|))O_P(\frac{n}{p})-(O_P(\frac{n}{p}\|P_2^T\beta\|^2)+O_P(\frac{\sqrt{n}}{p}\|P_2^T\beta\|))O_P(n)}{n^2/p^2}
        \Big|\\
        &=O_P(\sqrt{p}\|P_2^T\beta\|^2)+O_P(\frac{\sqrt{p}}{\sqrt{n}}\|P_2^T\beta\|)
    \end{aligned}
\end{equation}
Hence if $\sqrt{p}\|P_2^T\beta\|^2\to 0$ and $\frac{p}{n}\|P_2^T\beta\|^2\to 0$, CLT holds.

On the other hand. If $\|P_2^T\beta\|^2\to \infty$, we have
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{{A_1}/{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(\sqrt{n}\|P_2^T\beta\|)+O_P(n))O_P(\frac{n}{p}\|P_2^T\beta\|^2)-(O_P(\frac{\sqrt{n}}{p}\|P_2^T\beta\|)+O_P(\frac{n}{p}))O_P(n\|P_2^T\beta\|^2)}{\frac{n^2}{p^2}\|P_2^T\beta\|^4}
        \Big|\\
        &=O_P(\frac{\sqrt{p}}{\sqrt{n}}\|P_2^T\beta\|^{-1})+O_P(\sqrt{p}\|P_2^T\beta\|^{-2})
    \end{aligned}
\end{equation}

Hence if $\frac{n}{p}\|P_2^T\beta\|^2\to \infty$ and $\frac{1}{\sqrt{p}}\|P_2^T\beta\|^2\to \infty$, CLT holds.

\subsection{Consistency of Test}
$\beta$ from normal distribution. Then consistency can be proved.
Assume that $\beta\sim N(0,\sigma^2_{\beta}I_{p})$. Then
$\gamma={(\tilde{X}^T \tilde{X})}^{-1/2}\tilde{X}^T\beta\sim N(0,\sigma^2_{\beta}I_{n-1})$.


\begin{equation}
    \begin{aligned}
        T&=\frac{\beta^T \tilde{X}\tilde{X}^T \beta+
        2\beta^T \tilde{X}\tilde{\epsilon}+
        \tilde{\epsilon}^T\tilde{\epsilon}
    }{\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{X}^T \beta+
        2\beta^T \tilde{X}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}+
        \tilde{\epsilon}^T{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}
    }\\
        &=\frac{\gamma^T\tilde{X}^T\tilde{X}\gamma+2\gamma^T{(\tilde{X}^T\tilde{X})}^{1/2}\tilde{\epsilon}+\tilde{\epsilon}^T\tilde{\epsilon}}{\gamma^T\gamma+2\gamma^T{(\tilde{X}^T\tilde{X})}^{-1/2}\tilde{\epsilon}+\tilde{\epsilon}{(\tilde{X}^T\tilde{X})}^{-1}\tilde{\epsilon}}\\
        &=\frac{A_1+A_2+A_3}{B_1+B_2+B_3}
    \end{aligned}
\end{equation}

$A_1\sim \|\gamma\|^2\sum_{i=1}^{p}\lambda_i \chi^2_1\asymp \|\gamma\|^2 (p+\lambda_1)\asymp \sigma^2_\beta n(p+\lambda_1)$. $A_2=O_P(\sqrt{A_1})$. $A_3\asymp n$.

$B_1\asymp \sigma^2_\beta n$. $B_3 \leq \tilde{\epsilon}{(\tilde{X}^T P_2 P_2^T\tilde{X})}^{-1}\tilde{\epsilon}\asymp n/p$. $B_2=O_P(\sqrt{B_3}\sigma_\beta)$.


$A_1/B_1\sim \sum_{i=1}^p \lambda_i \chi^2_1$. Hence
\begin{equation}
    \begin{aligned}
    \mathbb{P}\Big(\frac{A_1/B_1-(p-r)\lambda}{\lambda\sqrt{2(p-r)}}\geq \Phi^{-1}(1-\alpha)\Big)
        &\sim\mathbb{P}\Big(N(0,1)\geq \Phi^{-1}(1-\alpha)-\frac{\sum_{i=1}^r \lambda_i \chi^2_i}{\lambda\sqrt{2(p-r)}}\Big)\\
        &=\textrm{E}[\Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\sum_{i=1}^r \lambda_i \chi^2_i}{\lambda\sqrt{2(p-r)}}\Big)]
    \end{aligned}
\end{equation}

And note that if $p \sigma_{\beta}^2\to \infty$,
\begin{equation}
    \begin{aligned}
        &\Big|\frac{T-{\lambda p}}{\lambda\sqrt{2p}
        }-
    \frac{{A_1}/{B_1}-{\lambda p}}{\lambda\sqrt{2p}
        }\Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{A_1+A_2+A_3}{B_1+B_2+B_3}-\frac{A_1}{B_1}
        \Big|\\
        &=
        \frac{1}{\lambda\sqrt{2p}}
        \Big|
        \frac{(A_2+A_3)B_1-(B_2+B_3)A_1}{(B_1+B_2+B_3)B_1}
        \Big|\\
        &=
        \frac{O_{P}(1)}{\lambda\sqrt{2p}}
        \Big|
        \frac{(O_P(\sigma_{\beta}\sqrt{n(p+\lambda_1)})+O_P(n))O_P(\sigma^2_\beta n)-(O_P(\sigma_{\beta}\frac{\sqrt{n}}{\sqrt{p}})+O_P(\frac{n}{p}))O_P(\sigma^2_{\beta} n(p+\lambda_1))}{\sigma^4_{\beta}n^2}
        \Big|\\
        &=O_P(\frac{p+\lambda_1}{\sigma_{\beta}\sqrt{n}p})+O_P(\frac{p+\lambda_1}{\sigma^2_{\beta}p^{3/2}})
    \end{aligned}
\end{equation}

Hence if 
\begin{equation}
    \frac{np^2+p^{5/2}+\lambda_1 p^{3/2}}{{(p+\lambda_1)}^2}\sigma^2_{\beta}\to \infty
\end{equation}
Then Power function holds.

\section{Simulation Results}

% latex table generated in R 3.3.1 by xtable 1.8-2 package
% Fri Nov  4 21:37:08 2016
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{rrrrr}
%          \hline
%          $n$ & $p$ & $|\beta|^2$ & Chen & New \\ 
%            \hline
%            40 & 310 & 0.00 & 0.05 & 0.00 \\ 
%              40 & 310 & 0.04 & 0.12 & 0.04 \\ 
%                80 & 550 & 0.00 & 0.05 & 0.00 \\ 
%                  80 & 550 & 0.04 & 0.13 & 0.00 \\ 
%                             \hline
%    \end{tabular}
%    \caption{Sparse case, $T=20$}
%\end{table}

\section*{References}
\bibliography{mybibfile}

\end{document}
