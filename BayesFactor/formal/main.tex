\documentclass[11pt]{article}
 
\newcommand\CG[1]{\textcolor{red}{#1}}

\usepackage{lineno,hyperref}

\usepackage[margin=1 in]{geometry}
\renewcommand{\baselinestretch}{1.25}


%\usepackage{refcheck}
\usepackage{authblk}
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[page,title]{appendix}
%\renewcommand\appendixname{haha}
\usepackage{enumerate}
\usepackage{changepage}
\usepackage{datetime}
\newdate{date}{9}{1}{2017}

%%%%%%%%%%%%%%  Notations %%%%%%%%%%
\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myP}{P}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

 \def\balpha{\bfsym \alpha}
 \def\bbeta{\bfsym \beta}
 \def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
 \def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
 \def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
 \def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
 \def\bnu{\bfsym {\nu}}
 \def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
 \def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \varepsilon}
 \def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
 \def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
 \def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
 \def\brho   {\bfsym {\rho}}
 \def\btau{\bfsym {\tau}}
 \def\bxi{\bfsym {\xi}}
 \def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{definition}{\quad\quad Definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}



\title{
A Bayesian-motivated test for linear model in high-dimensional setting
}



\author[1]{Rui Wang}
%\author[2]{xx}
%\author[1,3]{Xingzhong Xu\thanks{Corresponding author\\Email address: xuxz@bit.edu.cn}}
%\affil[1]{
%School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    %100081,China
%}
%\affil[2]{
    %xx
%}
%\affil[3]{
%Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China
%}



\begin{document}
\maketitle
\section{Introduction} 


The proposed test is the limit of Bayes factors.

Fixed design

Suppose we would like to test the hypotheses:
\begin{align*}
    \mathcal H_0:   \By = \BX_a \bbeta_a + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n),
    \\
    \mathcal H_1:   \By = \BX_a \bbeta_a + \BX_b \bbeta_b + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n).
\end{align*}
Here $\bbeta_a$ is $q$ dimensional and $\bbeta_b$ is $p$ dimensional.
We assume that as $n$ tends to infinity, $q$ is fixed while $p/n \to \infty$.
This assumption is reasonable.
We assume $\BX_a$ has full column rank and $\BX_b$ has full row rank.
In practice, $p_0$ is often $1$ and $\BX_a$ is $\mathbf 1_n$.


As \cite{Goeman2006} pointed out, if $\bbeta_b \neq 0$ but $\BX_b \bbeta_b =0$, no test has any power.
\cite{Goeman2006} used Bayesian method.
Their idea is to choose an  `unbiased' distribution of $\bbeta_b$.
As they noticed, their test has negligible power for many alternatives, and is not unbiased.

The following proposition implies that there is no nontrivial unbiased test.
\begin{proposition}\label{prop:unbiased}
Suppose
$\By\sim \mathcal N_n (\mu, \phi^{-1}\BI_n)$.
We test $H_0: \mu=\BX_a \bbeta_a, \bbeta_a \in \mathbb R^q$ versus
$H_1: \mu\in \mathbb R^n$, where $\BX_a$ is an $n\times q$ matrix with full column rank, $q<n$.
Let $\varphi(\By)$ be a test function, that is, a Borel measurable function, $0\leq \phi(\By)\leq 1$.
If $\int\varphi(\By) \mathcal N_n (\BX_a\bbeta_a, \phi^{-1} \BI_n) (d\By)=\alpha$ for $\bbeta_a \in \mathbb R^q$, $\phi>0$ and $\int\varphi(\By) \mathcal N_n (\mu, \phi^{-1} \BI_n) (d\By) \geq \alpha$ for $\mu\in \mathbb R^n$, $\phi>0$, then $\varphi(\By)=\alpha$, a.s.
\end{proposition}

So we can not find a universally good test.
Instead, we would like to find a test with good average behaviour.
So Bayesian methods are natural choices in this case.




Bayes hypothesis testing use the Bayes factor.
\begin{equation*}
    B_{10}= \frac {
        \int f_1(\By|\bbeta_b ,\bbeta_a, \phi) \pi_1(\bbeta_b,\bbeta_a,\phi) d\bbeta_b d\bbeta_a d\phi
}{
        \int f_0(\By|\bbeta_a, \phi) \pi_0(\bbeta_a,\phi) d\bbeta_a d\phi
    }.
\end{equation*}

There have been several extensions of $g$-priors to $p>n$ case: \cite{maruyama2011}, \cite{Shang2011}.

Under $H_0$, we impose the reference prior $\pi_0 (\bbeta_a,\phi)=c/\phi$.
Note that under $H_1$, the posterior corresponding to the referece prior is proper if and only if $\myrank (\BX_a, \BX_b)= q+p$ and $n>q + p$.
That is, the minimal training sample size is $q + p +1$.
So we cannot impose the reference prior under $ H_1$ provided $q + p  \geq n$.
We temporarily impose the conditional prior $\bbeta_b|\bbeta_a, \phi \sim \mathcal N_{p} (0, \kappa^{-1} \phi^{-1} \BI_{p}) $.
There are extansive literature consider the choice of $\kappa$.
\cite{Kass1995} choose $\kappa$ such that the amount of information about the parameter equal to the amount of information contained in one observation.
Thus, under $H_1$, we put the prior
\begin{equation*}
    \pi_1 (\bbeta_b | \bbeta_a, \phi) =
    \mathcal N_p\left(0,\frac{1}{\kappa \phi}\BI_p\right)(\bbeta_b)
    %\frac{(\kappa \phi )^{p/2}}{(2\pi )^{p/2}}  
    %\exp\left\{
        %-\frac{\kappa \phi }{2} \|\bbeta_b\|^2
    %\right\}
    ,\quad
    \pi_1(\bbeta_a, \phi) = \frac{c}{\phi}.
\end{equation*}

\begin{equation*}
    \begin{split}
    m_0(\By;\kappa, \tau) 
    &:=
    \int f_0^\tau (\By|\bbeta_a,\phi) \pi_0 (\bbeta_a, \phi) d\bbeta_a d\phi
    \\
    &=
    \frac{
        c_0 \Gamma\left( \frac{\tau n - q}{2}\right)
    }{
        \pi^{\frac{\tau n - q }{2}}
        \tau^{\frac{\tau n}{2}}
        |\BX_a^\top \BX_a|^{\frac{1}{2}}
        \| (\BI_n -\BP_a) \By\|^{\tau n -q}
    }.
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        m_1(\By;\kappa, \tau) 
    &:=
    \int f_1^\tau (\By|\bbeta_b,\bbeta_a,\phi) \pi_1 (\bbeta_b |\bbeta_a, \phi) \pi_1 (\bbeta_a, \phi)  d\bbeta_a d\bbeta_b d\phi
    \\
    &=
    \frac{c_1\kappa^{\frac p 2} \Gamma \left(\frac{\tau n -q}{2}\right)}{
        \pi^{\frac{\tau n -q}{ 2 }} \tau^{\frac{\tau n + p}{2}}
        |\BX_a^\top \BX_a|^{\frac 1 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
    }
    \frac{1}{\left[ \By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^* \right]^{\frac{\tau n - q}{2}}}
    .
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        \frac{m_1(\By;\kappa,\tau)}{m_0(\By;\kappa,\tau)} 
        =
        \frac{c_1 \kappa^{\frac{p}{2}}}{c_0 \tau^{\frac p 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
        }
        \left(
            \frac{\By^{*\top} \By^*}{
\By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^*
            }
        \right)^{\frac{\tau n - q }{2}}
    \end{split}
\end{equation*}


It is straightforward to show that the Bayes factor associated with these priors is
\begin{equation*}
    \begin{split}
        B_{10}^{\kappa}=  &
    \frac{\kappa^{p/2}}{
        |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |^{1/2}
    }
    \cdot
    \\
    &
    \left(
        \frac{
            \By\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
            -
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }
    \right)^{(n-q)/2}.
    \end{split}
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
        2\log B_{10}^{\kappa} =  &
    p\log \kappa
    -
        \log |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |
    \\
    &
    -(n-q)\log \left(
            1-
        \frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
    \right).
    \end{split}
\end{equation*}

Denote by $\BI_n-\BP_a=\tilde{\BU}_a \tilde{\BU}_a^\top$ the rank decomposition of $\BI_n - \BP_a$, where $\tilde{\BU}_a$ is a $n\times (n-q)$ column orthogonal matrix.
Let $\BX_b^* = \tilde{\BU}_a^\top \BX_b$, $\By^* =\tilde \BU_a^\top \By$.
Let $\gamma_i$ be the $i$th largest eigenvalue of $\BX^*_b \BX_b^{*\top}$, $i=1,\ldots, n-q$.
Denote by $\BX_b^* =\BU_{b}^* \BD_{b}^* \BV_b^{*\top}$ the singular value decomposition of $\BX_{b}^*$, where  $\BU_{b}^*$, $\BV_b^*$ are $(n-q)\times (n-q)$ and $p\times (n-q)$ column orthogonal matrices, respectively, and $\BD_{b}^*=\mydiag (\sqrt {\gamma_1},\ldots, \sqrt{\gamma_{n-q}})$.
Then

\begin{equation*}
    \begin{split}
        2\log B_{10}^\kappa
        =&
         p\log \kappa
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(p-(n-q))\log \kappa
         \\
         &-(n-q)\log\left(1-\frac{\By^{*\top} \BX_b^* \left( \BX_b^{*\top} \BX_b^* + \kappa \BI_p \right)^{-1} \BX_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         \\
        =&
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} \BU_b^*  \left[\frac 1 \kappa \left(\BI_{n-q}-\BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*} \right) \right] \BU_b^{*\top} \By^* }\right)
         \\
        =&
        (n-q)\log \kappa - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(n-q)\log\left(1-\frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         .
    \end{split}
\end{equation*}
The main part of $2\log B_{10}^\kappa$ is 
\begin{equation*}
    T_n^\kappa = \frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}.
\end{equation*}
A large value of $T_n^\kappa$ supports the alternative hypothesis.
Under the null hypothesis, 
\begin{equation*}
    \myE T_{n}^\kappa=
    \frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right).
\end{equation*}
Under the alternative hypothesis, consider $\bbeta_b=c \bbeta_b^\dagger$ where $\bbeta_b^\dagger\neq 0$ is a fixed direction and $c>0$.
As $c \to \infty$, 
\begin{equation*}
T_n^\kappa \to
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
.
\end{equation*}
We say $T_{n}^\kappa$ is consistent along the direction $\bbeta_b^\dagger$ if
\begin{equation*}
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
>
\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right),
\end{equation*}
or equivalently
\begin{equation*}
    \bbeta_b^{\dagger \top} \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
    \BV_b^{*\top} \bbeta_b^{\dagger}
    >0.
\end{equation*}
Let $k_{\kappa}$ be the number of positive eigenvalues of
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}.
\end{equation*}
Let $\mathcal S_{\kappa}$ be the linear space spanned by the first $k_{\kappa}$ columns of $\BV_{b}^*$.
Denote by $\mathcal S_{\kappa}^{\bot}$ the orthogonal complement space of $\mathcal S_\kappa$.
We have $\mathbb R^p=\mathcal S_\kappa \oplus \mathcal  S_{\kappa}^{\bot}$.
If $\bbeta_b^\dagger \in \mathcal S_\kappa$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}>0.
\end{equation*}
On the other hand, if $\bbeta_b^\dagger \in \mathcal S_{\kappa}^\bot$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}\leq 0.
\end{equation*}
We would like to choose a hyperparameter $\kappa$ which consists the most consistent directions.
To achieve this, we maximize $k_{\kappa}$ with respect to $\kappa$.

\begin{proposition}\label{prop:monotone}
    For $ \kappa _2 > \kappa_1 > 0$, we have
    $k_{\kappa_1} \geq k_{\kappa_2}$.
    That is, $k_{\kappa}$ ($\kappa>0$) is decreasing in $\kappa$.
\end{proposition}
The proposition implies that we should put $\kappa$ as small as possible.
This motivates us to consider $B_{10}^0=\lim_{\kappa\to 0} B_{10}^\kappa$.
It is straightforward to show that
\begin{equation*}
    2\log B_{10}^0= 
         - \sum_{i=1}^{n-q}\log ( \gamma_i )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }\right)
         .
\end{equation*}
$B_{10}^0$ can be regarded as the Bayes factor with respect to noninformative prior.

Define
\begin{equation*}
    T_n= \frac{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }{\By^{*\top} \By^*} .
\end{equation*}
Then we reject the null hypothesis if $T_n$ is small.
It can be seen that under the null hypohtesis,
\begin{equation*}
    T_n \sim
    \frac{\sum_{i=1}^{n-q} \gamma_i^{-1} Z_i^2}{\sum_{i=1}^{n-q} Z_i^2},
    \end{equation*}
    where $\gamma_i$ is the $i$th eigenvalue of $  \BX_b^* \BX_b^{*\top}$, $i=1,\ldots, n-q$, and $Z_1,\ldots, Z_{n-q}$ are iid $\mathcal N(0,1)$ random variables.


\section{Distribution under the null hypothesis}
Under the null hypothesis, the distribution of $2\log B_{10}$ does not rely on unknown parameters.
Further more, its distribution is valid as long as the distribution of $\epsilon$ is spherically symmetric.
\begin{proposition}
    Under the null hypothesis,
    \begin{equation*}
        T_n:=\frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI_n-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
        \sim
        \frac{\sum_{i=1}^{n-q} \frac{\gamma_i}{\gamma_i +\kappa}Z_i^2}{\sum_{i=1}^{n-q} Z_i^2},
    \end{equation*}
    , and $Z_1,\ldots, Z_{n-q}$ are iid $\mathcal N(0,1)$ random variables.
\end{proposition}
Let $\nu_i={\gamma_i}/(\gamma_i +\kappa)$, $\bar \nu = (n-q)^{-1} \sum_{i=1}^{n-q} \nu_i$.

\begin{lemma}
    Under the null hypothesis, a necessary and sufficient condition for
    \begin{equation}\label{eq:lemma1}
        \frac{n-q}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
        }
        (T_n- \bar \nu)
        \xrightarrow{\mathcal L} \mathcal N (0,1)
    \end{equation}
    is that
    \begin{equation}\label{eq:lemma12}
        \frac{\max_{i\in\{1,\ldots, n-q\}} (\nu_i- \bar \nu)^2}{\sum_{i=1}^{n-q}(\nu_i- \bar \nu)^2} \to 0.
    \end{equation}
\end{lemma}
\begin{proof}
    Note that 
    \begin{equation*}
        \frac{n-q}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
        }
        (T_n- \bar \nu)
    \sim
        \frac{n-q}{
    \sum_{i=1}^{n-q} Z_i^2
        }
    \frac{\sum_{i=1}^{n-q} (\nu_i- \bar \nu) Z_i^2}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
    }
    .
    \end{equation*}
    By Slutsky's theorem, \eqref{eq:lemma1} holds if and only if
    \begin{equation*}
    \frac{\sum_{i=1}^{n-q} (\nu_i- \bar \nu) Z_i^2}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
    }
    \xrightarrow{\mathcal L} \mathcal N (0,1).
    \end{equation*}
    From Lemma 1 of \cite{WANG2018225}, \eqref{eq:lemma12} is a necessary and sufficient condition for this to hold.
\end{proof}










%\section*{Acknowledgements}
%This work was supported by the National Natural Science Foundation of China under Grant Nos.\ xxxxx, xxxx.




\begin{appendices}

    \section{haha1}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:unbiased}}]
    We assume $0<\alpha<1$ since the case $\alpha=0$ or $1$ is trivial.
    Note that the condition implies $\int [\varphi(\By)-\alpha] \mathcal N_n (0,\phi^{-1} \BI_n) (d\By)=0$.
    Hence it suffices to prove $\varphi(\By) \geq \alpha$, a.s. 
    We prove this by contradiction.
    Suppose $\lambda(\{\By:\varphi (\By) <\alpha\})>0$.
    Then there exists a $\eta >0$, such that $\lambda(\{\By:\varphi (\By) <\alpha-\eta\})>0$.
    We denote $E=\{\By:\varphi (\By) <\alpha-\eta\}$.
    From Lebesgue density theorem \citep[Corollary 6.2.6]{book:992991}, there exists a point $z\in E$, such that, for each $\epsilon >0$ there is a $\delta_{\epsilon}>0$ such that
    \begin{equation*}
        \left|\frac{\lambda(E^\complement\cap C_{\epsilon})}{\lambda(C_{\epsilon})}\right|<\epsilon,
    \end{equation*}
    where $C_{\epsilon}=\prod_{i=1}^n [z_i-\delta_{\epsilon}, z_i + \delta_{\epsilon}]$.
    We put
    \begin{equation*}
        \epsilon=\left(\frac{\sqrt \pi}{\sqrt 2 \Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^n \frac{\eta}{3}.
    \end{equation*}
    Then for any $\phi>0$,
    \begin{equation*}
        \begin{split}
            \alpha \leq& 
            \int_{\mathbb R^n}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            =&
            \int_{E\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{E^\complement\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \int_{E^\complement\cap C_{\epsilon}} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}\lambda(E^\complement\cap C_{\epsilon})
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}
            \epsilon
            (2\delta_\epsilon)^n
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            =&
            \alpha-\eta
            +
            \left(\frac{\sqrt{\phi} \delta_{\epsilon}}{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^{n}
            \frac{\eta}{3}
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right).
        \end{split}
    \end{equation*}
    Putting 
    \begin{equation*}
        \phi = \left(\frac{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}{\delta_\epsilon}\right)^2
    \end{equation*}
    yields the contradiction $\alpha\leq \alpha-(2/3)\eta$.
    This completes the proof.

\end{proof}


\begin{proof}[\textbf{Proof of Proposition \ref{prop:monotone}}]
    For positive integer $m$, define $[m]=\{1,,\ldots, m\}$.
    For a set $A$, denote by $| A |$ its cardinality.
    We have
    \begin{equation*}
        \begin{split}
        k_{\kappa} =& \left|\left\{i\in [n-q]: \frac{\gamma_i^2}{\gamma_i +\kappa} - \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j \gamma_i}{\gamma_j +\kappa}>0 \right\}\right|
        \\
        =& \left|\left\{i\in [n-q]: \frac{\gamma_i}{\gamma_i +\kappa} > \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j }{\gamma_j +\kappa} \right\}\right|
        .
        \end{split}
    \end{equation*}
    Let $X$ be a random variable uniformly distributed on $\{\gamma_1,\ldots,\gamma_{n-q}\}$.
    That is, $\Pr(X=\gamma_i)=1/(n-q)$, $i=1,\ldots, n-q$.
    Then it can be seen that
    \begin{equation*}
        k_{\kappa}=(n-q) \Pr \left(\frac{X}{X+\kappa}>\myE \left[\frac{X}{X+\kappa}\right]\right).
    \end{equation*}
    Hence we only need to verify
    \begin{equation}\label{eq:toBeJen}
        \Pr \left(\frac{X}{X+\kappa_1}>\myE \left[\frac{X}{X+\kappa_1}\right]\right) 
\geq
\Pr \left(\frac{X}{X+\kappa_2}>\myE \left[\frac{X}{X+\kappa_2}\right]\right) .
    \end{equation}
    Let $Y=X/(X+\kappa_2)$.
    Then
    \begin{equation*}
        \frac{X}{(X+\kappa_1)} = \frac{\kappa_2 Y}{ \kappa_1 + (\kappa_2-\kappa_1) Y} := f(Y).
    \end{equation*}
    Note that $f(Y)$ is increasing for $Y\geq 0$.
    Then the inequality \eqref{eq:toBeJen} is equivalent to
    \begin{equation*}
        \Pr \left(Y> f^{-1}\left(\myE f(Y)\right)\right) 
\geq
\Pr \left( Y >\myE Y\right) .
    \end{equation*}
    Hence we only need to verify
        $f^{-1}\left(\myE f(Y)\right)
        \leq
        \myE Y$, or equivalently, $\myE f(Y)
        \leq
        f(\myE Y)$.
        But the last inequality is a direct consequence of the concavity of $f(Y)$.
        This completes the proof.

\end{proof}


    \section{haha2}
\end{appendices}



\bibliographystyle{apalike}
\bibliography{mybibfile}



\end{document}
