\documentclass[11pt]{article}
 
\newcommand\CG[1]{\textcolor{red}{#1}}

\usepackage{lineno,hyperref}

\usepackage[margin=1 in]{geometry}
\renewcommand{\baselinestretch}{1.25}


%\usepackage{refcheck}
\usepackage{authblk}
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[page,title]{appendix}
%\renewcommand\appendixname{haha}
\usepackage{enumerate}
\usepackage{changepage}
\usepackage{datetime}
\newdate{date}{9}{1}{2017}

%%%%%%%%%%%%%%  Notations %%%%%%%%%%
\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myP}{P}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator{\myCov}{Cov}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

 \def\balpha{\bfsym \alpha}
 \def\bbeta{\bfsym \beta}
 \def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
 \def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
 \def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
 \def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
 \def\bnu{\bfsym {\nu}}
 \def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
 \def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \varepsilon}
 \def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
 \def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
 \def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
 \def\brho   {\bfsym {\rho}}
 \def\btau{\bfsym {\tau}}
 \def\bxi{\bfsym {\xi}}
 \def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{definition}{\quad\quad Definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}



\title{
A Bayesian-motivated test for linear model in high-dimensional setting
}



\author[1]{Rui Wang}
%\author[2]{xx}
%\author[1,3]{Xingzhong Xu\thanks{Corresponding author\\Email address: xuxz@bit.edu.cn}}
%\affil[1]{
%School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    %100081,China
%}
%\affil[2]{
    %xx
%}
%\affil[3]{
%Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China
%}



\begin{document}
\maketitle
\section{Introduction} 


The proposed test is the limit of Bayes factors.

Fixed design

Suppose we would like to test the hypotheses:
\begin{align*}
    \mathcal H_0:   \By = \BX_a \bbeta_a + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n),
    \\
    \mathcal H_1:   \By = \BX_a \bbeta_a + \BX_b \bbeta_b + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n).
\end{align*}
Here $\bbeta_a$ is $q$ dimensional and $\bbeta_b$ is $p$ dimensional.
We assume that as $n$ tends to infinity, $q$ is fixed while $p/n \to \infty$.
This assumption is reasonable.
We assume $\BX_a$ has full column rank and $\BX_b$ has full row rank.
In practice, $p_0$ is often $1$ and $\BX_a$ is $\mathbf 1_n$.


As \cite{Goeman2006} pointed out, if $\bbeta_b \neq 0$ but $\BX_b \bbeta_b =0$, no test has any power.
\cite{Goeman2006} used Bayesian method.
Their idea is to choose an  `unbiased' distribution of $\bbeta_b$.
As they noticed, their test has negligible power for many alternatives, and is not unbiased.

The following proposition implies that there is no nontrivial unbiased test.
\begin{proposition}\label{prop:unbiased}
Suppose
$\By\sim \mathcal N_n (\mu, \phi^{-1}\BI_n)$.
We test $H_0: \mu=\BX_a \bbeta_a, \bbeta_a \in \mathbb R^q$ versus
$H_1: \mu\in \mathbb R^n$, where $\BX_a$ is an $n\times q$ matrix with full column rank, $q<n$.
Let $\varphi(\By)$ be a test function, that is, a Borel measurable function, $0\leq \phi(\By)\leq 1$.
If $\int\varphi(\By) \mathcal N_n (\BX_a\bbeta_a, \phi^{-1} \BI_n) (d\By)=\alpha$ for $\bbeta_a \in \mathbb R^q$, $\phi>0$ and $\int\varphi(\By) \mathcal N_n (\mu, \phi^{-1} \BI_n) (d\By) \geq \alpha$ for $\mu\in \mathbb R^n$, $\phi>0$, then $\varphi(\By)=\alpha$, a.s.
\end{proposition}

So we can not find a universally good test.
Instead, we would like to find a test with good average behaviour.
So Bayesian methods are natural choices in this case.




Bayes hypothesis testing use the Bayes factor.
\begin{equation*}
    B_{10}= \frac {
        \int f_1(\By|\bbeta_b ,\bbeta_a, \phi) \pi_1(\bbeta_b,\bbeta_a,\phi) d\bbeta_b d\bbeta_a d\phi
}{
        \int f_0(\By|\bbeta_a, \phi) \pi_0(\bbeta_a,\phi) d\bbeta_a d\phi
    }.
\end{equation*}

There have been several extensions of $g$-priors to $p>n$ case: \cite{maruyama2011}, \cite{Shang2011}.

Under $H_0$, we impose the reference prior $\pi_0 (\bbeta_a,\phi)=c/\phi$.
Note that under $H_1$, the posterior corresponding to the referece prior is proper if and only if $\myrank (\BX_a, \BX_b)= q+p$ and $n>q + p$.
That is, the minimal training sample size is $q + p +1$.
So we cannot impose the reference prior under $ H_1$ provided $q + p  \geq n$.
We temporarily impose the conditional prior $\bbeta_b|\bbeta_a, \phi \sim \mathcal N_{p} (0, \kappa^{-1} \phi^{-1} \BI_{p}) $.
There are extansive literature consider the choice of $\kappa$.
\cite{Kass1995} choose $\kappa$ such that the amount of information about the parameter equal to the amount of information contained in one observation.
Thus, under $H_1$, we put the prior
\begin{equation*}
    \pi_1 (\bbeta_b | \bbeta_a, \phi) =
    \mathcal N_p\left(0,\frac{1}{\kappa \phi}\BI_p\right)(\bbeta_b)
    %\frac{(\kappa \phi )^{p/2}}{(2\pi )^{p/2}}  
    %\exp\left\{
        %-\frac{\kappa \phi }{2} \|\bbeta_b\|^2
    %\right\}
    ,\quad
    \pi_1(\bbeta_a, \phi) = \frac{c}{\phi}.
\end{equation*}

\begin{equation*}
    \begin{split}
    m_0(\By;\kappa, \tau) 
    &:=
    \int f_0^\tau (\By|\bbeta_a,\phi) \pi_0 (\bbeta_a, \phi) d\bbeta_a d\phi
    \\
    &=
    \frac{
        c_0 \Gamma\left( \frac{\tau n - q}{2}\right)
    }{
        \pi^{\frac{\tau n - q }{2}}
        \tau^{\frac{\tau n}{2}}
        |\BX_a^\top \BX_a|^{\frac{1}{2}}
        \| (\BI_n -\BP_a) \By\|^{\tau n -q}
    }.
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        m_1(\By;\kappa, \tau) 
    &:=
    \int f_1^\tau (\By|\bbeta_b,\bbeta_a,\phi) \pi_1 (\bbeta_b |\bbeta_a, \phi) \pi_1 (\bbeta_a, \phi)  d\bbeta_a d\bbeta_b d\phi
    \\
    &=
    \frac{c_1\kappa^{\frac p 2} \Gamma \left(\frac{\tau n -q}{2}\right)}{
        \pi^{\frac{\tau n -q}{ 2 }} \tau^{\frac{\tau n + p}{2}}
        |\BX_a^\top \BX_a|^{\frac 1 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
    }
    \frac{1}{\left[ \By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^* \right]^{\frac{\tau n - q}{2}}}
    .
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        \frac{m_1(\By;\kappa,\tau)}{m_0(\By;\kappa,\tau)} 
        =
        \frac{c_1 \kappa^{\frac{p}{2}}}{c_0 \tau^{\frac p 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
        }
        \left(
            \frac{\By^{*\top} \By^*}{
\By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^*
            }
        \right)^{\frac{\tau n - q }{2}}
    \end{split}
\end{equation*}


It is straightforward to show that the Bayes factor associated with these priors is
\begin{equation*}
    \begin{split}
        B_{10}^{\kappa}=  &
    \frac{\kappa^{p/2}}{
        |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |^{1/2}
    }
    \cdot
    \\
    &
    \left(
        \frac{
            \By\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
            -
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }
    \right)^{(n-q)/2}.
    \end{split}
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
        2\log B_{10}^{\kappa} =  &
    p\log \kappa
    -
        \log |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |
    \\
    &
    -(n-q)\log \left(
            1-
        \frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
    \right).
    \end{split}
\end{equation*}

Denote by $\BI_n-\BP_a=\tilde{\BU}_a \tilde{\BU}_a^\top$ the rank decomposition of $\BI_n - \BP_a$, where $\tilde{\BU}_a$ is a $n\times (n-q)$ column orthogonal matrix.
Let $\BX_b^* = \tilde{\BU}_a^\top \BX_b$, $\By^* =\tilde \BU_a^\top \By$.
Let $\gamma_i$ be the $i$th largest eigenvalue of $\BX^*_b \BX_b^{*\top}$, $i=1,\ldots, n-q$.
Denote by $\BX_b^* =\BU_{b}^* \BD_{b}^* \BV_b^{*\top}$ the singular value decomposition of $\BX_{b}^*$, where  $\BU_{b}^*$, $\BV_b^*$ are $(n-q)\times (n-q)$ and $p\times (n-q)$ column orthogonal matrices, respectively, and $\BD_{b}^*=\mydiag (\sqrt {\gamma_1},\ldots, \sqrt{\gamma_{n-q}})$.
Then

\begin{equation*}
    \begin{split}
        2\log B_{10}^\kappa
        =&
         p\log \kappa
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(p-(n-q))\log \kappa
         \\
         &-(n-q)\log\left(1-\frac{\By^{*\top} \BX_b^* \left( \BX_b^{*\top} \BX_b^* + \kappa \BI_p \right)^{-1} \BX_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         \\
        =&
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} \BU_b^*  \left[\frac 1 \kappa \left(\BI_{n-q}-\BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*} \right) \right] \BU_b^{*\top} \By^* }\right)
         \\
        =&
        (n-q)\log \kappa - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(n-q)\log\left(1-\frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         .
    \end{split}
\end{equation*}
The main part of $2\log B_{10}^\kappa$ is 
\begin{equation*}
    T_n^\kappa = \frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}.
\end{equation*}
A large value of $T_n^\kappa$ supports the alternative hypothesis.
Under the null hypothesis, 
\begin{equation*}
    \myE T_{n}^\kappa=
    \frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right).
\end{equation*}
Under the alternative hypothesis, consider $\bbeta_b=c \bbeta_b^\dagger$ where $\bbeta_b^\dagger\neq 0$ is a fixed direction and $c>0$.
As $c \to \infty$, 
\begin{equation*}
T_n^\kappa \to
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
.
\end{equation*}
We say $T_{n}^\kappa$ is consistent along the direction $\bbeta_b^\dagger$ if
\begin{equation*}
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
>
\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right),
\end{equation*}
or equivalently
\begin{equation*}
    \bbeta_b^{\dagger \top} \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
    \BV_b^{*\top} \bbeta_b^{\dagger}
    >0.
\end{equation*}
Let $k_{\kappa}$ be the number of positive eigenvalues of
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}.
\end{equation*}
Let $\mathcal S_{\kappa}$ be the linear space spanned by the first $k_{\kappa}$ columns of $\BV_{b}^*$.
Denote by $\mathcal S_{\kappa}^{\bot}$ the orthogonal complement space of $\mathcal S_\kappa$.
We have $\mathbb R^p=\mathcal S_\kappa \oplus \mathcal  S_{\kappa}^{\bot}$.
If $\bbeta_b^\dagger \in \mathcal S_\kappa$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}>0.
\end{equation*}
On the other hand, if $\bbeta_b^\dagger \in \mathcal S_{\kappa}^\bot$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}\leq 0.
\end{equation*}
We would like to choose a hyperparameter $\kappa$ which consists the most consistent directions.
To achieve this, we maximize $k_{\kappa}$ with respect to $\kappa$.

\begin{proposition}\label{prop:monotone}
    For $ \kappa _2 > \kappa_1 > 0$, we have
    $k_{\kappa_1} \geq k_{\kappa_2}$.
    That is, $k_{\kappa}$ ($\kappa>0$) is decreasing in $\kappa$.
\end{proposition}
The proposition implies that we should put $\kappa$ as small as possible.
This motivates us to consider $B_{10}^0=\lim_{\kappa\to 0} B_{10}^\kappa$.
It is straightforward to show that
\begin{equation*}
    2\log B_{10}^0= 
         - \sum_{i=1}^{n-q}\log ( \gamma_i )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }\right)
         .
\end{equation*}
$B_{10}^0$ can be regarded as the Bayes factor with respect to noninformative prior.

Define
\begin{equation*}
    T_n= \frac{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }{\By^{*\top} \By^*} .
\end{equation*}
Then we reject the null hypothesis if $T_n$ is small.
It can be seen that under the null hypohtesis,
\begin{equation*}
    T_n \sim
    \frac{\sum_{i=1}^{n-q} \gamma_i^{-1} Z_i^2}{\sum_{i=1}^{n-q} Z_i^2},
    \end{equation*}
    where $\gamma_i$ is the $i$th eigenvalue of $  \BX_b^* \BX_b^{*\top}$, $i=1,\ldots, n-q$, and $Z_1,\ldots, Z_{n-q}$ are iid $\mathcal N(0,1)$ random variables.




\section{Asymptotic results}
Let $\bepsilon=(\epsilon_1,\ldots,\epsilon_n)^\top$, where $\epsilon_i$'s are iid random variable.
Denote $\mu_k=\myE \epsilon_1^k$.
Then $\mu_1=0$, $\mu_2=\phi^{-1}$.

\begin{assumption}
    Suppose 
\end{assumption}




\begin{lemma}
If $\phi^2\mu_4=o(n-q)$,
\begin{equation*}
    \By^{*\top} \By^*=(1+o_P(1))
    \left(
    \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q)
\right).
\end{equation*}
\end{lemma}
\begin{proof}
\begin{equation*}
    \By^{*\top} \By^* = \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    +2 \bepsilon^\top (\BI_n -\BP_a) \BX_b \bbeta_b + \bepsilon^\top (\BI_n - \BP_a) \bepsilon.
\end{equation*}


\begin{equation*}
    \myE \left(\By^{*\top} \By^*\right) = \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q).
\end{equation*}


\begin{equation*}
    \begin{split}
    \myVar \left( \By^{*\top} \By^* \right) 
    \leq  &
    2\myVar \left( 2 \bepsilon^\top (\BI_n -\BP_a) \BX_b \bbeta_b \right) + 2\myVar \left( \bepsilon^\top (\BI_n - \BP_a) \bepsilon \right)
    \\
    %=&8 \bbeta_b^\top \BX_b^\top (\BI_n -\BP_a) \BX_b \bbeta_b 
    \end{split}
\end{equation*}

From (i) of \cite[Proposition A.1]{chen2010tests},
\begin{equation*}
    \begin{split}
\myVar\left( \bepsilon^\top (\BI_n - \BP_a) \bepsilon \right)
&=
\phi^{-2}\left(
    (\phi^2 \mu_4 - 3) \sum_{i=1}^n \left((\BI_n -\BP_a)_{i,i}\right)^2
    + 2(n-q)
\right)
    \leq
    \phi^{-2}(2+\phi^2 \mu_4)(n-q)
    .
    \end{split}
\end{equation*}
Then
\begin{equation*}
    \begin{split}
    \myVar \left( \By^{*\top} \By^* \right) 
    \leq  &
    8 \phi^{-1} \bbeta_b^\top \BX_b^\top (\BI_n -\BP_a) \BX_b \bbeta_b 
    +
    2 \phi^{-2} (2+\phi^2 \mu_4) (n-q)
    \end{split}
\end{equation*}
Thus, if $\phi^2\mu_4=o(n-q)$, we have
\begin{equation*}
    \frac{
        \myVar \left( \By^{*\top} \By^* \right) 
    }{
        \left(\myE \left( \By^{*\top} \By^* \right) \right)^2
    }
    \to 0,
\end{equation*}
and consequently $\By^{*\top} \By^*=(1+o_P(1))\myE (\By^{*\top} \By^*)$.

\end{proof}


Note that under the normality, $T_n- \mytr ((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)$ has zero mean.

\begin{theorem}\label{generalTheorem}
    Let $\BA_n$ be an $(n-q)\times (n-q)$ symmetric matrix.
\begin{equation*}
    \left(
    \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q)
\right)
    \left(
        \frac{\By^{*\top} \BA_n \By^*}{\By^{*\top} \By^*} -\frac{\mytr (\BA_n)}{n-q}
    \right)
    \rightsquigarrow \mathcal N(0,1).
\end{equation*}
\end{theorem}
\begin{proof}
    \begin{equation*}
        \frac{\By^{*\top} \BA_n \By^*}{\By^{*\top} \By^*} -\frac{\mytr (\BA_n)}{n-q}
        =
        \frac{(\phi^{1/2}\By)^{\top}\left( \tilde{\BU}_a\BA_n \tilde{\BU}_a^\top
                -\frac{\mytr (\BA_n)}{n-q} \tilde{\BU}_a\tilde{\BU}_a^\top
        \right) (\phi^{1/2}\By)}{\phi\By^{*\top} \By^*}.
    \end{equation*}


\begin{equation*}
    \begin{split}
&(\phi^{1/2}\By)^{\top}\left( \tilde{\BU}_a\BA_n \tilde{\BU}_a^\top
                -\frac{\mytr (\BA_n)}{n-q} \tilde{\BU}_a\tilde{\BU}_a^\top
        \right) (\phi^{1/2}\By)
        \\
        =&
(\phi^{1/2}\bepsilon)^{\top}\left( \tilde{\BU}_a\BA_n \tilde{\BU}_a^\top
                -\frac{\mytr (\BA_n)}{n-q} \tilde{\BU}_a\tilde{\BU}_a^\top
        \right) (\phi^{1/2}\bepsilon)
        +
        \\
        &2\phi^{1/2} (\phi^{1/2}\bepsilon)^{\top}\left( \tilde{\BU}_a\BA_n \tilde{\BU}_a^\top
                -\frac{\mytr (\BA_n)}{n-q} \tilde{\BU}_a\tilde{\BU}_a^\top
        \right) \BX_b \bbeta_b
        +
        \\
        & \phi
\bbeta_b^\top \BX_b^\top \left( \tilde{\BU}_a\BA_n \tilde{\BU}_a^\top
                -\frac{\mytr (\BA_n)}{n-q} \tilde{\BU}_a\tilde{\BU}_a^\top
        \right) \BX_b \bbeta_b
    \end{split}
\end{equation*}

    From \cite[Theorem 5.1]{jiang1996reml},
\end{proof}


As in \cite{Roman2018}, sub-gaussian norm of a sub-gaussian random variable is defined as
\begin{equation*}
    \|X\|_{\psi_2} = \inf \{t>0: \myE \exp(X^2/t^2) \leq 2\}.
\end{equation*}
A random vector $Z\in \mathbb R^p$ is called sub-gaussian if $z\top Z$ are sub-gaussian random variables for all $z\in \mathbb R^p$.
The sub-gaussian norm of $Z$ is defined as
\begin{equation*}
    \|Z\|_{\psi_2}= \sup_{z\in S^{p-1}} \|z^\top Z\|_{\psi_2},
\end{equation*}
where $S^{p-1}$ is the unit sphere in $\mathbb R^p$.

Suppose $\BX_b =\BZ_b \Gamma + \mathbf 1_n \mu_b^\top$, where the rows of $\BZ_b$ are iid sub-gaussion random vectors with identity covariance matrix.


The following lemma is a simple extension of Theorem 4.6.1 of \cite{Roman2018}.
\begin{lemma}\label{lemma:con}
    Let $\BZ$ be an $N\times n$ random matrix whose columns $Z_i$ are independent sub-gaussian random vectors with $\myE (Z_i)=0$, $\myVar (Z_i)= \BI_n$.
    Suppose $K:=\max_i \|Z_i\|_{\psi_2}$ is uniformly bounded.
    Write $Z_i=(z_{i1},\ldots,z_{iN})^\top$.
    Assume that $\myE (z_{i\ell}^4)=3 + \Delta <\infty$ and for any intergers $\ell_v \geq 0$ with $\sum_{v=1}^s \ell_v \leq 4$,
    \begin{equation*}
        \myE (Z_{i j_1}^{\ell_1}Z_{i j_2}^{\ell_2}\cdots Z_{i j_s}^{\ell_s}) 
        =
    \myE (Z_{i j_1}^{\ell_1})\myE (Z_{i j_2}^{\ell_2})\cdots \myE (Z_{i j_s}^{\ell_s}) 
    \end{equation*}
    Let $\BW$ be a nonrandom $N \times N$ symmetric matrix.
    Then
    \begin{equation*}
        \|\BZ^\top \BW \BZ -\mytr (\BW) \BI_n\| 
        =
        O_P(\sqrt n \|\BW\|_F+ n \|\BW\|).
    \end{equation*}
    TO BE DONE: 
    \begin{equation*}
        \|U\BZ^\top \BW \BZ U^\top -\mytr (\BW) \BI_n\| 
    \end{equation*}
\end{lemma}


Next we verify the following.
Let $\bSigma=\Gamma^\top \Gamma$.
\begin{proof}
    Let
    \begin{equation*}
    \BB=\BX_b^* \BX_b^{*\top} 
    =
    \tilde{\BU}_a^\top \BX_b \BX_b^{\top} \tilde{\BU}_a 
    =
    \tilde{\BU}_a^\top \BZ_b\Gamma \Gamma^\top \BZ_b^{\top} \tilde{\BU}_a 
    \end{equation*}
    Note that Lemma \ref{lemma:con} implies that
    \begin{equation*}
        \|\BB-\mytr(\bSigma)\BI_{n-q} \|
        = 
        O_P(\sqrt{n} \|\bSigma\|_F + n \|\bSigma\|).
    \end{equation*}
    That is, uniformly for $i=1,\ldots,n-q$,
    \begin{equation*}
        \frac{ \lambda_i (\BB) } {\mytr (\bSigma)} = 1 + O_P \left( \frac{\sqrt n \|\bSigma\|_F}{\mytr(\bSigma)} + \frac{n \|\bSigma\|}{\mytr(\bSigma)}\right)
    \end{equation*}
    Define
    \begin{equation*}
        \delta_i= \frac{ \lambda_i (\BB) } {\mytr (\bSigma)}-1
    \end{equation*}
    \begin{equation*}
        \eta= \frac{\sqrt n \|\bSigma\|_F}{\mytr(\bSigma)} + \frac{n \|\bSigma\|}{\mytr(\bSigma)}
    \end{equation*}
    We assume $\eta \to 0$.

    Thus,
    we need to verify 
    \begin{equation*}
        \tilde{\BU}_a \BB^{-1} \tilde{\BU}_a^\top-\frac{\mytr( \BB^{-1} )}{n-q}\tilde{\BU}_a\tilde{\BU}_a^\top
    \end{equation*}
    satisfies that
    \begin{equation}
        \left\|
            \BB^{-1} -\frac{\mytr( \BB^{-1} )}{n-q} \BI_{n-q}
        \right\|^2
        \bigg/
        \mytr\left(
            \BB^{-1} -\frac{\mytr( \BB^{-1} )}{n-q} \BI_{n-q}
        \right)^2
        \to 0.
    \end{equation}
    Note that
    \begin{equation*}
        \mytr\left(
            \BB^{-1} -\frac{\mytr( \BB^{-1} )}{n-q} \BI_{n-q}
        \right)^2
        =\sum_{i=1}^{n-q} \frac{1}{\lambda_i^2(\BB)}
        -\frac{1}{n-q}\left(\sum_{i=1}^{n-q}\frac{1}{\lambda_i(\BB)}\right)^2.
    \end{equation*}
    By Taylor's theorem, uniformly for $i=1,\ldots, n$, we have
    \begin{equation*}
        \begin{split}
            &
        \frac{1}{\lambda_i (\BB) }=\frac{1}{\mytr(\bSigma)}
        \frac{1}{1+\delta_i}=
        \frac{1}{\mytr(\bSigma)}\left(
            1-\delta_i +\delta_i^2 +O_P(\eta^3)
        \right),
        \\
        &
        \frac{1}{\lambda_i^2 (\BB) }=\frac{1}{\mytr^2(\bSigma)}
        \frac{1}{(1+\delta_i)^2}=
        \frac{1}{\mytr^2(\bSigma)}\left(
            1-2\delta_i +3\delta_i^2 +O_P(\eta^3)
        \right).
        \end{split}
    \end{equation*}
    Thus,
    \begin{equation*}
        \begin{split}
        &\mytr\left(
            \BB^{-1} -\frac{\mytr( \BB^{-1} )}{n-q} \BI_{n-q}
        \right)^2
        \\
        =&
        \sum_{i=1}^{n-q}         \frac{1}{\mytr^2(\bSigma)}\left(
            1-2\delta_i +3\delta_i^2 +O_P(\eta^3)
        \right)
        -\frac{1}{n-q}\left(\sum_{i=1}^{n-q}
        \frac{1}{\mytr(\bSigma)}\left(
            1-\delta_i +\delta_i^2 +O_P(\eta^3)
        \right)
        \right)^2
        \\
        =&
\frac{1}{\mytr^2(\bSigma)}
\left(
    n-q
        -2\sum_{i=1}^{n-q} \delta_i 
             +3\sum_{i=1}^{n-q}\delta_i^2 +O_P(n\eta^3)
        -\frac{1}{n-q}\left(
            n-q - \sum_{i=1}^{n-q}\delta_i
            +\sum_{i=1}^{n-q}\delta_i^2 +O_P(n\eta^3)
        \right)^2
\right) 
\\
        =&
\frac{1}{\mytr^2(\bSigma)}
\left(
    n-q
        -2\sum_{i=1}^{n-q} \delta_i 
             +3\sum_{i=1}^{n-q}\delta_i^2 +O_P(n\eta^3)
             -(n-q)\left(
                 1 - \frac{1}{n-q}\sum_{i=1}^{n-q}\delta_i
                 +\frac{1}{n-q}\sum_{i=1}^{n-q}\delta_i^2 +O_P(\eta^3)
        \right)^2
\right) 
\\
        =&
\frac{1}{\mytr^2(\bSigma)}
\Bigg(
    n-q
        -2\sum_{i=1}^{n-q} \delta_i 
             +3\sum_{i=1}^{n-q}\delta_i^2 +O_P(n\eta^3)
             \\
             &-(n-q)\left(
                 1 + \left(\frac{1}{n-q}\sum_{i=1}^{n-q}\delta_i\right)^2
                 -\frac{2}{n-q}\sum_{i=1}^{n-q}\delta_i
                 +\frac{2}{n-q}\sum_{i=1}^{n-q}\delta_i^2 +O_P(\eta^3)
        \right)
\Bigg) 
\\
        =&
\frac{1}{\mytr^2(\bSigma)}
\Bigg(
             \sum_{i=1}^{n-q}\delta_i^2 
             -
\frac{1}{n-q}
                   \left(\sum_{i=1}^{n-q}\delta_i\right)^2
+O_P(n\eta^3)
\Bigg) 
\\
=&
\frac{1}{\mytr^2(\bSigma)}
\Bigg(
    \mytr\left(\frac{1}{\mytr (\bSigma)} \BB -\BI_{n-q}\right)^2
             -
\frac{1}{n-q}
\left(\frac{\mytr(\BB)}{\mytr(\bSigma)}-(n-q)\right)^2
+O_P(n\eta^3)
\Bigg) 
\\
=&
\frac{1}{\mytr^2(\bSigma)}
\Bigg(
    \frac{1}{\mytr^2 (\bSigma)}
    \left(
        \mytr (\BB^2) -\frac{1}{n-q} \mytr^2 (\BB)
    \right)
+O_P(n\eta^3)
\Bigg) 
        .
        \end{split}
    \end{equation*}
\end{proof}

\begin{lemma}
    Let $\BZ$ be an $n\times m$ random matrix whose rows $Z_i$ are independent random vectors with $\myE (Z_i)=0$, $\myVar (Z_i)= \BI_m$.
    Write $Z_i=(z_{i1},\ldots,z_{im})^\top$.
    Assume that $\myE (z_{i\ell}^4)=3 + \Delta <\infty$ and for any intergers $\ell_v \geq 0$ with $\sum_{v=1}^s \ell_v \leq 8$.
    Let $\BB$ be a $m\times m$ symmetric matrix.
    Let $\BP$ be a $n\times n$ symmetric maitrx.
    Then
    \begin{equation*}
    \mytr(\BP \BZ \BB \BZ^\top \BP)^2 - \frac{1}{\mytr(\BP^2)}\left( \mytr(\BP \BZ \BB \BZ^\top \BP)\right)^2
    \end{equation*}
\end{lemma}
\begin{proof}
    Let $\BQ=\BP^2$,
\begin{equation*}
    \begin{split}
    \mytr(\BP \BZ \BB \BZ^\top \BP)
    =&
    \mytr( \BZ \BB \BZ^\top \BQ)\\
    =&\sum_{i=1}^n \sum_{j=1}^n  Z_{i}^\top \BB Z_{j} q_{j,i}
    \\
    =&
    \sum_{i=1}^n q_{i,i} Z_{i}^\top \BB Z_{i} 
    +
    \sum_{i=1}^n \sum_{j\neq i}^n q_{i,j} Z_{i}^\top \BB Z_{j}
    \end{split}
\end{equation*}
Then
\begin{equation*}
    \left(\mytr(\BP \BZ \BB \BZ^\top \BP)\right)^2
    =
    \sum_{i=1}^n
    \sum_{i=1}^n
    q_{i,i} q_{j,j} Z_{i}^\top \BB Z_{i} 
     Z_{j}^\top \BB Z_{j} 
    +
     \sum_{i\neq j}^n
     \sum_{k\neq l}^n
     q_{i,j}
     q_{k,l}
     Z_{i}^\top \BB Z_{j}
      Z_{k}^\top \BB Z_{l}
      +\CG{what?}
\end{equation*}
On the other hand,
\begin{equation*}
    \begin{split}
    &\mytr(\BP \BZ \BB \BZ^\top \BP)^2
    \\
    =&
    \mytr(\BZ \BB \BZ^\top \BQ )^2
    \\
    =&
    \sum_{i=1}^n\sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n q_{j,k} q_{l,i} Z_{i}^\top \BB Z_{j} 
      Z_{k}^\top \BB Z_{l} 
      \\
    =&
    \sum_{i\neq j}^n \sum_{k\neq l}^n  q_{j,k} q_{l,i} Z_{i}^\top \BB Z_{j} 
      Z_{k}^\top \BB Z_{l} 
      +
    2\sum_{i=1}^n \sum_{j\neq k}^n  q_{i,j} q_{i,k} Z_{i}^\top \BB Z_{i} 
      Z_{j}^\top \BB Z_{k} 
      +
    \sum_{i=1}^n \sum_{j=1}^n  q_{i,j}^2 Z_{i}^\top \BB Z_{i} 
      Z_{j}^\top \BB Z_{j} 
      .
    \end{split}
\end{equation*}
Thus,
\begin{equation*}
\begin{split}
    &\mytr(\BP \BZ \BB \BZ^\top \BP)^2-
\frac{1}{\mytr(\BP^2)}\left( \mytr(\BP \BZ \BB \BZ^\top \BP)\right)^2
\\
=&
\sum_{i\neq j}^n \sum_{k\neq l}^n  \left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right) Z_{i}^\top \BB Z_{j} 
      Z_{k}^\top \BB Z_{l} 
      +
    2\sum_{i=1}^n \sum_{j\neq k}^n  q_{i,j} q_{i,k} Z_{i}^\top \BB Z_{i} 
      Z_{j}^\top \BB Z_{k} 
      \\
      &+
      \sum_{i=1}^n \sum_{j=1}^n  \left(q_{i,j}^2 - C q_{i,i} q_{j,j} \right) Z_{i}^\top \BB Z_{i} 
      Z_{j}^\top \BB Z_{j} 
      \\
      =:& A_1 + A_2 +A_3
\end{split}
\end{equation*}

en

\begin{equation*}
    \begin{split}
        A_1=&\sum_{i\neq j}^n \sum_{k=i,l=j} +
\sum_{i\neq j}^n \sum_{k=j,l=i}
+
\sum_{i\neq j}^n \sum_{k=i,l\notin\{i,j\}}
+
\sum_{i\neq j}^n \sum_{k=j,l\notin\{i,j\}}
+
\sum_{i\neq j}^n \sum_{l=i,k\notin\{i,j\}}
+
\sum_{i\neq j}^n \sum_{l=j,k\notin\{i,j\}}
+
\sum_{i\neq j}^n \sum_{k\notin\{i,j\}, l\notin\{i,j\}, k\neq l}
\\
=:& 
A_{1,1}+\cdots+A_{1,7}
    \end{split}
\end{equation*}
Note that
\begin{equation*}
    A_1^{(1)}:=A_{1,1}+A_{1,2}=\sum_{i\neq j}( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 ) (Z_i^\top \BB Z_j)^2
\end{equation*}
and
\begin{equation*}
    A_1^{(2)}:=
    A_{1,3}+
    A_{1,4}+
    A_{1,5}+
    A_{1,6}=
    \sum_{i,j,k \text{ distinct}}
    \left(2 q_{i,i}q_{j,k}-(2-4C) q_{i,j} q_{i,k}\right)
    Z_{i}^\top \BB Z_j Z_i^\top \BB Z_k
\end{equation*}
and
\begin{equation*}
    A_1^{(3)}:=
    A_{1,7}=\sum_{i,j,k,l\text{ distinct}}
\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right) Z_{i}^\top \BB Z_{j} 
      Z_{k}^\top \BB Z_{l}
\end{equation*}

For $A_1^{(3)}$, we have,
\begin{equation*}
    \myE A_{1}^{(3)}= 0 
\end{equation*}
and
\begin{equation*}
    \begin{split}
    &\myE (A_{1}^{(3)})^2 
    \\
    =&
    \myE
    \sum_{i,j,k,l \text{ distinct}}
    \sum_{\sigma: \text{permutation of $i,j,k,l$}}^{24}
    \\
    &
\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right) Z_{i}^\top \BB Z_{j} 
Z_{k}^\top \BB Z_{l}
\left(q_{\sigma(i),\sigma(l)} q_{\sigma(j),\sigma(k)} -C q_{\sigma(i),\sigma(j)} q_{\sigma(k),\sigma(l)}\right) Z_{\sigma(i)}^\top \BB Z_{\sigma(j)} 
Z_{\sigma(k)}^\top \BB Z_{\sigma(l)}
\\
\leq &
    \sum_{i,j,k,l \text{ distinct}}
    \sum_{\sigma: \text{permutation of $i,j,k,l$}}^{24}
    \\
    &
\left|\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right) 
\left(q_{\sigma(i),\sigma(l)} q_{\sigma(j),\sigma(k)} -C q_{\sigma(i),\sigma(j)} q_{\sigma(k),\sigma(l)}\right)
\right|
\\
    &\myE
    \left|
Z_{i}^\top \BB Z_{j} 
Z_{k}^\top \BB Z_{l}
 Z_{\sigma(i)}^\top \BB Z_{\sigma(j)} 
Z_{\sigma(k)}^\top \BB Z_{\sigma(l)}
\right|
\\
\leq &
    \sum_{i,j,k,l \text{ distinct}}
    \sum_{\sigma: \text{permutation of $i,j,k,l$}}^{24}
    \\
    &
\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right)^2+ 
\left(q_{\sigma(i),\sigma(l)} q_{\sigma(j),\sigma(k)} -C q_{\sigma(i),\sigma(j)} q_{\sigma(k),\sigma(l)}\right)^2
\\
    &
    \sqrt{\myE
    \left(
Z_{i}^\top \BB Z_{j} 
Z_{k}^\top \BB Z_{l}\right)^2
\myE\left(
 Z_{\sigma(i)}^\top \BB Z_{\sigma(j)} 
Z_{\sigma(k)}^\top \BB Z_{\sigma(l)}
\right)^2
}
\\
=&
48
    \sum_{i,j,k,l \text{ distinct}}
\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right)^2
\mytr^2(\BB^2)
\\
\leq&
48
    \sum_{i,j,k,l \text{ distinct}}
\left(q_{i,l} q_{j,k} -C q_{i,j} q_{k,l}\right)^2
\mytr^2(\BB^2)
\\
\leq &
96 (1+C^2) 
    \sum_{i,j,k,l \text{ distinct}}
    \left(q_{i,l} q_{j,k}\right)^2
    \mytr^2 (\BB^2)
    \\
\leq &
96 (1+C^2) 
    \sum_{i\neq j}
    \sum_{k\neq l}
    \left(q_{i,j} q_{k,l}\right)^2
    \mytr^2 (\BB^2)
    \\
\leq &
96 (1+C^2) 
    \left(
        \sum_{i\neq j}
    \left(q_{i,j} \right)^2
\right)^2
    \mytr^2 (\BB^2)
    \end{split}
\end{equation*}

For $A_1^{(2)}$,
we have $\myE A_1^{(2)}=0$, and
\begin{equation*}
    \begin{split}
        &\myE \left( A_1^{(2)}\right)^2
        \\
        =
        &
    \sum_{i,j,k \text{ distinct}}
    \\
    &
    \sum_{i',j',k' \text{ distinct}}
    \left(2 q_{i,i}q_{j,k}-(2-4C) q_{i,j} q_{i,k}\right)
    Z_{i}^\top \BB Z_j Z_i^\top \BB Z_k
    \left(2 q_{i',i'}q_{j',k'}-(2-4C) q_{i',j'} q_{i',k'}\right)
    Z_{i'}^\top \BB Z_{j'} Z_{i'}^\top \BB Z_{k'}
    \\
        =
        &
    \sum_{i,j,k \text{ distinct}}
    \left(
        \sum_{\{i',j',k'\}=\{i,j,k\}}
        +
        \sum_{\{j',k'\}=\{j,k\},i'\notin \{j,k\}}
\right)
    \end{split}
\end{equation*}
Note that
\begin{equation*}
    \begin{split}
    &
    \sum_{i,j,k \text{ distinct}}
    \sum_{\{i',j',k'\}=\{i,j,k\}}
    \\
    \leq&
    \sum_{i,j,k \text{ distinct}}
    \sum_{\{i',j',k'\}=\{i,j,k\}}
    \left(
    \left(2 q_{i,i}q_{j,k}-(2-4C) q_{i,j} q_{i,k}\right)^2
    +
    \left(2 q_{i',i'}q_{j',k'}-(2-4C) q_{i',j'} q_{i',k'}\right)^2
\right)
    \myE (Z_1^\top \BB Z_2)^4
    \\
    =&
    12
    \sum_{i,j,k \text{ distinct}}
    \left(2 q_{i,i}q_{j,k}-(2-4C) q_{i,j} q_{i,k}\right)^2
    \myE (Z_1^\top \BB Z_2)^4
    \\
    \leq&
    96
    \sum_{i,j,k \text{ distinct}}
\left(
    \left( q_{i,i}q_{j,k}\right)^2+(1-2C)^2\left( q_{i,j} q_{i,k}\right)^2
\right)
    \myE (Z_1^\top \BB Z_2)^4
    \\
    =&
    O(1)
    \left(
        (\sum_{i=1}^n q_{i,i}^2)
        (\sum_{i\neq j}^n q_{i,j}^2)
        +
        (1-2C)^2
        \sum_{i=1}^n \left(\sum_{j:j\neq i} q_{i,j}^2\right)^2
\right)
    \mytr^2(\BB^2)
    \end{split}
\end{equation*}
Now we deal with $A_1^{(1)}$.
\begin{equation*}
    A_1^{(1)}=\sum_{i\neq j}( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 ) (Z_i^\top \BB Z_j)^2
\end{equation*}

\begin{equation*}
    \begin{split}
    \myE A_1^{(1)}
    &=
    \sum_{i\neq j} q_{i,i} q_{j,j}  \mytr(\BB^2)
    + \sum_{i\neq j}(1-2C) q_{i,j}^2  \mytr(\BB^2)
    \\
    &=
\left((\sum_{i=1}^n q_{i,i})^2 -\sum_{i=1}^n q_{i,i}^2\right)  \mytr(\BB^2)
    + \sum_{i\neq j}(1-2C) q_{i,j}^2  \mytr(\BB^2)
    \\
    \end{split}
\end{equation*}
And
\begin{equation*}
    \begin{split}
    &\myVar \left( A_1^{(1)} \right)
    \\
    =
    &
    \myE \left(
    \sum_{i\neq j}( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 ) \left( (Z_i^\top \BB Z_j)^2- \mytr (\BB^2)\right)
\right)^2
\\
=
    &
    2\sum_{i\neq j}( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 )^2 \myE\left( (Z_i^\top \BB Z_j)^2- \mytr (\BB^2)\right)^2
    \\
    &+
    4\sum_{i,j,k}^*( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 ) ( q_{i,i} q_{k,k} + (1-2C) q_{i,k}^2 ) \myE\left( (Z_i^\top \BB Z_j)^2- \mytr (\BB^2)\right)\left( (Z_i^\top \BB Z_k)^2- \mytr (\BB^2)\right)
\\
=
    &
    2\sum_{i\neq j}( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 )^2 \myE\left( (Z_i^\top \BB Z_j)^2- \mytr (\BB^2)\right)^2
    \\
    &+
    4\sum_{i,j,k}^*( q_{i,i} q_{j,j} + (1-2C) q_{i,j}^2 ) ( q_{i,i} q_{k,k} + (1-2C) q_{i,k}^2 ) 
    \myE\left( Z_i^\top \BB^2 Z_i - \mytr (\BB^2)\right)^2
    \end{split}
\end{equation*}

But
\begin{equation*}
    \begin{split}
        \myE\left( (Z_i^\top \BB Z_j)^2- \mytr (\BB^2)\right)^2
        =
        \myE(Z_i^\top \BB Z_j)^4- \mytr^2 (\BB^2)
        =O(\mytr^2(\BB^2))
    \end{split}
\end{equation*}
and
\begin{equation*}
    \myE\left( Z_i^\top \BB^2 Z_i - \mytr (\BB^2)\right)^2
    \leq
    \sqrt{
        \myE\left( Z_i^\top \BB^2 Z_i - \mytr (\BB^2)\right)^4
    }
    =O(\mytr(\BB^4)).
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
    &\myVar \left( A_1^{(1)} \right)
    \\
    =&O(\mytr^2(\BB^2)) \left(
        (\sum_{i=1}^n q_{i,i}^2)^2
        +
        (1-2C)^2
        \sum_{i\neq j} q_{i,j}^4
    \right)
    \\
    &+
    O(\mytr(\BB^4))
    \left(
        (\sum_{i=1}^n q_{i,i}^2)
        (\sum_{i=1}^n q_{i,i})^2
        +(\sum_{i=1}^n q_{i,i})
        (\sum_{i=1}^n\sum_{j=1}^n q_{i,i}q_{i,j}^2)
        +
        \sum_{i=1}^n (\sum_{j=1}^n q_{i,j}^2)^2
    \right)
    \end{split}
\end{equation*}

Now we deal with $A_2$.
\begin{equation*}
    \begin{split}
        A_2=&
        \sum_{i=1}^n \sum_{j\neq k}^n q_{i,j} q_{i,k} Z_i^\top \BB Z_i Z_j^\top \BB Z_k
        \\
    =&
    2\sum_{i,j}^* q_{i,j} q_{i,i} Z_i^\top \BB Z_i Z_i^\top \BB Z_j
        +
        \sum_{i,j,k}^*  q_{i,j} q_{i,k} Z_i^\top \BB Z_i Z_j^\top \BB Z_k
\end{split} 
\end{equation*}

\end{proof}








%\section*{Acknowledgements}
%This work was supported by the National Natural Science Foundation of China under Grant Nos.\ xxxxx, xxxx.




\begin{appendices}

    \section{haha1}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:unbiased}}]
    We assume $0<\alpha<1$ since the case $\alpha=0$ or $1$ is trivial.
    Note that the condition implies $\int [\varphi(\By)-\alpha] \mathcal N_n (0,\phi^{-1} \BI_n) (d\By)=0$.
    Hence it suffices to prove $\varphi(\By) \geq \alpha$, a.s. 
    We prove this by contradiction.
    Suppose $\lambda(\{\By:\varphi (\By) <\alpha\})>0$.
    Then there exists a $\eta >0$, such that $\lambda(\{\By:\varphi (\By) <\alpha-\eta\})>0$.
    We denote $E=\{\By:\varphi (\By) <\alpha-\eta\}$.
    From Lebesgue density theorem \citep[Corollary 6.2.6]{book:992991}, there exists a point $z\in E$, such that, for each $\epsilon >0$ there is a $\delta_{\epsilon}>0$ such that
    \begin{equation*}
        \left|\frac{\lambda(E^\complement\cap C_{\epsilon})}{\lambda(C_{\epsilon})}\right|<\epsilon,
    \end{equation*}
    where $C_{\epsilon}=\prod_{i=1}^n [z_i-\delta_{\epsilon}, z_i + \delta_{\epsilon}]$.
    We put
    \begin{equation*}
        \epsilon=\left(\frac{\sqrt \pi}{\sqrt 2 \Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^n \frac{\eta}{3}.
    \end{equation*}
    Then for any $\phi>0$,
    \begin{equation*}
        \begin{split}
            \alpha \leq& 
            \int_{\mathbb R^n}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            =&
            \int_{E\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{E^\complement\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \int_{E^\complement\cap C_{\epsilon}} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}\lambda(E^\complement\cap C_{\epsilon})
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}
            \epsilon
            (2\delta_\epsilon)^n
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            =&
            \alpha-\eta
            +
            \left(\frac{\sqrt{\phi} \delta_{\epsilon}}{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^{n}
            \frac{\eta}{3}
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right).
        \end{split}
    \end{equation*}
    Putting 
    \begin{equation*}
        \phi = \left(\frac{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}{\delta_\epsilon}\right)^2
    \end{equation*}
    yields the contradiction $\alpha\leq \alpha-(2/3)\eta$.
    This completes the proof.

\end{proof}


\begin{proof}[\textbf{Proof of Proposition \ref{prop:monotone}}]
    For positive integer $m$, define $[m]=\{1,,\ldots, m\}$.
    For a set $A$, denote by $| A |$ its cardinality.
    We have
    \begin{equation*}
        \begin{split}
        k_{\kappa} =& \left|\left\{i\in [n-q]: \frac{\gamma_i^2}{\gamma_i +\kappa} - \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j \gamma_i}{\gamma_j +\kappa}>0 \right\}\right|
        \\
        =& \left|\left\{i\in [n-q]: \frac{\gamma_i}{\gamma_i +\kappa} > \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j }{\gamma_j +\kappa} \right\}\right|
        .
        \end{split}
    \end{equation*}
    Let $X$ be a random variable uniformly distributed on $\{\gamma_1,\ldots,\gamma_{n-q}\}$.
    That is, $\Pr(X=\gamma_i)=1/(n-q)$, $i=1,\ldots, n-q$.
    Then it can be seen that
    \begin{equation*}
        k_{\kappa}=(n-q) \Pr \left(\frac{X}{X+\kappa}>\myE \left[\frac{X}{X+\kappa}\right]\right).
    \end{equation*}
    Hence we only need to verify
    \begin{equation}\label{eq:toBeJen}
        \Pr \left(\frac{X}{X+\kappa_1}>\myE \left[\frac{X}{X+\kappa_1}\right]\right) 
\geq
\Pr \left(\frac{X}{X+\kappa_2}>\myE \left[\frac{X}{X+\kappa_2}\right]\right) .
    \end{equation}
    Let $Y=X/(X+\kappa_2)$.
    Then
    \begin{equation*}
        \frac{X}{(X+\kappa_1)} = \frac{\kappa_2 Y}{ \kappa_1 + (\kappa_2-\kappa_1) Y} := f(Y).
    \end{equation*}
    Note that $f(Y)$ is increasing for $Y\geq 0$.
    Then the inequality \eqref{eq:toBeJen} is equivalent to
    \begin{equation*}
        \Pr \left(Y> f^{-1}\left(\myE f(Y)\right)\right) 
\geq
\Pr \left( Y >\myE Y\right) .
    \end{equation*}
    Hence we only need to verify
        $f^{-1}\left(\myE f(Y)\right)
        \leq
        \myE Y$, or equivalently, $\myE f(Y)
        \leq
        f(\myE Y)$.
        But the last inequality is a direct consequence of the concavity of $f(Y)$.
        This completes the proof.

\end{proof}


\begin{lemma}
    Let $\BW$ be an $N\times N$ positive semi-definite matrix.
    Let $Z$ be an $N$ dimensional sub-gaussian random vector with $\myE Z=0$, $\myVar (Z)= \BI_n$ and $\|Z\|_{\psi_2} \leq K$.
    For all $t>0$,
    \begin{equation*}
        \Pr\left\{
            Z^\top \BW Z >
        \right\} 
        \leq e^{-t}.
    \end{equation*}
\end{lemma}
\begin{remark}
    This lemma is adapted from \cite{Hsu2012} Theorem 2.1 with minor modifications.
    Indeed, their result did not track the variance of $Z$.
\end{remark}
\begin{proof}
    Let $\BA=\sqrt{\BW}$.
    Let $Z^*$ be a vector of $N$ independent standard Gaussian random variables which are independent of $Z$.  
    \begin{equation*}
        \myE [\exp (\lambda Z^{*\top } \BA Z)]
        =
        \myE\myE [\exp (\lambda Z^{*\top } \BA Z)| Z]
        =\myE[\exp (\frac{\lambda^2}{2} Z^\top \BW Z)]
    \end{equation*}
\end{proof}
\begin{lemma}
    
\end{lemma}
\begin{proof}
    
\end{proof}

\begin{proof}[\textbf{Proof of Lemma \ref{lemma:con}}]
\begin{equation*}
    \begin{split}
    \|\BZ^\top \BW \BZ -\mytr (\BW) \BI_n\| 
    \leq
    \|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\| 
    + 
    \| \mydiag(\BZ^\top \BW \BZ) -\mytr (\BW) \BI_n\| 
    \end{split}
\end{equation*}
We have
\begin{equation*}
    \| \mydiag(\BZ^\top \BW \BZ) -\mytr (\BW) \BI_n\| 
    =
    \max_{1\leq i\leq n}
    |Z_i^\top \BW Z_i- \mytr (\BW)|
    \leq
    \sqrt{
\sum_{i=1}^{ n}
\left(Z_i^\top \BW Z_i- \mytr (\BW)\right)^2
}
\end{equation*}
From \cite[Proposition A.1]{chen2010tests},
\begin{equation*}
    \myE \left[\sum_{i=1}^n \left(Z_i^\top \BW Z_i- \mytr (\BW)\right)^2\right]
    =2n \mytr(\BW^2) + \Delta n \mytr(\BW\circ\BW)
    \leq (2+\Delta) n \mytr(\BW^2).
\end{equation*}
Hence
\begin{equation*}
    \| \mydiag(\BZ^\top \BW \BZ) -\mytr (\BW) \BI_n\| 
    =O_P(\sqrt n \|\BW\|_F).
\end{equation*}
Next we deal with
\begin{equation*}
    \|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\| 
\end{equation*}
From \cite[Lemma 5.2]{Roman2018}, there is a $1/4$-net $\mathcal C$ of the unit sphere $S^{n-1}$ such that $|\mathcal C|\leq 9^n$.
By \cite[Exercise 4.4.3]{Roman2018}, 
\begin{equation*}
    \|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\|\leq 2 \sup_{x\in\mathcal C} \left|x^\top \left(\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ) \right) x\right|.
\end{equation*}
Fix $x\in \mathcal C$.
Then
\begin{equation*}
 \left|x^\top \left(\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ) \right) x\right|
 =
 \left|
 \sum_{i=1}^{n} \sum_{j\neq i}^n
 x_i x_j Z_i^\top \BW Z_j  
 \right|
\end{equation*}
Now we bound the moment generating function of $\sum_{i=1}^{n} \sum_{j\neq i}^n
 x_i x_j Z_i^\top \BW Z_j$.
 We apply the decoupling technique in \cite{Roman2018}, Section 6.1.
 Let $\delta_1,\ldots,\delta_n$ be independent Bernoulli random variables with $\Pr\{\delta_i=0\}=\Pr\{\delta_i=1\}=1/2$.
For any $\lambda \in \mathbb R$,
\begin{equation*}
    \begin{split}
    \myE
    \exp
    \left\{
        \lambda
 \sum_{i=1}^{n} \sum_{j\neq i}^n
 x_i x_j Z_i^\top \BW Z_j  
 \right\}
 =&
    \myE
    \exp
    \left\{
        \myE\left(
            4
        \lambda
 \sum_{i=1}^{n} \sum_{j=1}^n
 \delta_i (1-\delta_j) x_i x_j Z_i^\top \BW Z_j  
 \bigg | \BZ \right)
 \right\}
 \\
 \leq &
        \myE
    \exp
    \left\{
            4
        \lambda
 \sum_{i=1}^{n} \sum_{j=1}^n
 \delta_i (1-\delta_j) x_i x_j Z_i^\top \BW Z_j  
 \right\}
 \\
 = &
        \myE
    \exp
    \left\{
            4
        \lambda
        (\sum_{i:\delta_i=1} x_i Z_i)^\top \BW
         (\sum_{j:\delta_j=0}
         x_j  Z_j  )
 \right\}
 \\
 \leq&
 \max_{I\subset [n]}
        \myE
    \exp
    \left\{
            4
        \lambda
        (\sum_{i\in I}x_i  Z_i)^\top \BW (\sum_{j\notin I}x_j Z_j  )
 \right\},
    \end{split}
\end{equation*}
where the first inequality follows from Jensen's inequality.
Fix an $I\subset [n]$.
From \cite{Roman2018}, Proposition 2.6.1, $\|\sum_{ i \in I} x_i Z_i\|_{\psi_2}\leq C_1 K$, $\|\sum_{ j \notin I} x_j Z_j\|_{\psi_2}\leq C_1 K$ for some absolute constant $C_1$.
Then \cite{Roman2018}, Lemma 6.2.2 and Lemma 6.2.3 imply that
there exist absolute constants $C_2,C_3$ such that, 
\begin{equation*}
        \myE
    \exp
    \left\{
            4
        \lambda
        (\sum_{i\in I}x_i  Z_i)^\top \BW (\sum_{j\notin I}x_j Z_j  )
 \right\}
 \leq
 \exp
 \left\{
     C_2 K^4
 \|\BW\|_F^2
     \lambda^2 
 \right\}
\end{equation*}
for all 
$|\lambda|\leq {C_3}/{(K^2\|\BW\|)}$.
Note that this bound does not depend on $I\subset [n]$.
It follows that
\begin{equation*}
    \begin{split}
    &\myE
    \exp
    \left\{
        \lambda
 \sum_{i=1}^{n} \sum_{j\neq i}^n
 x_i x_j Z_i^\top \BW Z_j  
 \right\}
 \leq 
\exp\left\{
    C_2 K^4 \|\BW\|_F^2 \lambda^2
\right\}
,
    \end{split}
\end{equation*}
for all $|\lambda|\leq {C_3}/{( K^2 \|\BW\|)}$.
Then applying Chernoff bound yields that, for any $t>0$,
\begin{equation*}
    \begin{split}
        \Pr\left(
 \left|
 \sum_{i=1}^{n} \sum_{j\neq i}^n
 x_i x_j Z_i^\top \BW Z_j  
 \right| > t
        \right) 
        %\\
        %\leq
        %&\inf_{0<\lambda\leq \frac{C_3}{ K^2 \|\BW\|}}
        %\exp(-\lambda t)
        %\myE 
        %\left(
        %\exp
    %\left\{
        %\lambda
 %\sum_{i=1}^{n} \sum_{j\neq i}^n
 %x_i x_j Z_i^\top \BW Z_j  
 %\right\}
 %+
        %\exp
    %\left\{
        %-\lambda
 %\sum_{i=1}^{n} \sum_{j\neq i}^n
 %x_i x_j Z_i^\top \BW Z_j  
 %\right\}
 %\right)
 %\\
 \leq
        &\inf_{0<\lambda\leq \frac{C_3}{ K^2 \|\BW\|}}
        2\exp\left\{-\lambda t + C_2  K^4\|\BW\|_F^2 \lambda^2 \right\}
        \\
        \leq
        &
        2\exp\left\{
            -\min\left(
                \frac{t^2}{4 C_2  K^4 \|\BW\|^2_F}
                ,
                \frac{C_3 t}{2  K^2 \|\BW\|}
            \right)
        \right\}.
    \end{split}
\end{equation*}
This inequality, combined with union bound, yields
\begin{equation*}
    \begin{split}
        \Pr\left(\|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\|>t\right)
        \leq &
        \Pr\left(
        2 \sup_{x\in\mathcal C} \left|x^\top \left(\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ) \right) x\right|
        >t
    \right)
    \\
    \leq&
    2\cdot 9^n
\exp\left\{
            -\min\left(
                \frac{t^2}{16 C_2  K^4 \|\BW\|^2_F}
                ,
                \frac{C_3 t}{4  K^2 \|\BW\|}
            \right)
        \right\}
        .
    \end{split}
\end{equation*}
Thus, there exists a large $C>0$ such that for every $t>0$,
\begin{equation*}
        \Pr\left(\|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\|>
            C(K^2 (\sqrt {n}+t) \|\BW\|_F + K^2 (n+t^2) \|\BW\|)
        \right)
        \leq 2\exp\{-t^2\}.
\end{equation*}
Consequently,  $\|\BZ^\top \BW \BZ - \mydiag(\BZ^\top \BW \BZ)\|=O_P(K^2(\sqrt n \|\BW\|_F+ n \|\BW\|))$.
This completes the proof.




 
\end{proof}

    \section{haha2}
\begin{theorem}
    Let $\zeta_1,\ldots,\zeta_d$ be iid random variables with mean $0$ and variance $1$, and assume $\mu_k:= \myE (\zeta_1^k)$ is finite for $k\leq 8$.
    Let $\bzeta=(\zeta_1,\ldots,\zeta_d)^\top\in \mathbb R^d$.
    For $k=1,\ldots, K$, let $\BQ_k=(q_{i_j}^{(k)})$ be a $d\times d$ symmetric matrix and let $\check\BQ_k=\mydiag(q_{11}^{(k)},\ldots, q_{dd}^{(k)})$, $\hat\BQ_k=\BI_d-\check\BQ_k$.
    Define $\hat{w}_k= \bzeta^\top \hat\BQ_k \bzeta$, $\check w_k = \bzeta^\top \check \BQ_k \bzeta -\mytr(\BQ_k)$, and
\begin{equation*}
W=
    \begin{pmatrix}
   \hat w_1\\
   \check w_1\\
   \vdots\\
   \hat w_K\\
   \check w_K
    \end{pmatrix}
    =
    \begin{pmatrix}
        \bzeta^\top \hat \BQ_1 \bzeta
        \\
        \bzeta^\top \check \BQ_1 \bzeta -\mytr (\BQ_1)
        \\
        \vdots\\
        \bzeta^\top \hat \BQ_1 \bzeta
        \\
        \bzeta^\top \check \BQ_1 \bzeta -\mytr (\BQ_1)
        \\
    \end{pmatrix}
    \in \mathbb R^{2K}.
\end{equation*}
Finally, let $Z\sim \mathcal N_{2K} (0,\BI_{2K})$ and $\BV = \myCov (W)$. 
There is an absolute constant $0<C<\infty$ such that
\begin{equation*}
    haha
\end{equation*}
\end{theorem}
\begin{proof}
    Let $f:\mathbb R^{2K} \to \mathbb R$ be a four-times differentiable function.
    From xxx, there is a $4-times$ differentiable function $g : \mathbb R^{2K}\to \mathbb R$ satisfying the Stein identity
\begin{equation*}
    \myE [f(W)]    - \myE [f(\BV^{1/2} W)]=
    \myE [\nabla^\top  \BV \nabla g(W) - W^\top \nabla g(W)]
\end{equation*}
and 
\begin{equation*}
    \left| \frac{\partial^k g(\Bx)}{\prod_{j=1}^k \partial x_{i_j}} \right| 
    \leq
    \frac{1}{k}
    \left| \frac{\partial^k f(\Bx)}{\prod_{j=1}^k \partial x_{i_j}} \right|
    \quad
    \text{for all $\Bx=(x_1,\ldots, x_{2K})^\top \in \mathbb R^{2K}$,
    $k=1,2,3$, and $i_j \in \{1,\ldots, 2K\}$.}
\end{equation*}
To prove the theorem, we bound
\begin{equation*}
    S=\myE [\nabla^\top \BV \nabla g(W) -W^\top \nabla g(W)].
\end{equation*}

Next, we use exchangeability.
Let $\bzeta' = (\zeta_1',\ldots, \zeta_d')^\top$ be an independent copy of $\zeta$, and let $\underline i \in \{1,\ldots, d\}$ be an independent and uniformly distributed random index.
Define the vector $W' \in \mathbb R^{2K}$ exactly as we defined $W$, except that $\bzeta_{\underline i}$ is replaced with $\bzeta_{\underline i}'$ throughout.
More precisely, let $e_i \in \mathbb R^d$ be the $i$th standard basis vector in $\mathcal R^d$ and define
\begin{equation*}
\begin{split}
    \hat w_k'=& (\bzeta + (\zeta_{\underline i}' - \zeta_{\underline i}) e_{\underline i})^\top 
    \hat \BQ_k
    (\bzeta + (\zeta_{\underline i}' - \zeta_{\underline i}) e_{\underline i})
\\
=& \hat w_k + 2 (\zeta_{\underline i}' - \zeta_{\underline i}) e_{\underline i}^\top \hat \BQ_k \bzeta,
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
    \check w_k'=& (\bzeta + (\zeta_{\underline i}' - \zeta_{\underline i}) e_{\underline i})^\top 
    \check \BQ_k
    (\bzeta + (\zeta_{\underline i}' - \zeta_{\underline i}) e_{\underline i})
    -\mytr(\BQ_k)
\\
=& \check w_k +  e_{\underline i}^\top \check \BQ_k e_{\underline i} ((\zeta_{\underline i}')^2 - \zeta_{\underline i}^2) ,
\end{split}
\end{equation*}
for $k=1,\ldots, K$.
Then $W'= (\hat w_1',\check w_1', \ldots, \hat w_K', \check w_K')^\top \in \mathbb R^{2K}$.
Its straightforward to verify that
\begin{equation*}
    \myE (\hat w_k' - \hat w_k|\bzeta)=-\frac{2}{d} \hat w_k,
    \quad
    \myE (\check w_k' - \check w_k|\bzeta)=-\frac{1}{d} \check w_k.
\end{equation*}
Then
\begin{equation*}
    \myE ( W' - W |\bzeta) = - \Lambda_K W,
\end{equation*}
where
\begin{equation*}
    \Lambda_1=
    \begin{pmatrix}
       \frac 2 d & 0\\
       0 & \frac 1 d
    \end{pmatrix}
    ,
    \quad
    \Lambda_K =
    \begin{pmatrix}
        \Lambda_1 & 0 & \cdots & 0\\
         0 &\Lambda_1  & \cdots & 0\\
         \vdots & \vdots && \vdots\\
         0 & 0 & \cdots & \Lambda_1
    \end{pmatrix}
    \in \mathbb R^{2K\times 2K}.
\end{equation*}

By exchangeability, we have
\begin{equation*}
    \begin{split}
        0=&\frac 1 2 \myE [(W'-W)^\top \Lambda_K^{-\top} (\nabla g(W') +\nabla g(W))]
        \\
        =& \myE [(W'-W)^\top \Lambda_K^{-\top} \nabla g(W)]
        +\frac{1}{2} \myE [(W'-W)^\top \Lambda_K^{-\top} (\nabla g(W') -\nabla g(W))]
        \\
        =&
        - \myE [W^\top  \nabla g(W)]
        +\frac{1}{2} \myE [(W'-W)^\top \Lambda_K^{-\top} (\nabla g(W') -\nabla g(W))].
    \end{split}
\end{equation*}
That is,
\begin{equation*}
         \myE [W^\top  \nabla g(W)]
         =
        \frac{1}{2} \myE [(W'-W)^\top \Lambda_K^{-\top} (\nabla g(W') -\nabla g(W))].
\end{equation*}
Apply Taylor's theorem,
\begin{equation}
    \begin{split}
         &W^\top  \nabla g(W)
         \\
         =&
         \frac 1 2 \sum_{i,j=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ij} g(W) (w_i'-w_i) (w_j'-w_j)
         +
         \frac 1 4 \sum_{i,j,k=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijk} g(W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k)
         \\
         &+
         \frac{1}{12} \sum_{i,j,k,l=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijkl} g(t^*(W'-W)+W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k) (w_l'-w_l)
         \\
         =&
         \frac 1 2 
         \mytr
         [
         (W'-W)
         (W'-W)^\top \Lambda_K^{-\top} \nabla^2 g(W)
     ]
         +
         \frac 1 4 \sum_{i,j,k=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijk} g(W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k)
         \\
         &+
         \frac{1}{12} \sum_{i,j,k,l=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijkl} g(t^*(W'-W)+W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k) (w_l'-w_l),
    \end{split}
\end{equation}
where $t^*\in [0,1]$.
Also by exchangeability,
\begin{equation*}
    \myE[(W'-W) (W'-W)^\top] 
    =
    2\myE[ W(W-W')^\top] 
    =
    2\myE[ W W^\top \Lambda_K^\top] 
    =2\BV \Lambda_K^\top.
\end{equation*}
It follows that
\begin{equation*}
    \myE [\nabla^\top \BV \nabla g(W)] 
    =
    \myE \mytr [\BV \nabla^2 g(W)] 
    =
    \frac 1 2 \myE \mytr [ \myE [(W'-W)(W'-W)^\top] \Lambda_K^{-\top} \nabla^2 g(W)] 
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
    S=&\myE [\nabla^\top \BV \nabla g(W) -W^\top \nabla g(W)]\\
    =
&
    \frac 1 2 \myE \mytr [ \myE [(W'-W)(W'-W)^\top] \Lambda_K^{-\top} \nabla^2 g(W)] 
         -\frac 1 2 
\myE
         \mytr
         [
         (W'-W)
         (W'-W)^\top \Lambda_K^{-\top} \nabla^2 g(W)
     ]
     \\
         &-
         \frac 1 4 \myE
         \sum_{i,j,k=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijk} g(W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k)
         \\
         &-
         \frac{1}{12} \myE
         \sum_{i,j,k,l=1}^{2K}
         \Lambda_{K,ii}^{-1} D^{ijkl} g(t^*(W'-W)+W) (w_i'-w_i) (w_j'-w_j)(w_k'-w_k) (w_l'-w_l).
    \end{split}
\end{equation*}


\end{proof}

\end{appendices}



\bibliographystyle{apalike}
\bibliography{mybibfile}



\end{document}
