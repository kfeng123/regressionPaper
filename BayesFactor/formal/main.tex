\documentclass[11pt]{article}
 
\newcommand\CG[1]{\textcolor{red}{#1}}

\usepackage{lineno,hyperref}

\usepackage[margin=1 in]{geometry}
\renewcommand{\baselinestretch}{1.25}


%\usepackage{refcheck}
\usepackage{authblk}
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[page,title]{appendix}
%\renewcommand\appendixname{haha}
\usepackage{enumerate}
\usepackage{changepage}
\usepackage{datetime}
\newdate{date}{9}{1}{2017}

%%%%%%%%%%%%%%  Notations %%%%%%%%%%
\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myP}{P}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator{\myCov}{Cov}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

 \def\balpha{\bfsym \alpha}
 \def\bbeta{\bfsym \beta}
 \def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
 \def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
 \def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
 \def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
 \def\bnu{\bfsym {\nu}}
 \def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
 \def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \varepsilon}
 \def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
 \def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
 \def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
 \def\brho   {\bfsym {\rho}}
 \def\btau{\bfsym {\tau}}
 \def\bxi{\bfsym {\xi}}
 \def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{definition}{\quad\quad Definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}





\begin{document}
\title{
A Bayesian-motivated test for linear model in high-dimensional setting
}



\author[1]{Rui Wang}
\author[1,2]{Xingzhong Xu\thanks{Corresponding author\\Email address: xuxz@bit.edu.cn}}
\affil[1]{
School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    100081,China
}
\affil[2]{
Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China
}

\maketitle
\begin{abstract}
    Using the idea of Bayesian factor, a new test for linear model in high-dimensional setting is proposed.

    Our theory is also useful in.
\end{abstract}
\section{Introduction} 
Consider the high-dimensional linear regression model of the form
\begin{equation*}
    \By = 
    \BX_a \bbeta_a + \BX_b \bbeta_b + \bepsilon, %\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n),
\end{equation*}
where $\By \in \mathbb R^n$ is the response, 
$\BX_a$, $\BX_b$ are $n\times q$ and $n\times p$ design matrices, respectively,  $\bbeta_a\in \mathbb R^q$, $\bbeta_b\in \mathbb R^p$ are unknown regression coefficients, and $\bepsilon=(\epsilon_1,\ldots,\epsilon_n)^\top$ are the iid errors with mean $0$ and covariance $\sigma^2=\phi^{-1}$.
Here we break the predictors into two parts $\BX_a$ and $\BX_b$ such that $\BX_a$ contains the predictors that are known to have effect on the response,
and we would like to know if $\BX_b$ contains useful predictors.
That is, we are interested in testing the hypotheses
\begin{align}\label{theHypothesis}
    \mathcal H_0:   \bbeta_b =0,\quad
    \text{v.s.} \quad
    \mathcal H_1:   \bbeta_b \neq 0.
\end{align}
Motivated by many recent applications of high dimensional regression, we consider the situation where $p+q$ is much larger than $n$.


\subsection{Overview of existing tests}
The conventional test for hypotheses \eqref{theHypothesis} is the $F$-test which is also the likelihood ratio test under normality.
However, the $F$-test is not well defined in high dimensional setting.
In fact, if $\bepsilon$ is normal distributed and $\myrank[\BX_a;\BX_b]=n$, then the likelihood is unbounded under the alternative hypothesis.
This calls for new test methodologies in high-dimensional setting.

Two different high-dimensional settings have been extensively considered in the literature.
One is the small $p$, large $q$ setting.
An important example of this setting is testing individual coefficients of a high-dimensional regression.
See \cite{buhlmann2013statistical}, \cite{Zhang2013} and \cite{Lan2016} for testing procedures in this setting.
In this paper, however, we focus on the other setting, namely the large $p$, small $q$ setting.
In this case, there are just a few covariates, namely $\BX_a$, are known to have effect on the response, while there remain a large number of covariates, namely $\BX_b$, to be tested.
%We assume $\BX_a$ has full column rank and $\BX_b$ has full row rank.
In practice, which covariates belong to the part $\BX_a$ is determined apriori.
If no prior knowledge is available, $\BX_a$ can be $\mathbf 1_n$.
%In both cases, $q$ is often relatively small.
%However, there may remain a very large number of variables in $\BX_b$, so $p>n$.

Many test procedures have been proposed in the large $p$, small $q$ setting.
Based on an empirical Bayes model, \cite{Goeman2006} and \cite{Goeman2011} proposed a score test as well as a method to determine the critical value of their test statistic.
Later, \cite{Lan2014Testing} proposed a similar test, but using normal distribution to determine the critical value.
There are also many other lines of search.
\cite{Zhong2011Tests} proposed a test based on $U$-statistics for the case $\BX_a=\mathbf 1_n$.
Later, \cite{Wang2015} generalized their test for general design matrix $\BX_a$.
Scalar invariant tests: \cite{Feng2013} and \cite{Xu2016a}.
Debiased tests: .



Also, many Bayesian hypothesis testing procedures have been proposed in the literature.
See \cite{javier2006Obj,Goddard2016,zhou2018On} and the references therein.

\subsection{Main contributions}
Bayesian methods may be powerful tools in high dimensional setting.

\subsection{Organization of the paper}

The proposed test is the limit of Bayes factors.



\section{Methodology}
As \cite{Goeman2006} pointed out, if $\bbeta_b \neq 0$ but $\BX_b \bbeta_b =0$, no test has any power.
\cite{Goeman2006} used Bayesian method.
Their idea is to choose an  `unbiased' distribution of $\bbeta_b$.
As they noticed, their test has negligible power for many alternatives, and is not unbiased.
In theory, we prove that there is no nontrivial unbiased test.


The following proposition implies that there is no nontrivial unbiased test.
\begin{proposition}\label{prop:unbiased}
Suppose
$\By\sim \mathcal N_n (\mu, \phi^{-1}\BI_n)$.
We test $H_0: \mu=\BX_a \bbeta_a, \bbeta_a \in \mathbb R^q$ versus
$H_1: \mu\in \mathbb R^n$, where $\BX_a$ is an $n\times q$ matrix with full column rank, $q<n$.
Let $\varphi(\By)$ be a test function, that is, a Borel measurable function, $0\leq \phi(\By)\leq 1$.
If $\int\varphi(\By) \mathcal N_n (\BX_a\bbeta_a, \phi^{-1} \BI_n) (d\By)=\alpha$ for $\bbeta_a \in \mathbb R^q$, $\phi>0$ and $\int\varphi(\By) \mathcal N_n (\mu, \phi^{-1} \BI_n) (d\By) \geq \alpha$ for $\mu\in \mathbb R^n$, $\phi>0$, then $\varphi(\By)=\alpha$, a.s.
\end{proposition}

So we can not find a universally good test.
Instead, we would like to find a test with good average behaviour.
So Bayesian methods are natural choices in this case.




Bayes hypothesis testing use the Bayes factor.
\begin{equation*}
    B_{10}= \frac {
        \int f_1(\By|\bbeta_b ,\bbeta_a, \phi) \pi_1(\bbeta_b,\bbeta_a,\phi) d\bbeta_b d\bbeta_a d\phi
}{
        \int f_0(\By|\bbeta_a, \phi) \pi_0(\bbeta_a,\phi) d\bbeta_a d\phi
    }.
\end{equation*}

There have been several extensions of $g$-priors to $p>n$ case: \cite{maruyama2011}, \cite{Shang2011}.

Under $H_0$, we impose the reference prior $\pi_0 (\bbeta_a,\phi)=c/\phi$.
Note that under $H_1$, the posterior corresponding to the referece prior is proper if and only if $\myrank (\BX_a, \BX_b)= q+p$ and $n>q + p$.
That is, the minimal training sample size is $q + p +1$.
So we cannot impose the reference prior under $ H_1$ provided $q + p  \geq n$.
We temporarily impose the conditional prior $\bbeta_b|\bbeta_a, \phi \sim \mathcal N_{p} (0, \kappa^{-1} \phi^{-1} \BI_{p}) $.
There are extansive literature consider the choice of $\kappa$.
\cite{Kass1995} choose $\kappa$ such that the amount of information about the parameter equal to the amount of information contained in one observation.
Thus, under $H_1$, we put the prior
\begin{equation*}
    \pi_1 (\bbeta_b | \bbeta_a, \phi) =
    \mathcal N_p\left(0,\frac{1}{\kappa \phi}\BI_p\right)(\bbeta_b)
    %\frac{(\kappa \phi )^{p/2}}{(2\pi )^{p/2}}  
    %\exp\left\{
        %-\frac{\kappa \phi }{2} \|\bbeta_b\|^2
    %\right\}
    ,\quad
    \pi_1(\bbeta_a, \phi) = \frac{c}{\phi}.
\end{equation*}

\begin{equation*}
    \begin{split}
    m_0(\By;\kappa, \tau) 
    &:=
    \int f_0^\tau (\By|\bbeta_a,\phi) \pi_0 (\bbeta_a, \phi) d\bbeta_a d\phi
    \\
    &=
    \frac{
        c_0 \Gamma\left( \frac{\tau n - q}{2}\right)
    }{
        \pi^{\frac{\tau n - q }{2}}
        \tau^{\frac{\tau n}{2}}
        |\BX_a^\top \BX_a|^{\frac{1}{2}}
        \| (\BI_n -\BP_a) \By\|^{\tau n -q}
    }.
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        m_1(\By;\kappa, \tau) 
    &:=
    \int f_1^\tau (\By|\bbeta_b,\bbeta_a,\phi) \pi_1 (\bbeta_b |\bbeta_a, \phi) \pi_1 (\bbeta_a, \phi)  d\bbeta_a d\bbeta_b d\phi
    \\
    &=
    \frac{c_1\kappa^{\frac p 2} \Gamma \left(\frac{\tau n -q}{2}\right)}{
        \pi^{\frac{\tau n -q}{ 2 }} \tau^{\frac{\tau n + p}{2}}
        |\BX_a^\top \BX_a|^{\frac 1 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
    }
    \frac{1}{\left[ \By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^* \right]^{\frac{\tau n - q}{2}}}
    .
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        \frac{m_1(\By;\kappa,\tau)}{m_0(\By;\kappa,\tau)} 
        =
        \frac{c_1 \kappa^{\frac{p}{2}}}{c_0 \tau^{\frac p 2}
        |\BX_b^{*\top} \BX_b^* + \frac{\kappa}{\tau } \BI_p|^{\frac 1 2}
        }
        \left(
            \frac{\By^{*\top} \By^*}{
\By^{*\top} \By^* - \By^{*\top} \BX_b^* ( \BX_b^{*\top}\BX_b^* + \frac{\kappa}{\tau} \BI_p )^{-1} \BX_b^{*\top} \By^*
            }
        \right)^{\frac{\tau n - q }{2}}
    \end{split}
\end{equation*}


It is straightforward to show that the Bayes factor associated with these priors is
\begin{equation*}
    \begin{split}
        B_{10}^{\kappa}=  &
    \frac{\kappa^{p/2}}{
        |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |^{1/2}
    }
    \cdot
    \\
    &
    \left(
        \frac{
            \By\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
            -
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }
    \right)^{(n-q)/2}.
    \end{split}
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
        2\log B_{10}^{\kappa} =  &
    p\log \kappa
    -
        \log |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |
    \\
    &
    -(n-q)\log \left(
            1-
        \frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
    \right).
    \end{split}
\end{equation*}

Denote by $\BI_n-\BP_a=\tilde{\BU}_a \tilde{\BU}_a^\top$ the rank decomposition of $\BI_n - \BP_a$, where $\tilde{\BU}_a$ is a $n\times (n-q)$ column orthogonal matrix.
Let $\BX_b^* = \tilde{\BU}_a^\top \BX_b$, $\By^* =\tilde \BU_a^\top \By$.
Let $\gamma_i$ be the $i$th largest eigenvalue of $\BX^*_b \BX_b^{*\top}$, $i=1,\ldots, n-q$.
Denote by $\BX_b^* =\BU_{b}^* \BD_{b}^* \BV_b^{*\top}$ the singular value decomposition of $\BX_{b}^*$, where  $\BU_{b}^*$, $\BV_b^*$ are $(n-q)\times (n-q)$ and $p\times (n-q)$ column orthogonal matrices, respectively, and $\BD_{b}^*=\mydiag (\sqrt {\gamma_1},\ldots, \sqrt{\gamma_{n-q}})$.
Then

\begin{equation*}
    \begin{split}
        2\log B_{10}^\kappa
        =&
         p\log \kappa
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(p-(n-q))\log \kappa
         \\
         &-(n-q)\log\left(1-\frac{\By^{*\top} \BX_b^* \left( \BX_b^{*\top} \BX_b^* + \kappa \BI_p \right)^{-1} \BX_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         \\
        =&
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} \BU_b^*  \left[\frac 1 \kappa \left(\BI_{n-q}-\BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*} \right) \right] \BU_b^{*\top} \By^* }\right)
         \\
        =&
        (n-q)\log \kappa - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(n-q)\log\left(1-\frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         .
    \end{split}
\end{equation*}
The main part of $2\log B_{10}^\kappa$ is 
\begin{equation*}
    T_n^\kappa = \frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}.
\end{equation*}
A large value of $T_n^\kappa$ supports the alternative hypothesis.
Under the null hypothesis, 
\begin{equation*}
    \myE T_{n}^\kappa=
    \frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right).
\end{equation*}
Under the alternative hypothesis, consider $\bbeta_b=c \bbeta_b^\dagger$ where $\bbeta_b^\dagger\neq 0$ is a fixed direction and $c>0$.
As $c \to \infty$, 
\begin{equation*}
T_n^\kappa \to
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
.
\end{equation*}
We say $T_{n}^\kappa$ is consistent along the direction $\bbeta_b^\dagger$ if
\begin{equation*}
\frac{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}{
    \bbeta_b^{\dagger \top} \BV_b^* \BD_b^{*2} \BV_b^{*\top} \bbeta_b^{\dagger}
}
>
\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right),
\end{equation*}
or equivalently
\begin{equation*}
    \bbeta_b^{\dagger \top} \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
    \BV_b^{*\top} \bbeta_b^{\dagger}
    >0.
\end{equation*}
Let $k_{\kappa}$ be the number of positive eigenvalues of
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}.
\end{equation*}
Let $\mathcal S_{\kappa}$ be the linear space spanned by the first $k_{\kappa}$ columns of $\BV_{b}^*$.
Denote by $\mathcal S_{\kappa}^{\bot}$ the orthogonal complement space of $\mathcal S_\kappa$.
We have $\mathbb R^p=\mathcal S_\kappa \oplus \mathcal  S_{\kappa}^{\bot}$.
If $\bbeta_b^\dagger \in \mathcal S_\kappa$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}>0.
\end{equation*}
On the other hand, if $\bbeta_b^\dagger \in \mathcal S_{\kappa}^\bot$,
\begin{equation*}
    \BV_b^*
\left[
    \BD_b^{*2} \left( \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*2} 
-\frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right)
     \BD_b^{*2} 
\right]
\BV_b^{*\top}\leq 0.
\end{equation*}
We would like to choose a hyperparameter $\kappa$ which consists the most consistent directions.
To achieve this, we maximize $k_{\kappa}$ with respect to $\kappa$.

\begin{proposition}\label{prop:monotone}
    For $ \kappa _2 > \kappa_1 > 0$, we have
    $k_{\kappa_1} \geq k_{\kappa_2}$.
    That is, $k_{\kappa}$ ($\kappa>0$) is decreasing in $\kappa$.
\end{proposition}
The proposition implies that we should put $\kappa$ as small as possible.
This motivates us to consider $B_{10}^0=\lim_{\kappa\to 0} B_{10}^\kappa$.
It is straightforward to show that
\begin{equation*}
    2\log B_{10}^0= 
         - \sum_{i=1}^{n-q}\log ( \gamma_i )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }\right)
         .
\end{equation*}
$B_{10}^0$ can be regarded as the Bayes factor with respect to noninformative prior.

Define
\begin{equation*}
    T_n= \frac{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }{\By^{*\top} \By^*} .
\end{equation*}
Then we reject the null hypothesis if $T_n$ is small.
It can be seen that under the null hypohtesis,
\begin{equation*}
    T_n \sim
    \frac{\sum_{i=1}^{n-q} \gamma_i^{-1} Z_i^2}{\sum_{i=1}^{n-q} Z_i^2},
    \end{equation*}
    where $\gamma_i$ is the $i$th eigenvalue of $  \BX_b^* \BX_b^{*\top}$, $i=1,\ldots, n-q$, and $Z_1,\ldots, Z_{n-q}$ are iid $\mathcal N(0,1)$ random variables.




\section{Asymptotic results}
Let $\bepsilon=(\epsilon_1,\ldots,\epsilon_n)^\top$, where $\epsilon_i$'s are iid random variable.
Denote $\mu_k=\myE \epsilon_1^k$.
Then $\mu_1=0$, $\mu_2=\phi^{-1}$.

\begin{assumption}
    Suppose 
\end{assumption}




\begin{lemma}\label{lemma:denom}
If $\phi^2\mu_4=o(n-q)$,
\begin{equation*}
    \By^{*\top} \By^*=(1+o_P(1))
    \left(
    \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q)
\right).
\end{equation*}
\end{lemma}
\begin{proof}
\begin{equation*}
    \By^{*\top} \By^* = \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    +2 \bepsilon^\top (\BI_n -\BP_a) \BX_b \bbeta_b + \bepsilon^\top (\BI_n - \BP_a) \bepsilon.
\end{equation*}


\begin{equation*}
    \myE \left(\By^{*\top} \By^*\right) = \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q).
\end{equation*}


\begin{equation*}
    \begin{split}
    \myVar \left( \By^{*\top} \By^* \right) 
    \leq  &
    2\myVar \left( 2 \bepsilon^\top (\BI_n -\BP_a) \BX_b \bbeta_b \right) + 2\myVar \left( \bepsilon^\top (\BI_n - \BP_a) \bepsilon \right)
    \\
    %=&8 \bbeta_b^\top \BX_b^\top (\BI_n -\BP_a) \BX_b \bbeta_b 
    \end{split}
\end{equation*}

From (i) of \cite[Proposition A.1]{chen2010tests},
\begin{equation*}
    \begin{split}
\myVar\left( \bepsilon^\top (\BI_n - \BP_a) \bepsilon \right)
&=
\phi^{-2}\left(
    (\phi^2 \mu_4 - 3) \sum_{i=1}^n \left((\BI_n -\BP_a)_{i,i}\right)^2
    + 2(n-q)
\right)
    \leq
    \phi^{-2}(2+\phi^2 \mu_4)(n-q)
    .
    \end{split}
\end{equation*}
Then
\begin{equation*}
    \begin{split}
    \myVar \left( \By^{*\top} \By^* \right) 
    \leq  &
    8 \phi^{-1} \bbeta_b^\top \BX_b^\top (\BI_n -\BP_a) \BX_b \bbeta_b 
    +
    2 \phi^{-2} (2+\phi^2 \mu_4) (n-q)
    \end{split}
\end{equation*}
Thus, if $\phi^2\mu_4=o(n-q)$, we have
\begin{equation*}
    \frac{
        \myVar \left( \By^{*\top} \By^* \right) 
    }{
        \left(\myE \left( \By^{*\top} \By^* \right) \right)^2
    }
    \to 0,
\end{equation*}
and consequently $\By^{*\top} \By^*=(1+o_P(1))\myE (\By^{*\top} \By^*)$.

\end{proof}


Note that under the normality, $T_n- \mytr ((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)$ has zero mean.




\begin{theorem}\label{generalTheorem}
    Suppose the rows of $\BX_b$ are iid random vectors with distribution $\mathcal N(0,\sigma_b^2\BI_p)$.
    Suppose $p/(n-q)\to c \in (1,+\infty)$.
    Then
\begin{equation*}
    \left(
    \bbeta_b^\top \BX_b^\top (\BI_n-\BP_a) \BX_b \bbeta_b
    + \phi^{-1}(n-q)
\right)
    \left(
        \frac{\By^{*\top} (\BX_b^* \BX_b^{*\top})^{-1} \By^*}{\By^{*\top} \By^*} -\frac{\mytr ((\BX_b^* \BX_b^{*\top})^{-1})}{n-q}
    \right)
    \rightsquigarrow \mathcal N(0,1).
\end{equation*}
\end{theorem}
\begin{proof}
    Note that $\BX_b^* \BX_b^{*\top} \sim \text{Wishart}(p,\sigma_b^2 \BI_{n-q})$.
    \begin{equation*}
        \frac{\By^{*\top} (\BX_b^* \BX_b^{*\top})^{-1} \By^*}{\By^{*\top} \By^*} -\frac{\mytr ((\BX_b^* \BX_b^{*\top})^{-1})}{n-q}
        =
        \frac{\phi\By^{*\top}\left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
        \right) \By^*}{\phi\By^{*\top} \By^*}.
    \end{equation*}

We have
\begin{equation*}
    \begin{split}
&
\phi\By^{*\top}\left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
        \right) \By^*
        \\
        =&
\phi
\bepsilon^{\top} \tilde \BU_a \left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
        \right) \tilde \BU_a^\top \bepsilon
        \\
        &+
2\phi\bepsilon^{\top} \tilde \BU_a \left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)  \BX_b^* \bbeta_b
        \\
        &+
\phi
 \bbeta_b^\top
 \BX_b^{*\top}
\left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)  \BX_b^* \bbeta_b
            \\
            =:& A_1 +A_2 +A_3.
    \end{split}
\end{equation*}

We have $\myE (A_1|\BX_b)=\myE (A_2|\BX_b)=0$.
It is also straightforward to see that 
\begin{equation*}
    \begin{split}
        \myVar (A_1|\BX_b)
        &= 2\mytr  \left( (\BX_b^* \BX_b^{*\top})^{-2} \right)
                -2\frac{1}{n-q} \mytr^2 \left( (\BX_b^* \BX_b^{*\top})^{-1}\right)
                ,
                \\
                \myVar (A_2|\BX_b)&= 
                4 \phi \bbeta_b^\top \BX_b^{*\top}
                \left( (\BX_b^* \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \bbeta_b
            ,\\
            \text{Cov} (A_1,A_2|\BX_b)&=0.
    \end{split}
\end{equation*}

\CG{By some theory, we have}
    From \cite[Theorem 5.1]{jiang1996reml},
\begin{equation*}
    \frac{A_1+A_2}{\sqrt{\myVar(A_1|\BX_b) + \myVar (A_2 |\BX_b)}} \rightsquigarrow \mathcal N(0,1).
\end{equation*}


From lemma \ref{lemma:MP},
\begin{equation*}
    \begin{split}
    \myVar (A_1 |\BX_b)
    &=
(1+o_P(1))
2 \sigma_b^{-4} p^{-2} (n-q) \left( \nu_{-2,c}-\nu_{-1,c}^2 \right)
    .
    \end{split}
\end{equation*}
Now we deal with $\myVar (A_2 |\BX_b)$.
Let $\BO$ be a $p\times p$ random matrix with Haar distribution which is independent of $\BX_b$.
The rotation invariance of normal distribution implies that $\BX_b\BO$ has the same distribution as $\BX_b$ and is independent of $\BO$.
Then
\begin{equation*}
    \begin{split}
    \myVar (A_2 |\BX_b)
    &=
                4\phi \bbeta_b^\top \BO \BO^\top  \BX_b^{*\top}
                \left( (\BX_b^* \BO \BO^\top \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^* \BO \BO^\top  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO \BO^\top  \bbeta_b
            \\
            &\overset{d}{=}
                4\phi \bbeta_b^\top \BO  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO^\top  \bbeta_b.
    \end{split}
\end{equation*}
Note that $\BO^\top \bbeta_b/\|\BO^\top \bbeta_b\|$ is uniformly distributed on the unit sphere $S^{p-1}$.
From Lemma \ref{lemma:uniform},
\begin{equation*}
    \begin{split}
        &
                \myE\left(
                    4\phi \bbeta_b^\top \BO  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO^\top  \bbeta_b
            \bigg| \BX_b
        \right)
        \\
        =&
        4\phi p^{-1}\|\bbeta_b\|^2
        \mytr \left(  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* 
\right)
\\
=&
        4\phi p^{-1}\|\bbeta_b\|^2
        \left(  
            \frac{1}{(n-q)^2}
            \mytr^2 \left[ (\BX_b^* \BX_b^{*\top})^{-1} \right]
            \mytr(\BX_b^* \BX_b^{*\top})
            -\mytr\left[(\BX_b^*  \BX_b^{*\top})^{-1}\right]
        \right).
    \end{split}
\end{equation*}
Then Lemma \ref{lemma:MP} implies that
\begin{equation*}
    \begin{split}
        &
                \myE\left(
                    4\phi \bbeta_b^\top \BO  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO^\top  \bbeta_b
            \bigg| \BX_b
        \right)
        \\
        =&
        (1+o_P(1))4\phi \|\bbeta_b\|^2
        \sigma_b^{-2} p^{-2} (n-q)
        \left(  
            \nu_{1,c}
            \nu_{-1,c}^2
            -\nu_{-1,c}
        \right).
    \end{split}
\end{equation*}
Similarly,
\begin{equation*}
    \begin{split}
        &
                \myVar\left(
                    4\phi \bbeta_b^\top \BO  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO^\top  \bbeta_b
            \bigg| \BX_b
        \right)
        \\
        \leq&
        \frac{32}{p^2} \phi^2 \|\bbeta_b\|^4
        \Bigg(
            \mytr((\BX_b^* \BX_b^{*\top})^{-2}) 
            +\frac{1}{(n-q)^2}
            \mytr^4((\BX_b^* \BX_b^{*\top})^{-1}) 
            \mytr((\BX_b^* \BX_b^{*\top})^{2}) 
            \\
            &
            +2\mytr^2((\BX_b^* \BX_b^{*\top})^{-1})
            -\frac{4}{(n-q)^2} \mytr^3 ((\BX_b^* \BX_b^{*\top})^{-1})
\mytr (\BX_b^* \BX_b^{*\top})
        \Bigg)
        \\
        =&
        (1+o_P(1))
        32 \phi^2 \|\bbeta_b\|^4
        \sigma_b^{-4} p^{-4} (n-q)
        \left(
            \myE (\xi^{-2})
            +
            2\left(\myE (\xi^{-1})\right)^2
            -4
            \left(\myE (\xi^{-1})\right)^3 \myE (\xi)
            +
            \left(\myE (\xi^{-1})\right)^4 \myE (\xi^2)
        \right)
        \\
        =&o_P(1)
\left(
                \myE\left(
                    4\bbeta_b^\top \BO  \BX_b^{*\top}
                \left( (\BX_b^*  \BX_b^{*\top})^{-1} 
                -\frac{\mytr ( (\BX_b^*  \BX_b^{*\top})^{-1})}{n-q} 
                \BI_{n-q}
            \right)^2  \BX_b^* \BO^\top  \bbeta_b
            \bigg| \BX_b
        \right)
    \right)^2
    \end{split}
\end{equation*}
Thus, we conclude that
\begin{equation*}
    \myVar (A_2 | \BX_b)= 
        (1+o_P(1))4
        \phi
        \|\bbeta_b\|^2
        \sigma_b^{-2} p^{-2} (n-q)
        \left(  
            \nu_{1,c}
            \nu_{-1,c}^2
            -\nu_{-1,c}
        \right).
\end{equation*}
Using the same technique, one can show that
\begin{equation*}
    A_3=(1+o_P(1)) \phi \|\bbeta_b\|^2 p^{-1} (n-q) \left(1- \nu_{1,c}\nu_{-1,c}\right).
\end{equation*}

    From lemma \ref{lemma:denom}, we have
    \begin{equation*}
        \phi\By^{*\top} \By^*
        =
        (1+o_P(1))\left(
            \phi \bbeta_b^\top \BX_b^{*\top}  \BX_b^* \bbeta_b
    + 
    (n-q)
\right).
    \end{equation*}
    Using the same technique, we have
    \begin{equation*}
        \phi\By^{*\top} \By^*
        =
        (1+o_P(1))(n-q)\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    .
    \end{equation*}

We have 
\begin{equation*}
    \begin{split}
    &\sqrt{\myVar(A_1|\BX_b) + \myVar (A_2 |\BX_b)}
    \\
    =&(1+o_P(1))
    \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi\|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
.
    \end{split}
\end{equation*}
Since $[-\infty,0]$ is compact, for every subsequence of $\{n\}$, there is a further subsequence along which 
\begin{equation*}
    \frac{
\phi \|\bbeta_b\|^2 p^{-1} (n-q) \left(1- \nu_{1,c}\nu_{-1,c}\right)
    }{
    \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
    }
    \to s \in [-\infty, 0].
\end{equation*}
Then along this further subsequence, we have
\begin{equation*}
    \frac{A_1+A_2+A_3}{
        \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
}
\rightsquigarrow \mathcal N(s,1).
\end{equation*}
If $s=-\infty$, the above expression means the left hand side tends to $-\infty$ in probability.
Thus
\begin{equation*}
    \begin{split}
        &
\left(
    T_n - \mytr((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)
\right)
    \frac{
     (n-q)\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    }{
        \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
    } 
\\
&
\rightsquigarrow \mathcal N(s,1).
    \end{split}
\end{equation*}
Under the null hypothesis,
\begin{equation*}
    \begin{split}
        &
\left(
    T_n - \mytr((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)
\right)
    \frac{
     n-q
    }{
        \sqrt{
    2\sigma_b^{-4} p^{-2} (n-q)
         (\nu_{-2,c} - \nu_{-1,c}^2)
}
    } 
\rightsquigarrow \mathcal N(0,1).
    \end{split}
\end{equation*}
Thus the critical value of
$
    T_n - \mytr((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)
$ is
\begin{equation*}
    \Phi(\alpha) 
    \frac{
        \sqrt{
    2\sigma_b^{-4} p^{-2} (n-q)
         (\nu_{-2,c} - \nu_{-1,c}^2)
}
    }{
     n-q
    } 
    .
\end{equation*}
Then the power function is
\begin{equation*}
    \begin{split}
    &\Pr
    \left(
        T_n - \mytr((\BX_b^* \BX_b^{*\top})^{-1})/(n-q)
        \leq
    \Phi(\alpha) 
    \frac{
        \sqrt{
    2\sigma_b^{-4} p^{-2} (n-q)
         (\nu_{-2,c} - \nu_{-1,c}^2)
}
    }{
     n-q
    } 
\right)
\\
=&
\Pr\left( \mathcal N (s,1)
    \leq
    \Phi(\alpha) 
    \frac{
(n-q)\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
        \sqrt{
    2\sigma_b^{-4} p^{-2} (n-q)
         (\nu_{-2,c} - \nu_{-1,c}^2)
}
    }{
        (n-q)
        \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
    } 
\right)
+o(1)
\\
=&
\Pr\left( \mathcal N (s,1)
    \leq
    \Phi(\alpha) 
    \frac{
\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    }{
        \sqrt{
            1
        +
        \frac{
            2\phi \sigma_b^2 \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
        }{\nu_{-2,c} - \nu_{-1,c}^2}
}
    } 
\right)
+o(1)
\\
=&
\Phi\left( 
    -s
    +
    \Phi(\alpha) 
    \frac{
\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    }{
        \sqrt{
            1
        +
        \frac{
            2\phi \sigma_b^2 \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
        }{\nu_{-2,c} - \nu_{-1,c}^2}
}
    } 
\right)
+o(1)
\\
=&
\Phi\left( 
    \frac{
\phi \|\bbeta_b\|^2 p^{-1} (n-q) \left(\nu_{1,c}\nu_{-1,c}-1\right)
    }{
    \sqrt{
    2\sigma_b^{-2} p^{-2} (n-q)
    \left( 
        \sigma_b^{-2} (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
    }
    +
    \Phi(\alpha) 
    \frac{
\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    }{
        \sqrt{
            1
        +
        \frac{
            2\phi \sigma_b^2 \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
        }{\nu_{-2,c} - \nu_{-1,c}^2}
}
    } 
\right)
+o(1)
\\
=&
\Phi\left( 
    \frac{
       \sqrt{n-q}  \phi \sigma_b^2 \|\bbeta_b\|^2  \left(\nu_{1,c}\nu_{-1,c}-1\right)
    }{
    \sqrt{
    2  
    \left( 
         (\nu_{-2,c} - \nu_{-1,c}^2)
        +
        2\phi \sigma_b^2 \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
    \right)
}
    }
    +
    \Phi(\alpha) 
    \frac{
\left(
            1+ \phi \sigma_b^2 \|\bbeta_b\|^2  \nu_{1,c}
\right)
    }{
        \sqrt{
            1
        +
        \frac{
            2\phi \sigma_b^2 \|\bbeta_b\|^2 (\nu_{1,c} \nu_{-1,c}^2 -\nu_{-1,c})
        }{\nu_{-2,c} - \nu_{-1,c}^2}
}
    } 
\right)
+o(1)
    \end{split}
\end{equation*}








\end{proof}

\begin{lemma}\label{lemma:uniform}
    Let $\BA$ be an $p\times p$ symmetric matrix.
    Let $Z$ be a $p$ dimensional random vector with uniform distribution on the unit sphere $ S^{p-1}$.
    Then 
    \begin{equation*}
        \begin{split}
            &\myE (Z^\top \BA Z) = \frac{1}{p} \mytr (\BA),
        \quad
        \myVar (Z^\top \BA Z) =
        \frac{2}{p(p+2)}\left( \mytr (\BA^2) - \frac{1}{p} \mytr^2 (\BA) \right)
        \leq
        \frac{2}{p^2} \mytr (\BA^2)  
        .
        \end{split}
    \end{equation*}
\end{lemma}
\begin{proof}
    The result follows from direct calculation and the fact that for nonnegative integers $k_1,\ldots, k_{p}$,
\begin{equation*}
    \myE \prod_{i=1}^p z_i^{2k_i} 
    =
    \frac{\Gamma(p/2) \prod_{i=1}^p \Gamma(k_i+1/2)}{ \pi^{p/2} \Gamma(\sum_{i=1}^p k_i + p/2)}
    ,
\end{equation*}
where $z_i$ is the $i$th coordinate of $Z$.
\end{proof}

The following lemma is a direct consequence of MP law and Bai Yin law.
\begin{lemma}\label{lemma:MP}
    Under the assumptions of Theorem \ref{generalTheorem}, for every $r\in \mathbb R$,
    \begin{equation*}
        \frac{1}{\sigma_b^{2r} p^r (n-q)}\mytr ((\BX_b^* \BX_b^{*\top})^{r} )
        \xrightarrow{a.s.}
        \nu_{r,c}
    \end{equation*}
    where $
        \nu_{r,c}
        =
        \myE \xi^r,
        $, and $\xi$ is a random variable with density function
    \begin{equation*}
        p_{c}(x)=\mathbf{1}_{\left[(1-c^{-1/2})^2,(1+c^{-1/2})^2\right]}(x)
        \frac{c}{2\pi x} \sqrt{4/c - \left(x-(1/c+1)\right)^2}.
    \end{equation*}
\end{lemma}

















%\section*{Acknowledgements}
%This work was supported by the National Natural Science Foundation of China under Grant Nos.\ xxxxx, xxxx.




\begin{appendices}

    \section{haha1}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:unbiased}}]
    We assume $0<\alpha<1$ since the case $\alpha=0$ or $1$ is trivial.
    Note that the condition implies $\int [\varphi(\By)-\alpha] \mathcal N_n (0,\phi^{-1} \BI_n) (d\By)=0$.
    Hence it suffices to prove $\varphi(\By) \geq \alpha$, a.s. 
    We prove this by contradiction.
    Suppose $\lambda(\{\By:\varphi (\By) <\alpha\})>0$.
    Then there exists a $\eta >0$, such that $\lambda(\{\By:\varphi (\By) <\alpha-\eta\})>0$.
    We denote $E=\{\By:\varphi (\By) <\alpha-\eta\}$.
    From Lebesgue density theorem \citep[Corollary 6.2.6]{book:992991}, there exists a point $z\in E$, such that, for each $\epsilon >0$ there is a $\delta_{\epsilon}>0$ such that
    \begin{equation*}
        \left|\frac{\lambda(E^\complement\cap C_{\epsilon})}{\lambda(C_{\epsilon})}\right|<\epsilon,
    \end{equation*}
    where $C_{\epsilon}=\prod_{i=1}^n [z_i-\delta_{\epsilon}, z_i + \delta_{\epsilon}]$.
    We put
    \begin{equation*}
        \epsilon=\left(\frac{\sqrt \pi}{\sqrt 2 \Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^n \frac{\eta}{3}.
    \end{equation*}
    Then for any $\phi>0$,
    \begin{equation*}
        \begin{split}
            \alpha \leq& 
            \int_{\mathbb R^n}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            =&
            \int_{E\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{E^\complement\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \int_{E^\complement\cap C_{\epsilon}} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}\lambda(E^\complement\cap C_{\epsilon})
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}
            \epsilon
            (2\delta_\epsilon)^n
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            =&
            \alpha-\eta
            +
            \left(\frac{\sqrt{\phi} \delta_{\epsilon}}{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^{n}
            \frac{\eta}{3}
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right).
        \end{split}
    \end{equation*}
    Putting 
    \begin{equation*}
        \phi = \left(\frac{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}{\delta_\epsilon}\right)^2
    \end{equation*}
    yields the contradiction $\alpha\leq \alpha-(2/3)\eta$.
    This completes the proof.

\end{proof}


\begin{proof}[\textbf{Proof of Proposition \ref{prop:monotone}}]
    For positive integer $m$, define $[m]=\{1,,\ldots, m\}$.
    For a set $A$, denote by $| A |$ its cardinality.
    We have
    \begin{equation*}
        \begin{split}
        k_{\kappa} =& \left|\left\{i\in [n-q]: \frac{\gamma_i^2}{\gamma_i +\kappa} - \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j \gamma_i}{\gamma_j +\kappa}>0 \right\}\right|
        \\
        =& \left|\left\{i\in [n-q]: \frac{\gamma_i}{\gamma_i +\kappa} > \frac{1}{n-q} \sum_{j=1}^{n-q}\frac{\gamma_j }{\gamma_j +\kappa} \right\}\right|
        .
        \end{split}
    \end{equation*}
    Let $X$ be a random variable uniformly distributed on $\{\gamma_1,\ldots,\gamma_{n-q}\}$.
    That is, $\Pr(X=\gamma_i)=1/(n-q)$, $i=1,\ldots, n-q$.
    Then it can be seen that
    \begin{equation*}
        k_{\kappa}=(n-q) \Pr \left(\frac{X}{X+\kappa}>\myE \left[\frac{X}{X+\kappa}\right]\right).
    \end{equation*}
    Hence we only need to verify
    \begin{equation}\label{eq:toBeJen}
        \Pr \left(\frac{X}{X+\kappa_1}>\myE \left[\frac{X}{X+\kappa_1}\right]\right) 
\geq
\Pr \left(\frac{X}{X+\kappa_2}>\myE \left[\frac{X}{X+\kappa_2}\right]\right) .
    \end{equation}
    Let $Y=X/(X+\kappa_2)$.
    Then
    \begin{equation*}
        \frac{X}{(X+\kappa_1)} = \frac{\kappa_2 Y}{ \kappa_1 + (\kappa_2-\kappa_1) Y} := f(Y).
    \end{equation*}
    Note that $f(Y)$ is increasing for $Y\geq 0$.
    Then the inequality \eqref{eq:toBeJen} is equivalent to
    \begin{equation*}
        \Pr \left(Y> f^{-1}\left(\myE f(Y)\right)\right) 
\geq
\Pr \left( Y >\myE Y\right) .
    \end{equation*}
    Hence we only need to verify
        $f^{-1}\left(\myE f(Y)\right)
        \leq
        \myE Y$, or equivalently, $\myE f(Y)
        \leq
        f(\myE Y)$.
        But the last inequality is a direct consequence of the concavity of $f(Y)$.
        This completes the proof.

\end{proof}






 

\section{haha3}
\begin{theorem}
    Let $\BA$ be an $n\times n$ symmetric matrix.
    Suppose the elements of $\bepsilon$ are symmetric and have finite eighth moments.
    Then
    \begin{equation*}
        \frac{
            \bepsilon^\top \BA \bepsilon-\sigma^2 \mytr (\BA)
        }{
            \sqrt{
    2\sigma^4 \mytr(\BA^2)
    +
        (\mu_4-3\sigma^4)\mytr(\BA\circ \BA)
            }             
        }
    \end{equation*}
    \label{approximation}
\end{theorem}
\begin{proof}
    Let
    \begin{equation*}
        \tilde a_{i,j}=
        \frac{a_{i,j}}{
            \sqrt{
    2\sigma^4 \mytr(\BA^2)
    +
        (\mu_4-3\sigma^4)\mytr(\BA\circ \BA)
            }             
        }
    \end{equation*}
Then
    \begin{equation*}
        \frac{
            \bepsilon^\top \BA \bepsilon-\sigma^2 \mytr (\BA)
        }{
            \sqrt{
    2\sigma^4 \mytr(\BA^2)
    +
        (\mu_4-3\sigma^4)\mytr(\BA\circ \BA)
            }             
        }
        =\sum_{i=1}^n \tilde a_{i,i}\epsilon_i^2
    +\sum_{i=1}^n \sum_{j\neq i}^n \tilde a_{i,j} \epsilon_i \epsilon_j
    \end{equation*}


    Let $z_1,\ldots,z_n$  be iid random variables with distribution $\mathcal N (0, \tilde{\sigma}^2) $ and $\check z_1, \ldots, \check z_n$ be iid random variables with distribution $\mathcal N (0,\tilde{\mu}_4-\tilde \sigma^4)$ which are independent of $\epsilon_1,\ldots, \epsilon_n$.
    For $l=1,\ldots, n$, let
    \begin{align*}
        S_l = & 
        \sum_{i=1}^{l-1} \tilde a_{i,i}(\epsilon_i^2-\sigma^2)
        +
        \sum_{i=l+1}^{n} \tilde a_{i,i}  \check  z_i
        +\sum_{i=1}^{l-1} \sum_{j\neq i}^{l-1} \tilde a_{i,j} \epsilon_i \epsilon_j
        +2\sum_{i=1}^{l-1} \sum_{j=l+1}^n \tilde a_{i,j} \epsilon_i z_j
        +\sum_{i=l+1}^n \sum_{j=l+1}^n \tilde a_{i,j} z_i z_j
        \\
        \xi_l = & \tilde a_{l,l} (\epsilon_l^2 -\sigma^2)
        +2\sum_{i < l} \tilde a_{i,l} \epsilon_i \epsilon_l
        +2\sum_{i > l} \tilde a_{i,l} z_i \epsilon_l
        \\
        \eta_l = &
        \tilde a_{l,l} \check z_l
        +2\sum_{i < l} \tilde a_{i,l} \epsilon_i z_l
        +2\sum_{i > l} \tilde a_{i,l} z_i z_l
    \end{align*}
    Then
    \begin{equation*}
        S_n + \xi_n =  \sum_{i=1}^n \tilde a_{i,i}(\epsilon_i^2 - \sigma^2)
        +\sum_{i=1}^n \sum_{j\neq i}^n \tilde a_{i,j} \epsilon_i \epsilon_j
    \end{equation*}
    \begin{equation*}
        S_1 +  \eta_1 = \sum_{i=1}^n \tilde a_{i,i}\check z_i
        +\sum_{i=1}^n \sum_{j\neq i}^n \tilde a_{i,j} z_i z_j
    \end{equation*}
    For $l=1,\ldots, n-1$, 
    \begin{equation*}
        S_l+ \xi_l =S_{l+1} + \eta_{l+1} 
    \end{equation*}

    Thus, for any $f \in \mathscr C^4 (\mathbb R)$,
    \begin{equation*}
        \begin{split}
        &
        \left|\myE f\left(\sum_{i=1}^n \tilde a_{i,i}(\epsilon_i^2 - \sigma^2)
        +\sum_{i=1}^n \sum_{j\neq i}^n \tilde a_{i,j} \epsilon_i \epsilon_j\right)
        -
        \myE f\left(\sum_{i=1}^n \tilde a_{i,i}\check z_i
        +\sum_{i=1}^n \sum_{j\neq i}^n \tilde a_{i,j} z_i z_j\right)\right| 
        \\
        =&
        \left| \myE f(S_n+\xi_n)-\myE f(S_1+\eta_1)\right|
        \\
        =&
        \left|\sum_{l=2}^{n} \left(\myE f(S_{l}+\xi_{l})-\myE f(S_{l-1}+\xi_{l-1})\right)+\myE f(S_{1}+\xi_{1})-\myE f(S_{1}+\eta_{1})\right|
        \\
        = &
       \left| \sum_{l=1}^{n} \myE f(S_{l}+\xi_{l})-\myE f(S_{l}+\eta_{l})\right|
        \end{split}
    \end{equation*}
    Apply Taylor's theorem, for $l=1,\ldots,n$,
    \begin{equation*}
        \begin{split}
            f(S_{l}+\xi_{l})=&
            f(S_{l})
            +
            \xi_{l}f'(S_{l})
            +
            \frac{1}{2}\xi_{l}^2f''(S_{l})
            +\frac{1}{6}\xi_{l}^3 f''' (S_{l})
            +
            \frac{1}{24}\xi_{l}^4 f'''' (S_{l}+\theta_1 \xi_{l}),
            \\
            f(S_{l}+\eta_{l})=&
            f(S_{l})
            +
            \eta_{l}f'(S_{l})
            +
            \frac{1}{2}\eta_{l}^2f''(S_{l})
            +\frac{1}{6}\eta_{l}^3 f''' (S_{l})
            +
            \frac{1}{24}\eta_{l}^4 f'''' (S_{l}+\theta_2 \eta_{l}),
        \end{split}
    \end{equation*}
    where $\theta_1,\theta_2\in[0,1]$.
    Thus,
    \begin{equation*}
        \begin{split}
             &\left| \myE f(S_{l}+\xi_{l})-\myE f(S_{l}+\eta_{l})\right|
             \\
\leq&
\left|
\myE (\xi_{l}-\eta_{l})f'(S_{l})
            +
            \myE \frac{1}{2}(\xi_{l}^2-\eta_l^2)f''(S_{l})
            +
            \myE \frac{1}{6}(\xi_{l}^3-\eta_l^3) f''' (S_{l})
            \right|
            +
        \frac{1}{12} \|f'''' \|_{\infty} \left( \myE (\xi_{l}^4)+\myE (\eta_{l}^4)\right)
             \\
=&
\left|
\myE f'(S_{l})
\myE_l (\xi_{l}-\eta_{l})
            +
            \myE \frac{1}{2}f''(S_{l})
\myE_l (\xi_{l}^2-\eta_l^2)
            +
            \myE \frac{1}{6} f''' (S_{l})
            \myE_l (\xi_{l}^3-\eta_l^3)
            \right|
            +
            \frac{1}{12} \|f'''' \|_{\infty} \left(\myE (\xi_{l}^4)+\myE (\eta_{l}^4)\right),
        \end{split}
    \end{equation*}
where $\myE_l$ is the expectation with respect to $\epsilon_l, z_l ,\check z_l$.
It is straightforward to show that
\begin{equation*}
    \begin{split}
        \myE_l (\xi_l-\eta_l)&=0, 
        \\
        \myE_l (\xi_l^2-\eta_l^2)&= 
        \left(
            (\mu_4-\sigma^4)
            -
            (\tilde \mu_4- \tilde \sigma^4)
        \right)
        \tilde a_{l,l}^2
        +4(\sigma^2 - \tilde \sigma^2) 
        \left( \sum_{i<l} \tilde a_{i,l} \epsilon_i + \sum_{i>l} \tilde a_{i,l} z_i   \right)^2
        ,
        \\
        \myE_l (\xi_l^3-\eta_l^3)&= 
        (\mu_6 - 3\mu_4 \sigma^2 + 2\sigma^6)
        \tilde a_{l,l}^3 
        +
        12 (\mu_4-\sigma^4) \tilde a_{l,l} \left( \sum_{i<l} \tilde a_{i,l} \epsilon_i + \sum_{i>l} \tilde a_{i,l} z_i    \right)^2 
        .
    \end{split}
\end{equation*}
Thus,
    \begin{equation*}
        \begin{split}
             &\left| \myE f(S_{l}+\xi_{l})-\myE f(S_{l}+\eta_{l})\right|
             \\
\leq&
\frac{1}{2}
\|f''\|_\infty
\left(
\left|
            (\mu_4-\sigma^4)
            -
            (\tilde \mu_4- \tilde \sigma^4)
\right|
        \tilde a_{l,l}^2
        +4|\sigma^2 - \tilde \sigma^2|
        \myE\left( \sum_{i<l} \tilde a_{i,l} \epsilon_i + \sum_{i>l} \tilde a_{i,l} z_i   \right)^2
        \right)
        \\
            &+
            \frac{1}{6} \|f'''\|_{\infty}
\left(
        (\mu_6 - 3\mu_4 \sigma^2 + 2\sigma^6)
        |\tilde a_{l,l}^3 |
        +
        12 (\mu_4-\sigma^4)
            \myE 
        |\tilde a_{l,l}| \left( \sum_{i<l} \tilde a_{i,l} \epsilon_i + \sum_{i>l} \tilde a_{i,l} z_i    \right)^2 
    \right)
    \\
            &+
            \frac{1}{12} \|f'''' \|_{\infty} \left(\myE (\xi_{l}^4)+\myE (\eta_{l}^4)\right)
            \\
=&\frac{1}{2}
\|f''\|_\infty
\left(
\left|
            (\mu_4-\sigma^4)
            -
            (\tilde \mu_4- \tilde \sigma^4)
\right|
        \tilde a_{l,l}^2
        +4|\sigma^2 - \tilde \sigma^2|
        \left( \sum_{i<l} \tilde a_{i,l}^2 \sigma^2 + \sum_{i>l} \tilde a_{i,l}^2 \tilde \sigma^2   \right)
        \right)
        \\
            &+
\frac{1}{6} \|f'''\|_\infty
\left(
        (\mu_6 - 3\mu_4 \sigma^2 + 2\sigma^6)
        |\tilde a_{l,l}^3 |
        +
        12 (\mu_4-\sigma^4)
        |\tilde a_{l,l}| \left( \sum_{i<l} \tilde a_{i,l}^2 \sigma^2 + \sum_{i>l} \tilde a_{i,l}^2 \tilde \sigma^2    \right)
    \right)
    \\
            &+
            \frac{1}{12} \|f'''' \|_{\infty} \left(\myE (\xi_{l}^4)+\myE (\eta_{l}^4)\right)
            .
        \end{split}
    \end{equation*}
    Thus,
    \begin{equation*}
        \begin{split}
             &
             \sum_{l=1}^n \left| \myE f(S_{l}+\xi_{l})-\myE f(S_{l}+\eta_{l})\right|
             \\
\leq&
\frac{1}{2}
\|f''\|_\infty
\left(
\left|
            \mu_4
            -
            \tilde \mu_4
\right|
\sum_{l=1}^n \tilde a_{l,l}^2
        +2|\sigma^4 - \tilde \sigma^4|
        \sum_{i=1}^n \sum_{l=1}^n \tilde a_{i,l}^2    \right)
        \\
            &+
            \frac{1}{6} \|f'''\|_{\infty}
\left(
        (\mu_6 - 3\mu_4 \sigma^2 + 2\sigma^6)
        \sum_{l=1}^n  |\tilde a_{l,l}^3 |
        +
        12 (\mu_4-\sigma^4)
        \sum_{l=1}^n |\tilde a_{l,l}| \left( \sum_{i<l} \tilde a_{i,l}^2 \sigma^2 + \sum_{i>l} \tilde a_{i,l}^2 \tilde \sigma^2    \right)
    \right)
    \\
            &+
            \frac{1}{12} \|f'''' \|_{\infty} \left(\myE (\xi_{l}^4)+\myE (\eta_{l}^4)\right)
            .
        \end{split}
    \end{equation*}

We have
\begin{equation*}
    \begin{split}
        \myE (\xi_l^4)&=
        O((\sum_{i=1}^n \tilde a_{i,l}^2 )^2)
        ,
        \quad
        \myE (\eta_l^4)=
O((\sum_{i=1}^n \tilde a_{i,l}^2 )^2)
.
    \end{split}
\end{equation*}
Then

    \begin{equation*}
        \begin{split}
             &
             \sum_{l=1}^n \left| \myE f(S_{l}+\xi_{l})-\myE f(S_{l}+\eta_{l})\right|
             \\
\leq&
\frac{1}{2}
\|f''\|_\infty
\left(
\left|
            \mu_4
            -
            \tilde \mu_4
\right|
\sum_{l=1}^n \tilde a_{l,l}^2
        +2|\sigma^4 - \tilde \sigma^4|
        \sum_{i=1}^n \sum_{l=1}^n \tilde a_{i,l}^2    \right)
        \\
            &+
            O(1) \|f'''\|_{\infty}
            \max_{l\in\{1,\ldots,n\}} |\tilde a_{l,l}|
            +
            O(1) \|f'''' \|_{\infty} 
                \max_{l\in\{1,\ldots, n\}}
            \left(
   \sum_{i=1}^n \tilde a_{i,l}^2 
        \right)
            .
        \end{split}
    \end{equation*}

 
\end{proof}


\end{appendices}



\bibliographystyle{apalike}
\bibliography{mybibfile}



\end{document}
