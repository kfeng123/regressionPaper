\documentclass[11pt]{article}
 
\newcommand\CG[1]{\textcolor{red}{#1}}

\usepackage{lineno,hyperref}

\usepackage[margin=1 in]{geometry}
\renewcommand{\baselinestretch}{1.25}


%\usepackage{refcheck}
\usepackage{authblk}
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[page,title]{appendix}
%\renewcommand\appendixname{haha}
\usepackage{enumerate}
\usepackage{changepage}
\usepackage{datetime}
\newdate{date}{9}{1}{2017}

%%%%%%%%%%%%%%  Notations %%%%%%%%%%
\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myP}{P}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

 \def\balpha{\bfsym \alpha}
 \def\bbeta{\bfsym \beta}
 \def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
 \def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
 \def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
 \def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
 \def\bnu{\bfsym {\nu}}
 \def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
 \def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \varepsilon}
 \def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
 \def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
 \def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
 \def\brho   {\bfsym {\rho}}
 \def\btau{\bfsym {\tau}}
 \def\bxi{\bfsym {\xi}}
 \def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{definition}{\quad\quad Definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}



\title{
A Bayesian-motivated test for linear model in high-dimensional setting
}



\author[1]{Rui Wang}
%\author[2]{xx}
%\author[1,3]{Xingzhong Xu\thanks{Corresponding author\\Email address: xuxz@bit.edu.cn}}
%\affil[1]{
%School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    %100081,China
%}
%\affil[2]{
    %xx
%}
%\affil[3]{
%Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China
%}



\begin{document}
\maketitle
\section{Introduction} 
Suppose we would like to compare models $\mathcal M_0$ and $\mathcal M_1$.
\begin{align*}
    \mathcal M_0:   \By = \BX_a \bbeta_a + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n),
    \\
    \mathcal M_1:   \By = \BX_a \bbeta_a + \BX_b \bbeta_b + \bepsilon,\quad \bepsilon\sim \mathcal N_n(0,\phi^{-1} \BI_n).
\end{align*}
Here $\bbeta_a$ is $q$ dimensional and $\bbeta_b$ is $p$ dimensional.
We assume that as $n$ tends to infinity, $q$ is fixed while $p/n \to \infty$.
This assumption is reasonable.
In practice, $p_0$ is often $1$ and $\BX_0$ is $\mathbf 1_n$.


Although several tests have been proposed, the following proposition implies that there is no unbiased test.
\begin{proposition}\label{prop:unbiased}
Suppose
$\By\sim \mathcal N_n (\mu, \phi^{-1}\BI_n)$.
We test $H_0: \mu=\BX_a \bbeta_a, \bbeta_a \in \mathbb R^q$ versus
$H_1: \mu\in \mathbb R^n$, where $\BX_a$ is an $n\times q$ matrix with full column rank, $q<n$.
Let $\varphi(\By)$ be a test function, that is, a Borel measurable function, $0\leq \phi(\By)\leq 1$.
If $\int\varphi(\By) \mathcal N_n (\BX_a\bbeta_a, \phi^{-1} \BI_n) (d\By)=\alpha$ for $\bbeta_a \in \mathbb R^q$, $\phi>0$ and $\int\varphi(\By) \mathcal N_n (\mu, \phi^{-1} \BI_n) (d\By) \geq \alpha$ for $\mu\in \mathbb R^n$, $\phi>0$, then $\varphi(\By)=\alpha$, a.s.
\end{proposition}

So we can not find a universally good test.
Instead, we would like to find a test with good average behaviour.
So Bayesian methods are natural choices in this case.




Bayes hypothesis testing use the Bayes factor.
\begin{equation*}
    B_{10}= \frac {
        \int f_1(y|\bbeta_b ,\bbeta_a, \phi) \pi_1(\bbeta_b,\bbeta_a,\phi) d\bbeta_b d\bbeta_a d\phi
}{
        \int f_0(y|\bbeta_a, \phi) \pi_0(\bbeta_a,\phi) d\bbeta_a d\phi
    }.
\end{equation*}

There have been several extensions of $g$-priors to $p>n$ case: \cite{maruyama2011}, \cite{Shang2011}.

Under $\mathcal M_0$, we impose the reference prior $\pi_0 (\bbeta_a,\phi)=c/\phi$.
Note that under $\mathcal M_1$, the posterior corresponding to the referece prior is proper only if $n>q + p$?
That is, the minimal training sample size is $q + p +1$.
So we cannot impose the reference prior under $\mathcal M_1$ provided $q + p +1 >n$.
We temporarily impose the conditional prior $\bbeta_b|\bbeta_a, \phi \sim \mathcal N_{p} (0, \kappa^{-1} \phi^{-1} \BI_{p}) $.
There are many literature consider the choice of $\kappa$.
\cite{Kass1995} choose $\kappa$ such that the amount of information about the parameter equal to the amount of information contained in one observation.
Thus, under $\mathcal M_1$, we put prior
\begin{equation*}
    \pi_1 (\bbeta_b | \bbeta_a, \phi) =\frac{(\kappa \phi )^{p/2}}{(2\pi )^{p/2}}  
    \exp\left\{
        -\frac{\kappa \phi }{2} \|\bbeta_b\|^2
    \right\}
    ,\quad
    \pi_1(\bbeta_a, \phi) = \frac{c}{\phi}.
\end{equation*}
It is straightforward to show that the Bayes factor associated with these priors is
\begin{equation*}
    \begin{split}
        B_{10}^{\kappa}=  &
    \frac{\kappa^{p/2}}{
        |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |^{1/2}
    }
    \cdot
    \\
    &
    \left(
        \frac{
            \By\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
            -
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }
    \right)^{(n-q)/2}.
    \end{split}
\end{equation*}
Thus,
\begin{equation*}
    \begin{split}
        2\log B_{10}^{\kappa} =  &
    p\log \kappa
    -
        \log |\BX_b^\top (\BI_n -\BP_a) \BX_b + \kappa \BI_p |
    \\
    &
    -(n-q)\log \left(
            1-
        \frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
    \right).
    \end{split}
\end{equation*}

Denote by $\BI_n-\BP_a=\tilde{\BU}_a \tilde{\BU}_a^\top$ the rank decomposition of $\BI_n - \BP_a$, where $\tilde{\BU}_a$ is a $n\times (n-q)$ column orthogonal matrix.
Let $\BX_b^* = \tilde{\BU}_a^\top \BX_b$, $\By^* =\tilde \BU_a^\top \BX_b$.
Let $\gamma_i$ be the $i$th largest eigenvalue of $\BX^*_b \BX_b^{*\top}$, $i=1,\ldots, n-q$.
Denote by $\BX_b^* =\BU_{b}^* \BD_{b}^* \BV_b^{*\top}$ the singular value decomposition of $\BX_{b}^*$, where  $\BU_{b}^*$, $\BV_b^*$ are $(n-q)\times (n-q)$ and $p\times (n-q)$ column orthogonal matrices, respectively, and $\BD_{b}^*=\mydiag (\sqrt {\gamma_1},\ldots, \sqrt{\gamma_{n-q}})$.
Then

\begin{equation*}
    \begin{split}
        2\log B_{10}^\kappa
        =&
         p\log \kappa
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(p-(n-q))\log \kappa
         \\
         &-(n-q)\log\left(1-\frac{\By^{*\top} \BX_b^* \left( \BX_b^{*\top} \BX_b^* + \kappa \BI_p \right)^{-1} \BX_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         \\
        =&
         - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} \BU_b^*  \left[\frac 1 \kappa \left(\BI_{n-q}-\BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*} \right) \right] \BU_b^{*\top} \By^* }\right)
         \\
        =&
        (n-q)\log \kappa - \sum_{i=1}^{n-q}\log ( \gamma_i + \kappa )
         -(n-q)\log\left(1-\frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}\right)
         .
    \end{split}
\end{equation*}
The main part of $2\log B_{10}^\kappa$ is 
\begin{equation*}
    T_n^\kappa = \frac{\By^{*\top} \BU_b^*  \BD_b^{*} \left(  \BD_b^{*2} + \kappa \BI_{n-q} \right)^{-1} \BD_b^{*}   \BU_b^{*\top} \By^* }{\By^{*\top} \By^*}.
\end{equation*}
A large value of $T_n^\kappa$ supports the alternative hypothesis.
Under the null hypothesis, 
\begin{equation*}
    \myE T_{n}^\kappa= \frac{1}{n-q} \mytr \left( \BD_b^{*2} ( \BD_b^{*2} + \kappa \BI_{n-q})^{-1}\right).
\end{equation*}
Under the alternative hypothesis, consider $\bbeta_1=c \bbeta_1^\dagger$ where $\bbeta_1^\dagger\neq 0$ is a fixed direction and $c>0$.
As $c \to \infty$, \CG{$T_n^\kappa \to $}.
We say $T_{n}^\kappa$ is consistent along the direction $\bbeta_1^\dagger$ if.
It turns out that, $T_n^\kappa$ is consistent along certain directions of $\bbeta_1$, while it is inconsistent along other directions.
Define $B_{10}^0=\lim_{\kappa\to 0} B_{10}^\kappa$.
Then
\begin{equation*}
    2\log B_{10}^0= 
         - \sum_{i=1}^{n-q}\log ( \gamma_i )
         +(n-q)\log\left(\frac{\By^{*\top} \By^*}{\By^{*\top} ( \BX_b^* \BX_b^{*\top} )^{-1} \By^* }\right)
         .
\end{equation*}

\section{Distribution under the null hypothesis}
Under the null hypothesis, the distribution of $2\log B_{10}$ does not rely on unknown parameters.
Further more, its distribution is valid as long as the distribution of $\epsilon$ is spherically symmetric.
\begin{proposition}
    Under the null hypothesis,
    \begin{equation*}
        T_n:=\frac{
            \By\top (\BI_n-\BP_a) \BX_b
            \left(\BX_b^\top  (\BI_n-\BP_a) \BX_b + \kappa \BI_p\right)^{-1}
            \BX_b^\top (\BI_n-\BP_a) \By
        }{
            \By\top (\BI_n-\BP_a) \By
        }
        \sim
        \frac{\sum_{i=1}^{n-q} \frac{\gamma_i}{\gamma_i +\kappa}Z_i^2}{\sum_{i=1}^{n-q} Z_i^2},
    \end{equation*}
    where $\gamma_i$ is the $i$th eigenvalue of $\BX_b^\top (\BI_n -\BP_a) \BX_b$, $i=1,\ldots, n-q$, and $Z_1,\ldots, Z_{n-q}$ are iid $\mathcal N(0,1)$ random variables.
\end{proposition}
Let $\nu_i={\gamma_i}/(\gamma_i +\kappa)$, $\bar \nu = (n-q)^{-1} \sum_{i=1}^{n-q} \nu_i$.

\begin{lemma}
    Under the null hypothesis, a necessary and sufficient condition for
    \begin{equation}\label{eq:lemma1}
        \frac{n-q}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
        }
        (T_n- \bar \nu)
        \xrightarrow{\mathcal L} \mathcal N (0,1)
    \end{equation}
    is that
    \begin{equation}\label{eq:lemma12}
        \frac{\max_{i\in\{1,\ldots, n-q\}} (\nu_i- \bar \nu)^2}{\sum_{i=1}^{n-q}(\nu_i- \bar \nu)^2} \to 0.
    \end{equation}
\end{lemma}
\begin{proof}
    Note that 
    \begin{equation*}
        \frac{n-q}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
        }
        (T_n- \bar \nu)
    \sim
        \frac{n-q}{
    \sum_{i=1}^{n-q} Z_i^2
        }
    \frac{\sum_{i=1}^{n-q} (\nu_i- \bar \nu) Z_i^2}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
    }
    .
    \end{equation*}
    By Slutsky's theorem, \eqref{eq:lemma1} holds if and only if
    \begin{equation*}
    \frac{\sum_{i=1}^{n-q} (\nu_i- \bar \nu) Z_i^2}{
        \sqrt{2\sum_{i=1}^{n-q}(\nu_i-\bar{\nu})^2}
    }
    \xrightarrow{\mathcal L} \mathcal N (0,1).
    \end{equation*}
    From Lemma 1 of \cite{WANG2018225}, \eqref{eq:lemma12} is a necessary and sufficient condition for this to hold.
\end{proof}


\section{Distribution under the alternative hypothesis}








%\section*{Acknowledgements}
%This work was supported by the National Natural Science Foundation of China under Grant Nos.\ xxxxx, xxxx.




\begin{appendices}

    \section{haha1}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:unbiased}}]
    We assume $0<\alpha<1$ since the case $\alpha=0$ or $1$ is trivial.
    Note that the condition implies $\int [\varphi(\By)-\alpha] \mathcal N_n (0,\phi^{-1} \BI_n) (d\By)=0$.
    Hence it suffices to prove $\varphi(\By) \geq \alpha$, a.s. 
    We prove this by contradiction.
    Suppose $\lambda(\{\By:\varphi (\By) <\alpha\})>0$.
    Then there exists a $\eta >0$, such that $\lambda(\{\By:\varphi (\By) <\alpha-\eta\})>0$.
    We denote $E=\{\By:\varphi (\By) <\alpha-\eta\}$.
    From Lebesgue density theorem \citep[Corollary 6.2.6]{book:992991}, there exists a point $z\in E$, such that, for each $\epsilon >0$ there is a $\delta_{\epsilon}>0$ such that
    \begin{equation*}
        \left|\frac{\lambda(E^\complement\cap C_{\epsilon})}{\lambda(C_{\epsilon})}\right|<\epsilon,
    \end{equation*}
    where $C_{\epsilon}=\prod_{i=1}^n [z_i-\delta_{\epsilon}, z_i + \delta_{\epsilon}]$.
    We put
    \begin{equation*}
        \epsilon=\left(\frac{\sqrt \pi}{\sqrt 2 \Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^n \frac{\eta}{3}.
    \end{equation*}
    Then for any $\phi>0$,
    \begin{equation*}
        \begin{split}
            \alpha \leq& 
            \int_{\mathbb R^n}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            =&
            \int_{E\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{E^\complement\cap C_{\epsilon}}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement}\varphi(\By) \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \int_{E^\complement\cap C_{\epsilon}} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            +
            \int_{C_{\epsilon}^\complement} \mathcal N_n (z, \phi^{-1} \BI_n) (d\By)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}\lambda(E^\complement\cap C_{\epsilon})
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            \leq&
            \alpha-\eta
            +
            \left(\frac{\phi}{2\pi}\right)^{n/2}
            \epsilon
            (2\delta_\epsilon)^n
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right)
            \\
            =&
            \alpha-\eta
            +
            \left(\frac{\sqrt{\phi} \delta_{\epsilon}}{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}\right)^{n}
            \frac{\eta}{3}
            +
            2n\left(1-\Phi(\sqrt \phi \delta_\epsilon)\right).
        \end{split}
    \end{equation*}
    Putting 
    \begin{equation*}
        \phi = \left(\frac{\Phi^{-1}\left(1-\frac{\eta}{6n}\right)}{\delta_\epsilon}\right)^2
    \end{equation*}
    yields the contradiction $\alpha\leq \alpha-(2/3)\eta$.
    This completes the proof.
    


\end{proof}
    \section{haha2}
\end{appendices}



\bibliographystyle{apalike}
\bibliography{mybibfile}



\end{document}
