\documentclass[11pt]{article}
 
\newcommand\CG[1]{\textcolor{red}{#1}}

\usepackage{lineno,hyperref}

\usepackage[margin=1 in]{geometry}
\renewcommand{\baselinestretch}{1.25}


%\usepackage{refcheck}
\usepackage{authblk}
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[page,title]{appendix}
%\renewcommand\appendixname{haha}
\usepackage{enumerate}
\usepackage{changepage}
\usepackage{datetime}
\usepackage[FIGTOPCAP]{subfigure}
\newdate{date}{9}{1}{2017}

%%%%%%%%%%%%%%  Notations %%%%%%%%%%
\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myRank}{Rank}
\DeclareMathOperator{\myP}{P}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}
\DeclareMathOperator{\myCov}{Cov}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

 \def\balpha{\bfsym \alpha}
 \def\bbeta{\bfsym \beta}
 \def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
 \def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
 \def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
 \def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
 \def\bnu{\bfsym {\nu}}
 \def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
 \def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \epsilon}
 \def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
 \def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
 \def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
 \def\brho   {\bfsym {\rho}}
 \def\btau{\bfsym {\tau}}
 \def\bxi{\bfsym {\xi}}
 \def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{definition}{\quad\quad Definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}





\begin{document}
\title{
Response to Reviewers
``
Nonexistence of unbiased test for high-dimensional linear model and a Bayesian-motivated test 
''
}



\author[1]{Rui Wang}
\author[1,2]{Xingzhong Xu\thanks{Corresponding author\\Email address: xuxz@bit.edu.cn}}
\affil[1]{
School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    100081,China
}
\affil[2]{
Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China
}

\maketitle

We thank both reviewer for their helpful comments and critiques.
We have carefully considered the comments and made corresponding changes to the paper.
Our responds are as follows.

\section{Response to reviewer 1}

\textbf{
    1.
    The manuscript should provide comparison with the literature that exploits sparsity because under the null hypothesis the model parameter is sparse (with at most $q$ non-zero entries).
    For example, \cite{zhang2016simultaneous} studies the same problem of testing (high-dimensional components) of linear regressions. Apart from assuming random designs, this literature attempts to detect deviations in $l_\infty$-norm, while the current manuscript seems to aim at detecting deviations in $l_2$-norm.
    In my opinion, more discussion on this literature would be helpful so that the reader would get a fair impression on what has been done and what assumptions different methods require.
    For example, here symmetry of error distribution is needed (Assumption 1), which is not usually required in the literature on high-dimensional inference.
}

\textbf{Answer:}
Following the reviewer's suggestion, we have provided the simulation results of the desparsifying lasso test of \cite{zhang2016simultaneous} (abbreviated as DL) and add the following comment to clarify the application regime of the proposed test.

\begin{adjustwidth}{3em}{3em}
In the sparse $\bbeta_b$ setting of
Model II and Model III, the proposed test is not as powerful as the DL test.
In fact, as an extreme value type test, the DL test is designed for sparse linear
models.
In contrast, the proposed test test is a quadratic form which offers
higher power when the signal is dense.
On the basis of the simulations, we can recommend using the proposed
test when there is no prior knowledge of sparsity.
\end{adjustwidth}

As the reviewer pointed out, the symmetry of error distribution is not usually required in the literature on high-dimensional inference.
However, this condition is a key assumption in the asymptotic theory of quadratic forms and is also assumed by \cite{Bai2017}.
It will be a highly nontrivial task to relax this assumption.
We leave it for possible future work.
We add the following comment in Section 5:
\begin{adjustwidth}{3em}{3em}
Our Theorem 2 gives a general approximation for the distributions of
quadratic forms. A key assumption in Theorem 2 is that the distribution of $\xi_1$ is symmetric about $0$..
This condition is also assumed by \cite{Bai2017} in the study of central limit theorem of quadratic form. However, this condition is not usually required in the literature on high-dimensional inference.
We think it will be an interesting and useful work to relax the symmetric condition in Theorem 2.
\end{adjustwidth}

\textbf{
    2.
    Theorem 4 provides the power analysis, but its assumption in Equation (8) needs more discussion.
    For example, at the end of Section 3, the manuscript uses the factor model to illustrate the power of the proposed test over Goeman et al. (2006).
    However, this discussion is based on Theorem 4 and my rough calculation suggests that Equation (8), which is required by Theorem 4, does not seem to hold for factor models.
    Let me outline some details.
    \begin{enumerate}[(a)]
        \item 
            Supppose that $q=0$ (so $\tilde \BU_a = \BI_n$) and $\BX_b= \begin{pmatrix} W_1^\top \\ \vdots \\ W_n^\top \end{pmatrix}$ with $W_i = B F_i + u_i$, where $F_i$ is a scalar from i.i.d. $\mathcal N(0,1)$, $u_i$ is from $\mathcal N(0, \BI_p)$ and $B$ is from $\mathcal N (0, \BI_p)$.
            Assume $F_i$, $u_i$ and $B$ are independent.
        \item
            Since $\myE (\gamma_I^{2k}) \geq \myVar (\gamma_I^k)$, a necessary condition for Equation (8) is
            \begin{equation*}
                \frac{(\gamma_1^k - \myE_I(\gamma_I^k))^2}{n\myE_I (\gamma_I^{2k})} \to 0,
            \end{equation*}
            where $\myE_I$ denotes expectation taken over randomness of $I$.
        \item
            Based on the computation above Theorem 4 (i.e. $\myE_I (\gamma_I^k) = \mytr (\BX_b \BX_b^\top)^k /n$), we know that $\myE_I (\gamma_I) = n^{-1} \sum_{i=1}^n W_i^\top W_i$ and 
            \begin{equation*}
                \myE_I (\gamma_I^2)  = n^{-1} \sum_{i=1}^n (W_i^\top W_i)^2 + n^{-1}\sum_{i=1}^n \sum_{j=1, j\neq i}^n (W_i^\top W_j)^2.
            \end{equation*}
        \item
            For $k=1$, we use the assumptions in (a) to determined the size of $\gamma_1$, $\myE_I (\gamma_I)$ and $\myE_I (\gamma_I^2)$.
            Simple computations would give use $\myE W_i^\top W_i = 2p$, $\myE (W_i^\top W_i)^2 = 6p^2 + 10p$ and $\myE (W_i^\top W_j)^2 = p^2 + 5p$ for $i\neq j$.
            Thus, $\gamma_1 = O_p(np)$, $\myE_I (\gamma_I) = O_p(p)$ and $\myE_I (\gamma_I^2) = O_p(np^2)$. Thus, $\frac{(\gamma_I^k - \myE_i (\gamma_I^k))^2}{n\myE_I (\gamma_I^{2k})}$ with $k=1$ seem to be of the order $O_P(1)$ instead of $o_P(1)$.
        \item
            Overall, I think discussions on why Equation (8) should hold is needed, at least for the examples used at the end of Section 4 (i.i.d. rows and factor models).
    \end{enumerate}
}

\textbf{Answer:}

\textbf{
    3.
    I think the power comparison with \cite{Goeman2006} should be more
    formal. Currently, there is a comparison on when $\myCov(-\gamma_I^{-1}, \gamma_I w_I^2)$ and $\myCov(\gamma_I, \gamma_I \omega_I^2)$ are positive.
    However, in the two expressions for asymptotic power on page 12, this is not the only difference.
    For example, there is also $\myVar (\gamma_I)$ versus $\myVar(\gamma_I^{-1})$.
    Therefore, in the end, which one is more powerful is not completely obvious to me.
    Even in the simple case of $w_i = 0$ for $i = 1, ..., n−q$, I am not certain which test has better asymptotic power.
I think a formal result comparing the power of the two test would be nice
even under restricted assumptions.
}

\textbf{Answer:}


\section{Response to reviewer 2}

\textbf{
    1. The title should be changed. Nonexistence of unbiased test is not the main focus of this article, it is just the motivation of the paper. I suggest to change the title to ``A Bayesian test for high dimensional linear regression models with fixed design matrix''.
}

\textbf{Answer:}
We agree that the original title is not good enough.
We partly follow the suggestion of the reviewer and change the title to ``A Bayesian motivated test for high dimensional linear regression models with fixed design matrix''.
We preserve the word ``motivated'' since Bayesian method is just the motivation of our test and our test is actually a frequentist test.

\textbf{
    2. The Introduction part should be re-written.
    The main focus of this article is two-aspects.
    One is the Bayesian test, and the other is the fixed design matrix. 
    The introduction part should clearly state about your contribution through the two aspects compared with other competitors.
}

\textbf{Answer:}
We follow the advice of the reviewer and extend the introduction part.

\textbf{
    3. The transition through Section 2 to Section 3 is not clear.
    When you considering tests with good average power.
    Why Bayesian methods are natural choices?
    Some discussions are needed. In addition, the non-existence of nontrivial unbiased test is a trivial result for high dimensional setting.
}

\textbf{Answer:}
In frequentist point of view, Bayesian methods aim at minimize the average risk.
See Lehmann's book.
In the revised paper, we add some discussion at the end of Section 2.

\textbf{
    4. Your proposed test is quite similar with that of the likelihood ratio test, some discussions for the differences are needed.
    In addition, since $\BX^*_b$ is of dimension $p$, when both $n$ and p are large, the matrix $\BX^*_b \BX^{*\top}_b$ is not invertible.
    Does your method applicable for $n>p$?
}

\text{Answer:}
We follow the suggestion of the reviewer.
Our method is not applicable for $n>p$.
If $n>p$, the prior distributions adopted by us will yield exactly the likelihood ratio test.
This is an advantage of out test as it can be seen as a natural extention of the likelihood ratio test to the high dimensional setting.
When $n>p$, the likelihood ratio test exists and should be used.

\textbf{
    5. The differences between your test and that of Goeman et al. (2006) and Lan et al. (2014) need to be discussed carefully.
}

\text{Answer:}
The first difference is the test statistic.
We use the different prior strategy, and produce different statistics.
The second difference is the way to determine the critical value.
Our method has a wider application scope.

\textbf{
    6. Throughout the article, I do not see any results related to fix design matrix, why the fix design is essential for your analysis?
    Is it useful for your theoretical results?
}

\text{Answer:}
The result in Section 2 requires the fix design matrix.
In fact, if the design matrix is random, there indeed exists tests with nontrivial power.
See the papers on minimax test.
For Bayesian methods, fixed design and random design will produce the same statistic.
As for the theoretical results for our test. These results also hold for random design matrix provided the random design matrix satisfies our conditions.



\section{List of major changes}






\bibliographystyle{apalike}
\bibliography{mybibfile}



\end{document}
